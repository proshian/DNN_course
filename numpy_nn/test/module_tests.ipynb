{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427c3428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\n"
     ]
    }
   ],
   "source": [
    "# move to project's root directory to make\n",
    "# numpy_nn and pytorch_nn packages accessable \n",
    "%cd ../..\n",
    "\n",
    "\n",
    "# # Another possible solution is appending to the sys.path\n",
    "\n",
    "# import sys\n",
    "# import  os\n",
    "# project_root = os.path.dirname(os.path.dirname(sys.path[0]))\n",
    "# if project_root not in sys.path:\n",
    "#     sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a56421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "\n",
    "from numpy_nn.modules.np_nn import (\n",
    "    FullyConnectedLayer,\n",
    "    ReLULayer,\n",
    "    SigmoidLayer,\n",
    "    ReLULayer,\n",
    "    AdamOptimizer,\n",
    "    CrossEntropyLoss,\n",
    "    LinearActivation,\n",
    "    Sequential,\n",
    "    Optimizer,\n",
    "    SoftMaxLayer,\n",
    "    GradientDescentOptimizer,\n",
    "    CrossEntropyLossWithSoftMax,\n",
    "    Conv2d,\n",
    "    Conv2dWithLoops,\n",
    "    Flatten,\n",
    "    MaxPool2d,\n",
    "    AdamOptimizer,\n",
    "    BatchNormalization2d,\n",
    ")\n",
    "\n",
    "from pytorch_nn.models.resnet import (\n",
    "    Bottleneck as Bottleneck_torch,\n",
    "    resnet101 as resnet101_torch\n",
    ")\n",
    "\n",
    "from pytorch_nn.models.resnet_without_batchnorm import (\n",
    "    Bottleneck as Bottleneck_torch_without_batchnorm,\n",
    "    resnet101 as resnet101_torch_without_batchnorm\n",
    ")\n",
    "\n",
    "from numpy_nn.models.resnet import Bottleneck, resnet101\n",
    "\n",
    "from numpy_nn.models.resnet_without_batchnorm import (\n",
    "    Bottleneck as Bottleneck_np_without_batchnorm,\n",
    "    resnet101 as resnet101_np_without_batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e86a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "w gradients all close: True\n",
      "b gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FullyConnectedLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_output_features).astype(np.float32)\n",
    "\n",
    "torch_fc = torch.nn.Linear(n_input_features, n_output_features)\n",
    "torch_out = torch_fc(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_wg = torch_fc.weight.grad.detach().numpy().T\n",
    "torch_bg = torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_fc = FullyConnectedLayer(n_input_features, n_output_features)\n",
    "my_fc.weights = torch_fc.weight.detach().numpy().T\n",
    "my_fc.bias = torch_fc.bias.detach().numpy().reshape(-1, 1).T\n",
    "my_out = my_fc.forward(input_data)\n",
    "my_input_g = my_fc.backward(output_gradient)\n",
    "my_wg = my_fc.weights_gradient\n",
    "my_bg = my_fc.bias_gradient\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"w gradients all close:\", np.allclose(my_wg, torch_wg))\n",
    "print(\"b gradients all close:\", np.allclose(my_bg, torch_bg))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, input_data_torch.grad))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_bg))\n",
    "\n",
    "# print(\"all parameters shape same: \", my_fc.weights.shape == torch_fc.weight.T.shape and my_fc.bias.shape == torch.unsqueeze(torch_fc.bias, 1).shape)\n",
    "# print(\"output sum of square dif:\", np.square(my_out - torch_out_np).sum())\n",
    "# print(torch_wg.sum(), my_wg.sum())\n",
    "# print(\"w gradient sum of square dif:\", np.square(my_wg - torch_wg).sum())\n",
    "\n",
    "#print()\n",
    "#print(my_wg)\n",
    "#print()\n",
    "#print(torch_wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba7f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_val all close: True\n",
      "loss gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CrossEntropyLoss test\n",
    "\"\"\"\n",
    "def one_hot(y: np.ndarray, n_classes: int):\n",
    "    encoded = np.zeros((y.size, n_classes))\n",
    "    encoded[np.arange(y.size), y] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "n_classes = 3\n",
    "pred = np.random.rand(batch_size, n_classes).astype(np.float32)\n",
    "true = one_hot(np.random.randint(0, n_classes, batch_size), n_classes)\n",
    "pred_torch = torch.from_numpy(pred).float()\n",
    "true_torch = torch.from_numpy(true).float()\n",
    "pred_torch.requires_grad = True\n",
    "\n",
    "torch_loss  = torch.nn.CrossEntropyLoss()\n",
    "torch_loss_val = torch_loss(pred_torch, true_torch)\n",
    "torch_loss_val.backward()\n",
    "\n",
    "my_loss = CrossEntropyLossWithSoftMax()\n",
    "my_loss_val = my_loss.forward(pred, true)\n",
    "my_loss.backward()\n",
    "\n",
    "print(\"loss_val all close:\", np.allclose(my_loss_val, torch_loss_val.detach().numpy()))\n",
    "print(\"loss gradients all close:\", np.allclose(my_loss.backward(), pred_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99dabc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ReLULayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_relu = torch.nn.ReLU()\n",
    "torch_out = torch_relu(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_relu = ReLULayer()\n",
    "my_out = my_relu.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_relu.backward(output_gradient), input_data_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "461e801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SigmoidLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "torch_out = torch_sigmoid(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_sigmoid = SigmoidLayer()\n",
    "my_out = my_sigmoid.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_sigmoid.backward(output_gradient), input_data_torch.grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1d5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SigmoidLayer test on a 4D tensor\n",
    "\"\"\"\n",
    "\n",
    "n_input_channels = 3\n",
    "n_samples = 2\n",
    "height = 5\n",
    "width = 5\n",
    "input_data = np.random.rand(n_samples, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_channels, height, width).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "torch_out = torch_sigmoid(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_sigmoid = SigmoidLayer()\n",
    "my_out = my_sigmoid.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_sigmoid.backward(output_gradient), input_data_torch.grad))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_bg))\n",
    "\n",
    "# print(\"all parameters shape same: \", my_fc.weights.shape == torch_fc.weight.T.shape and my_fc.bias.shape == torch.unsqueeze(torch_fc.bias, 1).shape)\n",
    "# print(\"output sum of square dif:\", np.square(my_out - torch_out_np).sum())\n",
    "# print(torch_wg.sum(), my_wg.sum())\n",
    "# print(\"w gradient sum of square dif:\", np.square(my_wg - torch_wg).sum())\n",
    "\n",
    "#print()\n",
    "#print(my_wg)\n",
    "#print()\n",
    "#print(torch_wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65f81748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FlattenLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_channels = 3\n",
    "n_samples = 2\n",
    "height = 5\n",
    "width = 5\n",
    "\n",
    "input_data = np.random.rand(n_samples, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_channels * height * width).astype(np.float32)\n",
    "\n",
    "my_flatten = Flatten()\n",
    "my_out = my_flatten.forward(input_data)\n",
    "my_out_g = my_flatten.backward(output_gradient)\n",
    "\n",
    "torch_flatten = torch.nn.Flatten()\n",
    "torch_out = torch_flatten(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_out_g, torch_input_g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea63232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2dWithLoops test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, width, height).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, output_width, output_height).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "my_conv = Conv2dWithLoops(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "my_conv.bias = torch_conv.bias.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "torch_bg = torch_conv.bias.grad.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "\n",
    "print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba16408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 2\n",
    "n_input_channels = 2\n",
    "n_output_channels = 2\n",
    "width = 4\n",
    "height = 4\n",
    "\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    my_conv.bias = torch_conv.bias.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    torch_bg = torch_conv.bias.grad.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward_as_matrix_multiplication(output_gradient)\n",
    "\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "if my_conv.bias is not None:\n",
    "    print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d test 2\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 3\n",
    "n_input_channels = 1\n",
    "n_output_channels = 64\n",
    "width = 32\n",
    "height = 32\n",
    "\n",
    "kernel_size = 7\n",
    "stride = 2\n",
    "padding = 3\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False)\n",
    "\n",
    "my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    my_conv.bias = torch_conv.bias.detach().numpy().reshape(my_conv.bias.shape)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    torch_bg = torch_conv.bias.grad.detach().numpy().reshape(my_conv.bias.shape)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward_as_matrix_multiplication(output_gradient)\n",
    "\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "if my_conv.bias is not None:\n",
    "    print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ee26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MaxPool2d test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 2\n",
    "n_channels = 3\n",
    "height = 6\n",
    "width = 4\n",
    "\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_pool = torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_pool = MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_out = my_pool.forward(input_data)\n",
    "\n",
    "torch_out = torch_pool(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_pool.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47856b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MaxPool2d test 2\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "n_channels = 3\n",
    "height = 16\n",
    "width = 16\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_pool = torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_pool = MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_out = my_pool.forward(input_data)\n",
    "\n",
    "torch_out = torch_pool(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_pool.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b5717f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride = 1\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "conv1 weights gradients all close: False\n",
      "conv2 weights gradients all close: False\n",
      "conv3 weights gradients all close: False\n",
      "bn3 gamma gradients all close: False\n",
      "bn3 beta gradients all close: False\n",
      "\n",
      "stride = 2\n",
      "output all close: False\n",
      "input gradients all close: False\n",
      "conv1 weights gradients all close: False\n",
      "conv2 weights gradients all close: False\n",
      "conv3 weights gradients all close: False\n",
      "conv_to_match_dimensions weights gradients all close: False\n",
      "bn3 gamma gradients all close: False\n",
      "bn3 beta gradients all close: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BottleNeckLayer test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "in_channels = 8\n",
    "bottleneck_depth = 2\n",
    "width = 6\n",
    "height = 6\n",
    "\n",
    "expansion_factor = 4\n",
    "n_output_channels = bottleneck_depth * expansion_factor\n",
    "\n",
    "momentum = 0.1\n",
    "\n",
    "for stride_for_downsampling in (1, 2):  # Checking both cases: no downsampling and downsampling\n",
    "    print(f\"stride = {stride_for_downsampling}\")\n",
    "    input_data = np.random.rand(batch_size, in_channels, width, height).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "\n",
    "    if stride_for_downsampling == 1:\n",
    "        output_width = width\n",
    "        output_height = height\n",
    "    if stride_for_downsampling == 2:\n",
    "        output_width = width // stride_for_downsampling\n",
    "        output_height = height // stride_for_downsampling\n",
    "    output_gradient = np.random.rand(batch_size, n_output_channels, output_width, output_height).astype(np.float32)\n",
    "\n",
    "    torch_bottleneck = Bottleneck_torch(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "    my_bottleneck = Bottleneck(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "\n",
    "    my_bottleneck.conv1.weights = torch_bottleneck.conv1.weight.detach().numpy().reshape(my_bottleneck.conv1.weights.shape)\n",
    "    my_bottleneck.conv2.weights = torch_bottleneck.conv2.weight.detach().numpy().reshape(my_bottleneck.conv2.weights.shape)\n",
    "    my_bottleneck.conv3.weights = torch_bottleneck.conv3.weight.detach().numpy().reshape(my_bottleneck.conv3.weights.shape)\n",
    "\n",
    "    my_bottleneck.bn1.gamma = torch_bottleneck.bn1.weight.detach().numpy().reshape(my_bottleneck.bn1.gamma.shape)\n",
    "    my_bottleneck.bn1.beta = torch_bottleneck.bn1.bias.detach().numpy().reshape(my_bottleneck.bn1.beta.shape)\n",
    "    my_bottleneck.bn1.running_mean = torch_bottleneck.bn1.running_mean.detach().numpy().reshape(my_bottleneck.bn1.running_mean.shape)\n",
    "    my_bottleneck.bn1.running_var = torch_bottleneck.bn1.running_var.detach().numpy().reshape(my_bottleneck.bn1.running_var.shape)\n",
    "    my_bottleneck.bn1.momentum = torch_bottleneck.bn1.momentum = momentum\n",
    "\n",
    "    my_bottleneck.bn2.gamma = torch_bottleneck.bn2.weight.detach().numpy().reshape(my_bottleneck.bn2.gamma.shape)\n",
    "    my_bottleneck.bn2.beta = torch_bottleneck.bn2.bias.detach().numpy().reshape(my_bottleneck.bn2.beta.shape)\n",
    "    my_bottleneck.bn2.running_mean = torch_bottleneck.bn2.running_mean.detach().numpy().reshape(my_bottleneck.bn2.running_mean.shape)\n",
    "    my_bottleneck.bn2.running_var = torch_bottleneck.bn2.running_var.detach().numpy().reshape(my_bottleneck.bn2.running_var.shape)\n",
    "    my_bottleneck.bn2.momentum = torch_bottleneck.bn2.momentum = momentum\n",
    "\n",
    "    my_bottleneck.bn3.gamma = torch_bottleneck.bn3.weight.detach().numpy().reshape(my_bottleneck.bn3.gamma.shape)\n",
    "    my_bottleneck.bn3.beta = torch_bottleneck.bn3.bias.detach().numpy().reshape(my_bottleneck.bn3.beta.shape)\n",
    "    my_bottleneck.bn3.running_mean = torch_bottleneck.bn3.running_mean.detach().numpy().reshape(my_bottleneck.bn3.running_mean.shape)\n",
    "    my_bottleneck.bn3.running_var = torch_bottleneck.bn3.running_var.detach().numpy().reshape(my_bottleneck.bn3.running_var.shape)\n",
    "    my_bottleneck.bn3.momentum = torch_bottleneck.bn3.momentum = momentum\n",
    "\n",
    "\n",
    "    if my_bottleneck.conv_to_match_dimensions:\n",
    "        my_bottleneck.conv_to_match_dimensions.weights = torch_bottleneck.conv_to_match_dimensions.weight.detach().numpy()\n",
    "        my_bottleneck.bn_for_residual.gamma = torch_bottleneck.bn_for_residual.weight.detach().numpy().reshape(my_bottleneck.bn_for_residual.gamma.shape)\n",
    "        my_bottleneck.bn_for_residual.beta = torch_bottleneck.bn_for_residual.bias.detach().numpy().reshape(my_bottleneck.bn_for_residual.beta.shape)\n",
    "        my_bottleneck.bn_for_residual.running_mean = torch_bottleneck.bn_for_residual.running_mean.detach().numpy().reshape(my_bottleneck.bn_for_residual.running_mean.shape)\n",
    "        my_bottleneck.bn_for_residual.running_var = torch_bottleneck.bn_for_residual.running_var.detach().numpy().reshape(my_bottleneck.bn_for_residual.running_var.shape)\n",
    "        torch_bottleneck.bn_for_residual.momentum = my_bottleneck.bn_for_residual.momentum = momentum\n",
    "    \n",
    "    my_bottleneck.train()\n",
    "    torch_bottleneck.train()\n",
    "\n",
    "    my_out = my_bottleneck.forward(input_data)\n",
    "    torch_out = torch_bottleneck(input_data_torch)\n",
    "\n",
    "    torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "    my_input_g = my_bottleneck.backward(output_gradient)\n",
    "\n",
    "    atol = 1e-3\n",
    "    print(\"output all close:\", np.allclose(my_out, torch_out.detach().numpy(), atol=atol))\n",
    "    print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "    print(\"conv1 weights gradients all close:\", np.allclose(my_bottleneck.conv1.weights_gradient, torch_bottleneck.conv1.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"conv2 weights gradients all close:\", np.allclose(my_bottleneck.conv2.weights_gradient, torch_bottleneck.conv2.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"conv3 weights gradients all close:\", np.allclose(my_bottleneck.conv3.weights_gradient, torch_bottleneck.conv3.weight.grad.detach().numpy(), atol=atol))\n",
    "    if my_bottleneck.conv_to_match_dimensions:\n",
    "        print(\"conv_to_match_dimensions weights gradients all close:\", np.allclose(my_bottleneck.conv_to_match_dimensions.weights_gradient, torch_bottleneck.conv_to_match_dimensions.weight.grad.detach().numpy(), atol=atol))\n",
    "    \n",
    "    print(\"bn3 gamma gradients all close:\", np.allclose(my_bottleneck.bn3.gamma_gradient, torch_bottleneck.bn3.weight.grad.detach().numpy(), atol=atol))  \n",
    "    print(\"bn3 beta gradients all close:\", np.allclose(my_bottleneck.bn3.beta_gradient, torch_bottleneck.bn3.bias.grad.detach().numpy(), atol=atol))  \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b68d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4f96846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: False\n",
      "input gradients all close: False\n",
      "fc weights gradients all close: False\n",
      "fc bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "resnet 101 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 10).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "my_resnet = resnet101(10, 1)\n",
    "torch_resnet.train()\n",
    "my_resnet.train()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: False\n",
      "fc weights gradients all close: True\n",
      "fc bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "atol=1e-0\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa7642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 145.09114   ,  -25.02309   ,   80.27217   , ...,\n",
       "            15.455878  ,  -15.968     ,  -11.062028  ],\n",
       "         [ -65.584076  ,  -45.690857  ,   12.723385  , ...,\n",
       "            64.765526  ,   21.317276  ,  -26.620539  ],\n",
       "         [  27.107925  , -231.20212   , -161.362     , ...,\n",
       "            85.78881   ,   16.74698   ,  -30.340061  ],\n",
       "         ...,\n",
       "         [  20.201824  ,   38.926605  ,   11.627379  , ...,\n",
       "           -76.438225  ,  -21.171398  ,  -26.883305  ],\n",
       "         [  92.24137   ,   82.08002   ,  -39.229794  , ...,\n",
       "           -46.737946  ,   20.103363  ,  -11.546026  ],\n",
       "         [  44.216526  ,   71.4704    ,   -7.657406  , ...,\n",
       "            53.8864    ,   14.559802  ,  -37.05036   ]]],\n",
       "\n",
       "\n",
       "       [[[-108.48956   ,  157.80595   , -169.80869   , ...,\n",
       "           -29.78574   ,   63.242413  ,   33.802334  ],\n",
       "         [ -14.517706  ,  -79.369545  ,  127.26038   , ...,\n",
       "           -29.300535  ,   -4.644705  , -129.8431    ],\n",
       "         [ 146.50601   ,  -89.67427   , -110.794     , ...,\n",
       "           159.24185   ,  -53.898125  ,   20.943548  ],\n",
       "         ...,\n",
       "         [  23.930323  ,  -22.230701  ,  101.71112   , ...,\n",
       "           -30.384583  ,   17.075752  ,    3.2818975 ],\n",
       "         [   1.5226359 ,   15.246853  ,    6.670807  , ...,\n",
       "            -8.696663  ,   -8.826391  ,    7.199544  ],\n",
       "         [   8.500245  ,  -36.825554  ,   41.921444  , ...,\n",
       "           -25.236414  ,  -13.761876  ,   -8.544212  ]]],\n",
       "\n",
       "\n",
       "       [[[  56.104935  ,   57.40413   ,   72.859314  , ...,\n",
       "           -29.467472  ,  -18.956877  ,  -33.26039   ],\n",
       "         [   1.3187962 ,   37.844017  ,   70.156944  , ...,\n",
       "           -81.158806  ,  -93.64993   ,  -48.944954  ],\n",
       "         [  11.528883  ,  120.13256   ,  -92.27815   , ...,\n",
       "            48.368168  ,   -5.8721867 ,   -5.1051865 ],\n",
       "         ...,\n",
       "         [ -13.897338  ,   16.412176  , -127.73457   , ...,\n",
       "             8.884907  ,  -26.428036  ,   18.431664  ],\n",
       "         [ -27.132586  ,   60.85431   ,   79.953545  , ...,\n",
       "            19.70967   ,  -14.863576  ,  -15.534958  ],\n",
       "         [ -29.975739  ,   22.327108  ,  -24.590982  , ...,\n",
       "           -22.971685  ,    6.8361006 ,  -11.834162  ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ -65.88694   ,  -81.47362   ,   19.999859  , ...,\n",
       "           -79.8415    ,  -57.441475  ,    5.801012  ],\n",
       "         [ -22.500137  ,  110.286156  ,  261.0417    , ...,\n",
       "            22.389545  ,  -75.13658   ,   43.53329   ],\n",
       "         [ -14.530243  ,  -74.15854   , -296.82745   , ...,\n",
       "           -59.53099   ,   38.095387  ,   20.552067  ],\n",
       "         ...,\n",
       "         [ -34.310482  ,   39.935097  ,  -43.767017  , ...,\n",
       "           -58.09526   ,   10.418945  ,   -1.4441528 ],\n",
       "         [  72.55706   ,   10.935901  ,  -27.851511  , ...,\n",
       "            46.86786   ,  -23.019747  ,   -4.965951  ],\n",
       "         [  58.85099   ,  -98.09843   ,   -1.9262955 , ...,\n",
       "           -54.944305  ,    4.704422  ,  -39.5886    ]]],\n",
       "\n",
       "\n",
       "       [[[ 145.12103   ,   59.31098   ,   32.69378   , ...,\n",
       "           -98.76242   ,  -40.925797  ,   47.167435  ],\n",
       "         [ -26.584682  ,  -96.08638   ,  145.86224   , ...,\n",
       "           -11.345228  ,  -69.194954  ,   18.604813  ],\n",
       "         [  19.96343   , -119.164825  , -212.34618   , ...,\n",
       "            55.735058  ,   13.906794  ,  -59.23385   ],\n",
       "         ...,\n",
       "         [ -55.13511   ,  -31.743565  ,   30.310375  , ...,\n",
       "            93.799965  ,   40.604538  ,   21.465427  ],\n",
       "         [   9.693649  ,   -0.69996786,    4.853736  , ...,\n",
       "            26.918686  ,  -10.621126  ,   45.90657   ],\n",
       "         [  -4.809579  ,    7.461362  ,  -11.501159  , ...,\n",
       "             6.029513  ,    8.785125  ,    4.9199457 ]]],\n",
       "\n",
       "\n",
       "       [[[ -23.610062  ,  -52.070534  ,  -59.489174  , ...,\n",
       "           -25.679188  ,  -23.648611  ,  -37.45632   ],\n",
       "         [  46.74628   ,   26.973299  ,   48.691578  , ...,\n",
       "           -51.06077   ,  -14.199205  ,   57.145874  ],\n",
       "         [ 113.19569   ,  -69.81437   ,   -4.815773  , ...,\n",
       "            -9.743327  ,   46.74962   ,   13.774216  ],\n",
       "         ...,\n",
       "         [ -97.297195  ,   17.87552   ,  -64.38756   , ...,\n",
       "           -22.138569  ,  -30.9189    ,   11.992727  ],\n",
       "         [  10.2315445 ,  -70.46197   ,   46.003124  , ...,\n",
       "            38.48402   ,  -12.641319  ,    4.6433687 ],\n",
       "         [  92.93469   ,   27.929916  ,  -44.633705  , ...,\n",
       "            38.95639   ,  -23.701395  ,  -29.377611  ]]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54841c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 170.67320924,  -46.81848262,  123.22044644, ...,\n",
       "            36.21926342,  -27.41851351,   -5.32035041],\n",
       "         [ -85.51042166, -109.2029572 ,   22.51898133, ...,\n",
       "            65.92639564,   15.0509407 ,  -21.15139888],\n",
       "         [   0.7833892 , -209.10058854, -233.5256561 , ...,\n",
       "            95.30540453,  -14.82276616,  -36.73474568],\n",
       "         ...,\n",
       "         [   7.51326682,   56.07831661,   31.74766702, ...,\n",
       "           -71.95863416,  -43.15187119,  -34.99049429],\n",
       "         [  80.10216494,   74.42708813,  -25.36521205, ...,\n",
       "           -50.29872127,    9.66731918,   -7.86839574],\n",
       "         [  25.45439444,   67.45193376,   10.51761966, ...,\n",
       "            46.86727751,   12.70940407,  -30.28840004]]],\n",
       "\n",
       "\n",
       "       [[[ -69.28798255,  152.44720921,  -73.58353681, ...,\n",
       "           -10.08209156,   76.94617724,    5.39621828],\n",
       "         [ -27.25747852,  -79.15825874,  104.68503426, ...,\n",
       "            -7.7341088 ,   30.67432065,  -89.63236232],\n",
       "         [ 117.4063394 ,  -66.46895267,  -97.53605005, ...,\n",
       "           104.09465521,  -61.86111406,   11.86889051],\n",
       "         ...,\n",
       "         [  33.42628307,  -29.240673  ,  147.17717428, ...,\n",
       "             3.86332155,   10.21386041,    4.38969076],\n",
       "         [  14.3437533 ,   23.63281477,   11.05268778, ...,\n",
       "             1.7415464 ,  -17.84195158,    0.2931302 ],\n",
       "         [   7.52095762,  -42.28653487,   45.20583303, ...,\n",
       "           -30.43755587,   -9.49957926,  -11.05935074]]],\n",
       "\n",
       "\n",
       "       [[[  75.76428846,   33.17255544,  126.66833911, ...,\n",
       "           -83.71282859,  -30.13311192,  -61.57105335],\n",
       "         [ -29.96370673,  -28.40674736,   38.22074281, ...,\n",
       "           -33.46152238,  -29.01995764,  -36.66870449],\n",
       "         [  37.85461451,  114.18023005, -127.17574217, ...,\n",
       "            96.52372986,   19.28137026,  -52.88047967],\n",
       "         ...,\n",
       "         [ -22.32280644,   22.38824708, -112.97167584, ...,\n",
       "            34.55328525,   -1.91914577,   -3.00454975],\n",
       "         [ -25.91369928,   33.64896216,  109.64937983, ...,\n",
       "            35.38323436,   -7.28164888,  -13.18738151],\n",
       "         [ -24.54510158,   41.41024672,  -12.59423773, ...,\n",
       "           -16.170574  ,    0.36962269,  -13.51386065]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ -21.48810602,  -32.72270407,   63.58387269, ...,\n",
       "           -27.6672104 ,  -33.04811877,   11.1794896 ],\n",
       "         [ -42.61017118,  117.17858686,  233.26073631, ...,\n",
       "             4.06946523,  -82.38397678,   32.44455185],\n",
       "         [  33.99750564,  -72.91064641, -243.23100318, ...,\n",
       "           -67.48737542,   21.3809141 ,   16.50899624],\n",
       "         ...,\n",
       "         [ -58.06370129,   43.14487156,  -59.03595627, ...,\n",
       "           -43.42022275,  -28.08292901,  -17.84371475],\n",
       "         [  81.64836189,   -2.97572663,  -26.47625828, ...,\n",
       "            44.77893376,  -13.96329192,   -2.59473519],\n",
       "         [  60.51617443,  -93.70741223,  -23.63531439, ...,\n",
       "           -43.96709392,   -5.59450362,  -35.01200963]]],\n",
       "\n",
       "\n",
       "       [[[ 192.07763406,   99.40151686,   65.46927996, ...,\n",
       "           -98.39324766,  -34.29730668,   34.23754919],\n",
       "         [ -94.27110494,  -43.56485322,  127.5092788 , ...,\n",
       "            28.02556943,  -69.91220222,  -16.99467288],\n",
       "         [  49.41310339, -180.75345194, -191.28351856, ...,\n",
       "            35.36806709,   29.90647937,  -44.77050688],\n",
       "         ...,\n",
       "         [ -43.20400947,  -38.86028222,  -18.09712454, ...,\n",
       "            66.5106897 ,   51.34282152,    4.05356184],\n",
       "         [ -15.11749829,    0.79711907,  -35.2353299 , ...,\n",
       "            52.43386294,  -10.55196807,   35.76663701],\n",
       "         [  26.18495059,   20.21991033,  -29.10882924, ...,\n",
       "            -0.82610963,    7.44063939,   -3.14588563]]],\n",
       "\n",
       "\n",
       "       [[[  47.22890125, -104.84931522,  -72.03262376, ...,\n",
       "           -19.19947794,  -10.56185516,  -24.04811618],\n",
       "         [  52.2658388 ,   69.70861257,  -55.61931439, ...,\n",
       "           -65.87777178,   -6.83146791,   66.65236388],\n",
       "         [  54.59324444, -121.43868192,   79.75889565, ...,\n",
       "           -36.49442313,   41.22569161,   26.91749465],\n",
       "         ...,\n",
       "         [-100.23396144,  -23.89322057,  -67.04802106, ...,\n",
       "           -44.37124147,  -29.73358652,   16.9256636 ],\n",
       "         [  -2.4605894 ,  -37.04994207,   62.76820278, ...,\n",
       "            47.83107822,  -20.93137462,   11.33173703],\n",
       "         [  76.95908549,    2.57033442,  -62.63624217, ...,\n",
       "            46.86149626,  -22.60487182,  -34.61998108]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "511f0e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n",
      "fc weights gradients all close: True\n",
      "fc bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "resnet 101 without batchnormtest\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 10).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch_without_batchnorm(10, 1)\n",
    "torch_resnet.eval()\n",
    "my_resnet = resnet101_np_without_batchnorm(10, 1)\n",
    "my_resnet.eval()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14fb02cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conv1 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 64, 16, 16).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "torch_resnet.eval()\n",
    "my_resnet = resnet101(10, 1)\n",
    "my_resnet.eval()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.conv1.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet.conv1(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.conv1.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1356b5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: train\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "\n",
      "phase: eval\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "bn1 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 16\n",
    "n_channels = 64\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "for phase in ['train', 'eval']:\n",
    "    print(f\"phase: {phase}\")\n",
    "\n",
    "    # ! Has been noticed that moving 4 lines below outside the loop leads to not passing tests\n",
    "    input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "\n",
    "    torch_resnet = resnet101_torch(n_classes, n_channels)\n",
    "    my_resnet = resnet101(n_classes, n_channels)\n",
    "    if phase == 'train':\n",
    "        my_resnet.train()\n",
    "        torch_resnet.train()\n",
    "    elif phase == 'eval':\n",
    "        my_resnet.eval()\n",
    "        torch_resnet.eval()\n",
    "    else:\n",
    "        raise Exception(\"unknown phase\") \n",
    "\n",
    "    my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "    my_out = my_resnet.bn1.forward(input_data)\n",
    "\n",
    "    torch_out = torch_resnet.bn1(input_data_torch)\n",
    "    torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "    torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "    my_input_g = my_resnet.bn1.backward(output_gradient)\n",
    "\n",
    "    atol=1e-3\n",
    "\n",
    "    print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "    print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6997b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57fc2d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: train\n",
      "output all close: False\n",
      "input gradients all close: False\n",
      "\n",
      "phase: eval\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conv2_x test\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 8\n",
    "n_channels = 64\n",
    "\n",
    "for phase in ['train', 'eval']:\n",
    "    print(f\"phase: {phase}\")\n",
    "\n",
    "    input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, 256, 8, 8).astype(np.float32)\n",
    "\n",
    "    torch_resnet = resnet101_torch(10, 1)\n",
    "    my_resnet = resnet101(10, 1)\n",
    "\n",
    "    if phase == 'train':\n",
    "        my_resnet.train()\n",
    "        torch_resnet.train()\n",
    "    elif phase == 'eval':\n",
    "        my_resnet.eval()\n",
    "        torch_resnet.eval()\n",
    "    else:\n",
    "        raise Exception(\"unknown phase\") \n",
    "\n",
    "\n",
    "    my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "    my_out = my_resnet.conv2_x.forward(input_data)\n",
    "\n",
    "    torch_out = torch_resnet.conv2_x(input_data_torch)\n",
    "    torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "    torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "    my_input_g = my_resnet.conv2_x.backward(output_gradient)\n",
    "\n",
    "    atol=1e-3\n",
    "\n",
    "    print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "    print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24faafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv3_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 8\n",
    "# n_channels = 256\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 512, 4, 4).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv3_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv3_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv3_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31786ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv4_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 4\n",
    "# n_channels = 512\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 1024, 2, 2).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv4_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv4_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv4_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv5_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 2\n",
    "# n_channels = 1024\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 2048, 1, 1).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv5_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv5_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv5_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# AdamOptimizer test\n",
    "# \"\"\"\n",
    "\n",
    "# from torch.optim import Adam as Adam_torch\n",
    "\n",
    "# n_input_features = 6\n",
    "# n_output_features = 3\n",
    "# batch_size = 5\n",
    "# input_data = np.random.rand(batch_size, n_input_features).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, n_output_features).astype(np.float32)\n",
    "\n",
    "# torch_fc = torch.nn.Linear(n_input_features, n_output_features)\n",
    "# torch_out = torch_fc(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_wg = torch_fc.weight.grad.detach().numpy().T\n",
    "# torch_bg = torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "# my_fc = FullyConnectedLayer(n_input_features, n_output_features)\n",
    "# my_fc.weights = torch_fc.weight.detach().numpy().T\n",
    "# my_fc.bias = torch_fc.bias.detach().numpy().reshape(-1, 1).T\n",
    "# my_out = my_fc.forward(input_data)\n",
    "# my_input_g = my_fc.backward(output_gradient)\n",
    "# my_wg = my_fc.weights_gradient\n",
    "# my_bg = my_fc.bias_gradient\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "# print(\"before adam weights gradients all close:\", np.allclose(my_wg, torch_wg, atol=atol))\n",
    "# print(\"before adam bias gradients all close:\", np.allclose(my_bg, torch_bg, atol=atol))\n",
    "# print(my_wg, \"\\n\", torch_wg)\n",
    "\n",
    "# my_adam = AdamOptimizer(my_fc.get_trainable_layers(), 0.001, 0.9, 0.999, 1e-8)\n",
    "# my_adam.step()\n",
    "\n",
    "# torch_adam = Adam_torch(torch_fc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n",
    "# torch_adam.step()\n",
    "\n",
    "# print(\"after adam weights gradients all close:\", np.allclose(my_fc.weights_gradient, torch_fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "# print(\"after adam bias gradients all close:\", np.allclose(my_fc.bias_gradient, torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T, atol=atol))\n",
    "# print(my_fc.weights_gradient, \"\\n\", torch_fc.weight.grad.detach().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bbaff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy_nn.modules.np_nn import BatchNormalization2d, TrainableLayer\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c243bb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d665a56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "\n",
    "from numpy_nn.modules.np_nn import (\n",
    "    FullyConnectedLayer,\n",
    "    ReLULayer,\n",
    "    SigmoidLayer,\n",
    "    ReLULayer,\n",
    "    AdamOptimizer,\n",
    "    CrossEntropyLoss,\n",
    "    LinearActivation,\n",
    "    Sequential,\n",
    "    Optimizer,\n",
    "    SoftMaxLayer,\n",
    "    GradientDescentOptimizer,\n",
    "    CrossEntropyLossWithSoftMax,\n",
    "    Conv2d,\n",
    "    Conv2dWithLoops,\n",
    "    Flatten,\n",
    "    MaxPool2d,\n",
    "    AdamOptimizer,\n",
    "    BatchNormalization2d,\n",
    ")\n",
    "\n",
    "from pytorch_nn.models.resnet import (\n",
    "    Bottleneck as Bottleneck_torch,\n",
    "    resnet101 as resnet101_torch\n",
    ")\n",
    "\n",
    "from pytorch_nn.models.resnet_without_batchnorm import (\n",
    "    Bottleneck as Bottleneck_torch_without_batchnorm,\n",
    "    resnet101 as resnet101_torch_without_batchnorm\n",
    ")\n",
    "\n",
    "from numpy_nn.models.resnet import Bottleneck, resnet101\n",
    "\n",
    "from numpy_nn.models.resnet_without_batchnorm import (\n",
    "    Bottleneck as Bottleneck_np_without_batchnorm,\n",
    "    resnet101 as resnet101_np_without_batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e7675aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: train\n",
      "momentum: 0.01\n",
      "True\n",
      "iteration: 0\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 1\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 2\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "momentum: 0.8\n",
      "True\n",
      "iteration: 0\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 1\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 2\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "momentum: 1.0\n",
      "True\n",
      "iteration: 0\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 1\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 2\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "phase: eval\n",
      "momentum: 0.01\n",
      "True\n",
      "iteration: 0\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 1\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 2\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "momentum: 0.8\n",
      "True\n",
      "iteration: 0\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 1\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 2\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "momentum: 1.0\n",
      "True\n",
      "iteration: 0\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 1\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n",
      "iteration: 2\n",
      "check\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "weights gradients all close: False\n",
      "bias gradients all close: False\n",
      "betas all close: True\n",
      "gammas all close: True\n",
      "running_mean all close: True\n",
      "running_var all close: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BatchNorm test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 4\n",
    "n_channels = 5\n",
    "height = 8\n",
    "width = 8\n",
    "\n",
    "# Note: in \"eval\" gradients should not be calculated\n",
    "for phase in ['train', 'eval']:\n",
    "    print(f\"phase: {phase}\")\n",
    "    for momentum in [0.01, 0.8, 1.0]:\n",
    "        print(f\"momentum: {momentum}\")\n",
    "        torch_bn = torch.nn.BatchNorm2d(n_channels, momentum=momentum)\n",
    "        my_bn = BatchNormalization2d(n_channels, momentum=momentum)\n",
    "        if phase == 'test':\n",
    "            my_bn.eval()\n",
    "            torch_bn.eval()\n",
    "        else:\n",
    "            torch_bn.train()\n",
    "            my_bn.train()\n",
    "        \n",
    "\n",
    "        my_bn.gamma = torch_bn.weight.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "        my_bn.beta = torch_bn.bias.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "\n",
    "        my_bn.running_mean = torch_bn.running_mean.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "        my_bn.running_var = torch_bn.running_var.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "\n",
    "        my_bn.momentum = torch_bn.momentum = momentum\n",
    "\n",
    "        print(torch_bn.affine)\n",
    "\n",
    "\n",
    "        for iteration in range(3):\n",
    "            print(f\"iteration: {iteration}\")\n",
    "\n",
    "            input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "            input_data_torch = torch.from_numpy(input_data).float()\n",
    "            input_data_torch.requires_grad = True\n",
    "            output_gradient = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "\n",
    "            my_out = my_bn.forward(input_data)\n",
    "\n",
    "            torch_out = torch_bn(input_data_torch)\n",
    "\n",
    "            torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "            torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "\n",
    "            torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "            my_input_g = my_bn.backward(output_gradient)\n",
    "\n",
    "            \n",
    "\n",
    "            atol=1e-2 # ! setting to 1e-4 leads to more Fails. \n",
    "\n",
    "            print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "            print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "            print(\"weights gradients all close:\", np.allclose(my_bn.gamma_gradient, torch_bn.weight.grad.detach().numpy().reshape(1, n_channels, 1, 1), atol=atol))\n",
    "            print(\"bias gradients all close:\", np.allclose(my_bn.beta_gradient, torch_bn.bias.grad.detach().numpy().reshape(1, n_channels, 1, 1), atol=atol))\n",
    "            print(\"betas all close:\", np.allclose(my_bn.beta, torch_bn.bias.detach()))\n",
    "            print(\"gammas all close:\", np.allclose(my_bn.gamma, torch_bn.weight.detach()))\n",
    "            \n",
    "            running_mean_all_close = np.allclose(my_bn.running_mean.flatten(), torch_bn.running_mean.detach().numpy(), atol = atol)\n",
    "            running_var_all_close = np.allclose(my_bn.running_var.flatten(), torch_bn.running_var.detach().numpy(), atol = atol)\n",
    "\n",
    "            print(\"running_mean all close:\", running_mean_all_close)\n",
    "\n",
    "            if not running_mean_all_close:\n",
    "                print(\"my_bn.running_mean:\")\n",
    "                print(my_bn.running_mean)\n",
    "                print(\"torch_bn.running_mean:\")\n",
    "                print(torch_bn.running_mean.detach().numpy())\n",
    "\n",
    "            if not running_var_all_close:\n",
    "                print(\"my_bn.running_var:\")\n",
    "                print(my_bn.running_var)\n",
    "                print(\"torch_bn.running_var:\")\n",
    "                print(torch_bn.running_var.detach().numpy())\n",
    "\n",
    "\n",
    "            print(\"running_var all close:\", running_var_all_close)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90919991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(torch_bn.running_mean.detach().numpy(), my_bn.running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b49e091f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.52131367, 0.49844962, 0.5184468 , 0.51887125, 0.48160985],\n",
       "       dtype=float32),\n",
       " array([[[[0.52131367]],\n",
       " \n",
       "         [[0.49844962]],\n",
       " \n",
       "         [[0.5184468 ]],\n",
       " \n",
       "         [[0.51887125]],\n",
       " \n",
       "         [[0.48160982]]]], dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_bn.running_mean.detach().numpy(), my_bn.running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a75c6b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49844962"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bn.running_mean[0][1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35548e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4984496235847473"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_bn.running_mean[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf6bd6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]],\n",
       "\n",
       "        [[0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45b8f916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "793d2077",
   "metadata": {},
   "source": [
    "With any momentum, any phase and any input data and any matrix derivateive of cost function with respect to bn_output outputs and all gradients matchup. However, on other iterations gamma and beta gradients do not match up\n",
    "\n",
    "**gamma and beta gradients do not match up on any iteration except for the first one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca9fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "my_conv forward time: 0.5312726497650146\n",
      "my_conv_with_loops forward time: 2.215890645980835\n",
      "torch_conv forward time: 0.06669497489929199\n",
      "my_conv backward time: 0.7423074245452881\n",
      "my_conv_with_loops backward time: 2.638596773147583\n",
      "torch_conv backward time: 0.1759324073791504\n",
      "\n",
      "batch_size: 2\n",
      "my_conv forward time: 0.5586578845977783\n",
      "my_conv_with_loops forward time: 2.2504141330718994\n",
      "torch_conv forward time: 0.17436599731445312\n",
      "my_conv backward time: 1.2079191207885742\n",
      "my_conv_with_loops backward time: 5.122586965560913\n",
      "torch_conv backward time: 0.3848881721496582\n",
      "\n",
      "batch_size: 4\n",
      "my_conv forward time: 1.064781904220581\n",
      "my_conv_with_loops forward time: 2.2000110149383545\n",
      "torch_conv forward time: 0.1567244529724121\n",
      "my_conv backward time: 2.2330682277679443\n",
      "my_conv_with_loops backward time: 9.913957118988037\n",
      "torch_conv backward time: 0.3933844566345215\n",
      "\n",
      "batch_size: 8\n",
      "my_conv forward time: 1.9885218143463135\n",
      "my_conv_with_loops forward time: 2.341416597366333\n",
      "torch_conv forward time: 0.200042724609375\n",
      "my_conv backward time: 4.398773908615112\n",
      "my_conv_with_loops backward time: 20.821265697479248\n",
      "torch_conv backward time: 0.4874694347381592\n",
      "\n",
      "batch_size: 16\n",
      "my_conv forward time: 4.403775453567505\n",
      "my_conv_with_loops forward time: 3.899850606918335\n",
      "torch_conv forward time: 0.6100683212280273\n",
      "my_conv backward time: 10.690370321273804\n",
      "my_conv_with_loops backward time: 45.32488679885864\n",
      "torch_conv backward time: 0.542504072189331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d vs Conv2dWithLoops vs torch.nn.Conv2d time comparison forward and backward\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 3\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "for batch_size in [1, 2, 4, 8, 16]:\n",
    "    print(\"batch_size:\", batch_size)\n",
    "\n",
    "    input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "    torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "    my_conv_with_loops = Conv2dWithLoops(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv_with_loops.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    n_iterations = 1000\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out = my_conv.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out_with_loops = my_conv_with_loops.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out = torch_conv(input_data_torch)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g = my_conv.backward_as_matrix_multiplication(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g_with_loops = my_conv_with_loops.backward(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv backward time: {end - start}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07955d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bbf79ea567ebf64b92b9cf68f2b08a0cf8db5ff8a7f29f56d99d802810464d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
