{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "427c3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to project's root directory to make\n",
    "# numpy_nn and pytorch_nn packages accessable \n",
    "# %cd ../..\n",
    "\n",
    "\n",
    "# # Another possible solution is appending to the sys.path\n",
    "\n",
    "import sys\n",
    "import  os\n",
    "project_root = os.path.dirname(os.path.dirname(sys.path[0]))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a56421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SystemPoint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from test_layer import test_module\n",
    "\n",
    "from numpy_nn.modules.np_nn import (\n",
    "    FullyConnectedLayer,\n",
    "    ReLULayer,\n",
    "    SigmoidLayer,\n",
    "    ReLULayer,\n",
    "    AdamOptimizer,\n",
    "    CrossEntropyLoss,\n",
    "    LinearActivation,\n",
    "    Sequential,\n",
    "    Optimizer,\n",
    "    SoftMaxLayer,\n",
    "    GradientDescentOptimizer,\n",
    "    CrossEntropyLossWithSoftMax,\n",
    "    Conv2d,\n",
    "    Conv2dWithLoops,\n",
    "    Flatten,\n",
    "    MaxPool2d,\n",
    "    AdamOptimizer,\n",
    "    BatchNormalization2d,\n",
    "    ActivationLayer,\n",
    "    TrainableLayer,\n",
    ")\n",
    "\n",
    "from pytorch_nn.models.resnet import (\n",
    "    Bottleneck as Bottleneck_torch,\n",
    "    resnet101 as resnet101_torch\n",
    ")\n",
    "\n",
    "from pytorch_nn.models.resnet_without_batchnorm import (\n",
    "    Bottleneck as Bottleneck_torch_without_batchnorm,\n",
    "    resnet101 as resnet101_torch_without_batchnorm\n",
    ")\n",
    "\n",
    "from numpy_nn.models.resnet import Bottleneck, resnet101\n",
    "\n",
    "from numpy_nn.models.resnet_without_batchnorm import (\n",
    "    Bottleneck as Bottleneck_np_without_batchnorm,\n",
    "    resnet101 as resnet101_np_without_batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38f73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload a user's module test_layer\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87c80e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_test(my_conv2d_constructor: Callable, batch_size: int,\n",
    "                input_height: int, input_width: int, n_input_channels,\n",
    "                n_output_channels, kernel_size: int, stride: int, padding: int,\n",
    "                bias: bool, atol: float = 1e-5, random_sampler: Callable = np.random.rand,\n",
    "                print_tensors: bool = False, print_results: bool = False) -> None:\n",
    "    \n",
    "    my_conv2d_kwargs = torch_conv2d_kwargs = {\n",
    "        \"in_channels\": n_input_channels,\n",
    "        \"out_channels\": n_output_channels,\n",
    "        \"kernel_size\": kernel_size,\n",
    "        \"stride\": stride,\n",
    "        \"padding\": padding,\n",
    "        \"bias\": bias\n",
    "    }\n",
    "\n",
    "    output_height = (input_height + 2 * padding - kernel_size) // stride + 1\n",
    "    output_width = (input_width + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "    input_shape = (batch_size, n_input_channels, input_height, input_width)\n",
    "    output_shape = (batch_size, n_output_channels, output_height, output_width)\n",
    "\n",
    "    my_conv2d = my_conv2d_constructor(**my_conv2d_kwargs)\n",
    "    torch_conv2d = torch.nn.Conv2d(**torch_conv2d_kwargs)\n",
    "\n",
    "    test_module(my_conv2d, torch_conv2d, input_shape,\n",
    "                output_shape, atol=atol, random_sampler=random_sampler,\n",
    "                print_tensors=print_tensors, print_results=print_results)\n",
    "    \n",
    "\n",
    "\n",
    "def max_pool_2d_test(batch_size: int, height: int, width: int, n_channels: int,\n",
    "                     kernel_size: int, stride: int, padding: int, atol: float = 1e-5,\n",
    "                     random_sampler: Callable = np.random.rand, print_tensors: bool = False,\n",
    "                     print_results: bool = False):\n",
    "    \n",
    "    output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "    output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "    my_pool_args = torch_pool_args = [kernel_size, stride, padding]\n",
    "\n",
    "    my_pool = MaxPool2d(*my_pool_args)\n",
    "    torch_pool = torch.nn.MaxPool2d(*torch_pool_args)\n",
    "\n",
    "    test_module(my_pool,\n",
    "                torch_pool,\n",
    "                input_shape=[batch_size, n_channels, height, width],\n",
    "                output_shape=[batch_size, n_channels, output_height, output_width],\n",
    "                atol=atol,\n",
    "                random_sampler = random_sampler,\n",
    "                print_tensors=print_tensors,\n",
    "                print_results=print_results)\n",
    "\n",
    "\n",
    "\n",
    "def activation_test(my_activation: Callable,\n",
    "                    torch_activation: Callable,\n",
    "                    input_dim: List[int],\n",
    "                    atol: float = 1e-5,\n",
    "                    random_sampler: Callable = np.random.rand,\n",
    "                    print_tensors: bool = False,\n",
    "                    print_results: bool = False):\n",
    "    \"\"\"\n",
    "    Samples input data and output gradient from a uniform\n",
    "    distribution and tests if the output and input gradients\n",
    "    are close to the ones computed by pytorch\n",
    "    \"\"\"\n",
    "    test_module(my_activation(), torch_activation(),\n",
    "                input_shape=input_dim, output_shape=input_dim,\n",
    "                atol=atol, random_sampler=random_sampler,\n",
    "                print_tensors=print_tensors, print_results=print_results)\n",
    "\n",
    "def relu_test(input_dim: List[int], atol: float = 1e-5,\n",
    "              random_sampler: Callable = np.random.rand,\n",
    "              print_tensors: bool = False, print_results: bool = False):\n",
    "    activation_test(ReLULayer, torch.nn.ReLU, input_dim,\n",
    "                    atol=atol, random_sampler=random_sampler,\n",
    "                    print_tensors=print_tensors, print_results=print_results)\n",
    "\n",
    "def sigmoid_test(input_dim: List[int], atol: float = 1e-5,\n",
    "                 random_sampler: Callable = np.random.rand,\n",
    "                 print_tensors: bool = False, print_results: bool = False):\n",
    "    activation_test(SigmoidLayer, torch.nn.Sigmoid, input_dim,\n",
    "                    atol=atol, random_sampler=random_sampler,\n",
    "                    print_tensors=print_tensors, print_results=print_results)\n",
    "\n",
    "\n",
    "\n",
    "def flatten_test(input_shape: List[int], atol: float = 1e-5,\n",
    "                 random_sampler: Callable = np.random.rand,\n",
    "                 print_tensors: bool = False, print_results: bool = False):\n",
    "    \n",
    "    batch_size, *rest_input_dim = input_shape\n",
    "    output_shape = [batch_size, np.prod(rest_input_dim)]\n",
    "\n",
    "    test_module(Flatten(), torch.nn.Flatten(), input_shape=input_shape,\n",
    "               output_shape=output_shape, atol=atol, random_sampler=random_sampler,\n",
    "               print_tensors=print_tensors, print_results=print_results)\n",
    "\n",
    "\n",
    "\n",
    "def batchnorm2d_iterative_test(n_channels: int, batch_size: int, height: int,\n",
    "                               width: int, n_iter: int, phase: str = \"train\",\n",
    "                               momentum: float = 0.1, atol: float = 1e-5,\n",
    "                               random_sampler: Callable = np.random.rand,\n",
    "                               print_tensors = False, print_results = False) -> None:\n",
    "            \n",
    "    # since each iteration changes the running mean, running variance, mean and variance\n",
    "    # this test runs several iterations and checks that the results are the same\n",
    "\n",
    "    # maybe there should be a custom test that calls\n",
    "    # forward multiple times and calls backward only once\n",
    "    \n",
    "    input_shape = output_shape = [batch_size, n_channels, height, width]\n",
    "\n",
    "    my_bn = BatchNormalization2d(n_channels, momentum=momentum)\n",
    "    torch_bn = torch.nn.BatchNorm2d(n_channels, momentum=momentum)\n",
    "\n",
    "    if phase == \"eval\":\n",
    "        my_bn.eval()\n",
    "        torch_bn.eval()\n",
    "    else:\n",
    "        my_bn.train()\n",
    "        torch_bn.train()\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        print(\"Iteration:\", i)\n",
    "        # print(my_bn.training)\n",
    "        test_module(my_bn, torch_bn, input_shape, output_shape,\n",
    "                    atol=atol, random_sampler=random_sampler,\n",
    "                    skip_parameter_copying=bool(i),\n",
    "                    print_tensors=print_tensors,\n",
    "                    print_results=print_results)\n",
    "        # reset torch gradients\n",
    "        torch_bn.weight.grad = None\n",
    "        torch_bn.bias.grad = None\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bd5af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atol = 1e-06, np.random.randn\n",
      "Iteration: 0\n",
      "\n",
      "Iteration: 1\n",
      "\n",
      "Iteration: 2\n",
      "\n",
      "Iteration: 3\n",
      "\n",
      "Iteration: 4\n",
      "\n",
      "Iteration: 5\n",
      "\n",
      "Iteration: 6\n",
      "\n",
      "Iteration: 7\n",
      "\n",
      "Iteration: 8\n",
      "\n",
      "Iteration: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atol = 1e-6\n",
    "print(f\"atol = {atol}, np.random.randn\")\n",
    "batchnorm2d_iterative_test(n_channels=5, batch_size=4, height=8, width=8,\n",
    "                           n_iter=10, phase=\"train\", momentum=0.1,\n",
    "                           atol=atol, random_sampler=np.random.randn, print_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "059674e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atol = 0.0001, np.random.rand\n",
      "Iteration: 0\n",
      "\n",
      "Iteration: 1\n",
      "\n",
      "Iteration: 2\n",
      "\n",
      "Iteration: 3\n",
      "\n",
      "Iteration: 4\n",
      "\n",
      "Iteration: 5\n",
      "\n",
      "Iteration: 6\n",
      "\n",
      "Iteration: 7\n",
      "\n",
      "Iteration: 8\n",
      "\n",
      "Iteration: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atol = 1e-4\n",
    "print(f\"atol = {atol}, np.random.rand\")\n",
    "batchnorm2d_iterative_test(n_channels=5, batch_size=4, height=8, width=8,\n",
    "                           n_iter=10, phase=\"train\", momentum=0.8,\n",
    "                           atol=atol, random_sampler=np.random.rand, print_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a0bd846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atol = 1e-05, np.random.rand\n",
      "Iteration: 0\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Running means are equal\n",
      "Running vars are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n",
      "Iteration: 1\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Running means are equal\n",
      "Running vars are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n",
      "Iteration: 2\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Running means are equal\n",
      "Running vars are equal\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Weight gradients are not equal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m atol \u001b[39m=\u001b[39m \u001b[39m1e-5\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39matol = \u001b[39m\u001b[39m{\u001b[39;00matol\u001b[39m}\u001b[39;00m\u001b[39m, np.random.rand\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m batchnorm2d_iterative_test(n_channels\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, height\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, width\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                            n_iter\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, phase\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, momentum\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                            atol\u001b[39m=\u001b[39;49matol, random_sampler\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrand, print_results\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 7\u001b[0m in \u001b[0;36mbatchnorm2d_iterative_test\u001b[1;34m(n_channels, batch_size, height, width, n_iter, phase, momentum, atol, random_sampler, print_tensors, print_results)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIteration:\u001b[39m\u001b[39m\"\u001b[39m, i)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39m# print(my_bn.training)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m test_module(my_bn, torch_bn, input_shape, output_shape,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m             atol\u001b[39m=\u001b[39;49matol, random_sampler\u001b[39m=\u001b[39;49mrandom_sampler,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m             skip_parameter_copying\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(i),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m             print_tensors\u001b[39m=\u001b[39;49mprint_tensors,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m             print_results\u001b[39m=\u001b[39;49mprint_results)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39m# reset torch gradients\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X50sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m torch_bn\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\test_layer.py:145\u001b[0m, in \u001b[0;36mtest_module\u001b[1;34m(my_module, torch_module, input_shape, output_shape, atol, random_sampler, skip_parameter_copying, print_tensors, print_results)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmy and torch weight gradients:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m     \u001b[39mprint\u001b[39m(weight_grad_np\u001b[39m.\u001b[39mflatten(), weight_grad_torch\u001b[39m.\u001b[39mflatten())\n\u001b[1;32m--> 145\u001b[0m \u001b[39massert\u001b[39;00m weight_grads_close, \u001b[39m\"\u001b[39m\u001b[39mWeight gradients are not equal\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m print_results:\n\u001b[0;32m    147\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWeight gradients are equal\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Weight gradients are not equal"
     ]
    }
   ],
   "source": [
    "atol = 1e-5\n",
    "print(f\"atol = {atol}, np.random.rand\")\n",
    "batchnorm2d_iterative_test(n_channels=5, batch_size=4, height=8, width=8,\n",
    "                           n_iter=10, phase=\"train\", momentum=0.8,\n",
    "                           atol=atol, random_sampler=np.random.rand, print_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f54353c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2dWithLoops test\n",
      "passed\n",
      "\n",
      "Conv2d test\n",
      "passed\n",
      "\n",
      "Conv2d test without bias\n",
      "passed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2dWithLoops and Conv2d tests\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "print(\"Conv2dWithLoops test\")\n",
    "conv2d_test(Conv2dWithLoops, batch_size, height, width,\n",
    "            n_input_channels, n_output_channels, kernel_size,\n",
    "            stride, padding, bias=True, atol=1e-6, random_sampler=np.random.randn)\n",
    "\n",
    "conv2d_test(Conv2dWithLoops, batch_size, height, width,\n",
    "            n_input_channels, n_output_channels, kernel_size,\n",
    "            stride, padding, bias=True, atol=1e-6, random_sampler=np.random.rand)\n",
    "print(\"passed\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Conv2d test\")\n",
    "conv2d_test(Conv2d, batch_size, height, width, n_input_channels, n_output_channels,\n",
    "            kernel_size, stride, padding, bias=True, atol=1e-6, random_sampler=np.random.randn)\n",
    "conv2d_test(Conv2d, batch_size, height, width, n_input_channels, n_output_channels,\n",
    "            kernel_size, stride, padding, bias=True, atol=1e-6, random_sampler=np.random.rand)\n",
    "print(\"passed\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Conv2d test without bias\")\n",
    "conv2d_test(Conv2d, batch_size, height, width, n_input_channels, n_output_channels,\n",
    "            kernel_size, stride, padding, bias=False, atol=1e-6, random_sampler=np.random.randn)\n",
    "conv2d_test(Conv2d, batch_size, height, width, n_input_channels, n_output_channels,\n",
    "            kernel_size, stride, padding, bias=False, atol=1e-6, random_sampler=np.random.rand)\n",
    "print(\"passed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af989443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FullyConnectedLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_neurons = 6\n",
    "n_output_neurons = 3\n",
    "n_samples = 5\n",
    "\n",
    "my_fc_params = torch_fc_params = [n_input_neurons, n_output_neurons]\n",
    "\n",
    "test_module(FullyConnectedLayer(*my_fc_params),\n",
    "            torch.nn.Linear(*torch_fc_params),\n",
    "            input_shape=[n_samples, n_input_neurons],\n",
    "            output_shape=[n_samples, n_output_neurons],\n",
    "            atol=1e-6,\n",
    "            random_sampler=np.random.randn)\n",
    "\n",
    "test_module(FullyConnectedLayer(*my_fc_params),\n",
    "            torch.nn.Linear(*torch_fc_params),\n",
    "            input_shape=[n_samples, n_input_neurons],\n",
    "            output_shape=[n_samples, n_output_neurons],\n",
    "            atol=1e-6,\n",
    "            random_sampler=np.random.rand)\n",
    "\n",
    "print(\"passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ba7f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_val all close: True\n",
      "loss gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CrossEntropyLoss test random.rand\n",
    "\"\"\"\n",
    "\n",
    "def one_hot(y: np.ndarray, n_classes: int):\n",
    "    encoded = np.zeros((y.size, n_classes))\n",
    "    encoded[np.arange(y.size), y] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "n_classes = 3\n",
    "pred = np.random.rand(batch_size, n_classes).astype(np.float32)\n",
    "true = one_hot(np.random.randint(0, n_classes, batch_size), n_classes)\n",
    "pred_torch = torch.from_numpy(pred).float()\n",
    "true_torch = torch.from_numpy(true).float()\n",
    "pred_torch.requires_grad = True\n",
    "\n",
    "torch_loss  = torch.nn.CrossEntropyLoss()\n",
    "torch_loss_val = torch_loss(pred_torch, true_torch)\n",
    "torch_loss_val.backward()\n",
    "\n",
    "my_loss = CrossEntropyLossWithSoftMax()\n",
    "my_loss_val = my_loss.forward(pred, true)\n",
    "my_loss.backward()\n",
    "\n",
    "print(\"loss_val all close:\", np.allclose(my_loss_val, torch_loss_val.detach().numpy()))\n",
    "print(\"loss gradients all close:\", np.allclose(my_loss.backward(), pred_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a0aaf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_val all close: True\n",
      "loss gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CrossEntropyLoss test random.randn\n",
    "\"\"\"\n",
    "\n",
    "def one_hot(y: np.ndarray, n_classes: int):\n",
    "    encoded = np.zeros((y.size, n_classes))\n",
    "    encoded[np.arange(y.size), y] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "n_classes = 3\n",
    "pred = np.random.randn(batch_size, n_classes).astype(np.float32)\n",
    "true = one_hot(np.random.randint(0, n_classes, batch_size), n_classes)\n",
    "pred_torch = torch.from_numpy(pred).float()\n",
    "true_torch = torch.from_numpy(true).float()\n",
    "pred_torch.requires_grad = True\n",
    "\n",
    "torch_loss  = torch.nn.CrossEntropyLoss()\n",
    "torch_loss_val = torch_loss(pred_torch, true_torch)\n",
    "torch_loss_val.backward()\n",
    "\n",
    "my_loss = CrossEntropyLossWithSoftMax()\n",
    "my_loss_val = my_loss.forward(pred, true)\n",
    "my_loss.backward()\n",
    "\n",
    "print(\"loss_val all close:\", np.allclose(my_loss_val, torch_loss_val.detach().numpy()))\n",
    "print(\"loss gradients all close:\", np.allclose(my_loss.backward(), pred_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6887e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ReLU test\"\"\"\n",
    "n_input_features = 6\n",
    "n_samples = 5\n",
    "\n",
    "relu_test([n_samples, n_input_features], random_sampler=np.random.randn)\n",
    "relu_test([n_samples, n_input_features], random_sampler=np.random.rand)\n",
    "print(\"passed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0083991a",
   "metadata": {},
   "source": [
    "If we sample random numbers from normanl distribution instead of uniform distribution (randn instead of rand) sigmoid layer won't pass tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "481886fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed with atol=1e-10 and random_sampler=rand\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Outputs are not equal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m atol \u001b[39m=\u001b[39m \u001b[39m1e-1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m random_sampler \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m sigmoid_test([n_samples, n_input_features], print_results\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, atol\u001b[39m=\u001b[39;49matol, random_sampler\u001b[39m=\u001b[39;49mrandom_sampler)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpassed with atol=\u001b[39m\u001b[39m{\u001b[39;00matol\u001b[39m}\u001b[39;00m\u001b[39m and random_sampler=\u001b[39m\u001b[39m{\u001b[39;00mrandom_sampler\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m sigmoid_test([n_samples, n_input_features, height, width],  print_results\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, atol\u001b[39m=\u001b[39matol, random_sampler\u001b[39m=\u001b[39mrandom_sampler)\n",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 14\u001b[0m in \u001b[0;36msigmoid_test\u001b[1;34m(input_dim, atol, random_sampler, print_tensors, print_results)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msigmoid_test\u001b[39m(input_dim: List[\u001b[39mint\u001b[39m], atol: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1e-5\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m                  random_sampler: Callable \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m                  print_tensors: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, print_results: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     activation_test(SigmoidLayer, torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mSigmoid, input_dim,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m                     atol\u001b[39m=\u001b[39;49matol, random_sampler\u001b[39m=\u001b[39;49mrandom_sampler,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m                     print_tensors\u001b[39m=\u001b[39;49mprint_tensors, print_results\u001b[39m=\u001b[39;49mprint_results)\n",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 14\u001b[0m in \u001b[0;36mactivation_test\u001b[1;34m(my_activation, torch_activation, input_dim, atol, random_sampler, print_tensors, print_results)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mactivation_test\u001b[39m(my_activation: Callable,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m                     torch_activation: Callable,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                     input_dim: List[\u001b[39mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m                     print_tensors: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m                     print_results: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m    Samples input data and output gradient from a uniform\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m    distribution and tests if the output and input gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m    are close to the ones computed by pytorch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     test_module(my_activation(), torch_activation(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m                 input_shape\u001b[39m=\u001b[39;49minput_dim, output_shape\u001b[39m=\u001b[39;49minput_dim,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m                 atol\u001b[39m=\u001b[39;49matol, random_sampler\u001b[39m=\u001b[39;49mrandom_sampler,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m                 print_tensors\u001b[39m=\u001b[39;49mprint_tensors, print_results\u001b[39m=\u001b[39;49mprint_results)\n",
      "File \u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\test_layer.py:79\u001b[0m, in \u001b[0;36mtest_module\u001b[1;34m(my_module, torch_module, input_shape, output_shape, atol, random_sampler, skip_parameter_copying, print_tensors, print_results)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmy and torch outputs:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m     \u001b[39mprint\u001b[39m(output_np\u001b[39m.\u001b[39mflatten(), output_torch\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mflatten())\n\u001b[1;32m---> 79\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mallclose(output_np, output_torch\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), atol\u001b[39m=\u001b[39matol), \u001b[39m\"\u001b[39m\u001b[39mOutputs are not equal\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m print_results:\n\u001b[0;32m     81\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutputs are equal\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Outputs are not equal"
     ]
    }
   ],
   "source": [
    "\"\"\"SigmoidLayer test\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_samples = 5\n",
    "height = 4\n",
    "width = 4\n",
    "\n",
    "atol = 1e-10\n",
    "random_sampler = np.random.rand\n",
    "sigmoid_test([n_samples, n_input_features],  atol=atol, random_sampler=random_sampler)\n",
    "sigmoid_test([n_samples, n_input_features, height, width], atol=atol, random_sampler=random_sampler)\n",
    "print(f\"passed with atol={atol} and random_sampler={random_sampler.__name__}\")\n",
    "\n",
    "\n",
    "atol = 1e-1\n",
    "random_sampler = np.random.randn\n",
    "sigmoid_test([n_samples, n_input_features], print_results=True, atol=atol, random_sampler=random_sampler)\n",
    "print(f\"passed with atol={atol} and random_sampler={random_sampler.__name__}\")\n",
    "sigmoid_test([n_samples, n_input_features, height, width],  print_results=True, atol=atol, random_sampler=random_sampler)\n",
    "print(f\"passed with atol={atol} and random_sampler={random_sampler.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "efef87a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my and torch outputs:\n",
      "[0.5        0.5        0.5        0.5        0.5        0.5\n",
      " 0.672545   0.5        0.68394905 0.5829011  0.74580723 0.6801012\n",
      " 0.5919642  0.5        0.70637125 0.87413114 0.5111798  0.55161756\n",
      " 0.5        0.5        0.5        0.69325364 0.5        0.7582325\n",
      " 0.5642382  0.9157735  0.55885965 0.5        0.6037557  0.7547886 ] [0.28056666 0.35525218 0.3972644  0.4504413  0.47803488 0.32270715\n",
      " 0.672545   0.4808238  0.68394905 0.5829011  0.74580723 0.6801012\n",
      " 0.5919642  0.34749055 0.70637125 0.87413114 0.5111798  0.55161756\n",
      " 0.23294462 0.2770023  0.4305207  0.69325364 0.08641024 0.7582325\n",
      " 0.5642382  0.9157735  0.55885965 0.37466085 0.6037557  0.7547886 ]\n",
      "Outputs are equal\n",
      "my and torch input gradients:\n",
      "[ 0.23947466  0.25712623 -0.2722926   0.13596702  0.00429849  0.27555423\n",
      "  0.19052285 -0.20012388 -0.33675596 -0.03148636 -0.08271011 -0.24591815\n",
      " -0.44671371 -0.17627662 -0.18255537  0.07124901  0.04899441 -0.15259153\n",
      " -0.42602992  0.42814883  0.05128613  0.04286068 -0.22957445  0.14710533\n",
      " -0.15079796 -0.05559433 -0.0408535   0.19665086 -0.0349247  -0.035794  ] [ 0.19335088  0.23557708 -0.26079684  0.13463126  0.00429019  0.2409085\n",
      "  0.19052285 -0.19982953 -0.33675596 -0.03148636 -0.0827101  -0.24591815\n",
      " -0.44671372 -0.15987645 -0.18255536  0.07124901  0.04899441 -0.15259153\n",
      " -0.30449453  0.34298494  0.05029582  0.04286068 -0.07249365  0.14710534\n",
      " -0.15079795 -0.05559433 -0.0408535   0.1842934  -0.0349247  -0.035794  ]\n",
      "Input gradients are equal\n",
      "passed with atol=1 and random_sampler=randn\n",
      "my and torch outputs:\n",
      "[0.5        0.7249863  0.6366352  0.5412508  0.5        0.69917184\n",
      " 0.598446   0.8101272  0.5        0.528664   0.5        0.7036457\n",
      " 0.5        0.5        0.55640244 0.6848723  0.6924315  0.5\n",
      " 0.5        0.83230567 0.5        0.5        0.5        0.5428607\n",
      " 0.5        0.6737144  0.5        0.61594874 0.5        0.5\n",
      " 0.5        0.8551244  0.64492947 0.5        0.5        0.55973035\n",
      " 0.5        0.62111795 0.5        0.5        0.66999674 0.81591016\n",
      " 0.56626683 0.6770009  0.5        0.5        0.59579855 0.7662768\n",
      " 0.65731573 0.7749484  0.5        0.72690076 0.5        0.5\n",
      " 0.5        0.81372076 0.6651162  0.61998284 0.5374441  0.5\n",
      " 0.5        0.54626226 0.5347439  0.5        0.5074822  0.5\n",
      " 0.78972775 0.5        0.9181213  0.7841157  0.5        0.5\n",
      " 0.5        0.5        0.5929512  0.6849454  0.5        0.5\n",
      " 0.5        0.6592191  0.7807743  0.5        0.5        0.5\n",
      " 0.5        0.64414126 0.51957226 0.5        0.5        0.5\n",
      " 0.7924957  0.5        0.6628663  0.58303666 0.5        0.7173741\n",
      " 0.5        0.7731564  0.5        0.5        0.5950295  0.5943901\n",
      " 0.5        0.91303205 0.5416225  0.5        0.692018   0.79197896\n",
      " 0.5        0.5        0.6019779  0.5907526  0.7725815  0.5738188\n",
      " 0.6108461  0.5882937  0.8505417  0.54272777 0.7223658  0.62142295\n",
      " 0.6383754  0.501177   0.5        0.5        0.5        0.5\n",
      " 0.5        0.5        0.66004413 0.5        0.5208253  0.53973246\n",
      " 0.6155098  0.5        0.5        0.6995746  0.7463195  0.5\n",
      " 0.5        0.51948214 0.64219767 0.5        0.5        0.5400461\n",
      " 0.5        0.54081017 0.55703086 0.6720402  0.66067827 0.5\n",
      " 0.5        0.51237494 0.5        0.5        0.6764903  0.51051503\n",
      " 0.6631435  0.5616352  0.8687183  0.5        0.5785297  0.9048648\n",
      " 0.60654604 0.9069467  0.6075404  0.5        0.68750966 0.5\n",
      " 0.5        0.74086875 0.59144753 0.84979755 0.6928315  0.5\n",
      " 0.5        0.82104766 0.77224684 0.6348728  0.5        0.5\n",
      " 0.90420955 0.5        0.5        0.5        0.83570874 0.5\n",
      " 0.5        0.72493917 0.5        0.5        0.7788484  0.5\n",
      " 0.5        0.5        0.5        0.5        0.576321   0.60284394\n",
      " 0.5        0.5        0.5        0.5        0.6254611  0.5\n",
      " 0.51012343 0.5        0.69342    0.7820094  0.5        0.5\n",
      " 0.57637733 0.5        0.5        0.80734754 0.5        0.5\n",
      " 0.5        0.5915622  0.5        0.5        0.6290685  0.5\n",
      " 0.60677844 0.5        0.5        0.5        0.5        0.5\n",
      " 0.6627621  0.5116866  0.5        0.6119213  0.5        0.51600546\n",
      " 0.5        0.5        0.76333916 0.5        0.5        0.7173719\n",
      " 0.65589833 0.7124779  0.5        0.784632   0.5        0.6049008\n",
      " 0.5        0.6696467  0.7820301  0.5765767  0.5        0.6163943\n",
      " 0.5        0.5        0.57453424 0.5        0.5        0.71419674\n",
      " 0.5        0.5        0.73252445 0.5        0.5643106  0.63521343\n",
      " 0.5        0.745237   0.5        0.6799308  0.5        0.5719378\n",
      " 0.5        0.7309358  0.7328127  0.7219978  0.5        0.5\n",
      " 0.5285343  0.6476072  0.5        0.893069   0.6126328  0.5\n",
      " 0.6980624  0.5        0.5        0.50655943 0.5064713  0.5\n",
      " 0.5        0.7564249  0.5        0.5        0.5400369  0.5\n",
      " 0.5        0.5        0.5        0.5        0.5        0.84096706\n",
      " 0.5        0.5946064  0.5        0.5        0.6280099  0.6676349\n",
      " 0.5        0.5544897  0.7140875  0.92273575 0.5        0.5\n",
      " 0.5        0.5        0.9065336  0.6003981  0.8615523  0.5\n",
      " 0.5        0.76199347 0.5        0.5        0.5        0.65300816\n",
      " 0.5        0.5136779  0.5152971  0.5        0.5        0.7099572\n",
      " 0.74387187 0.5256815  0.5        0.5        0.5        0.6660862\n",
      " 0.74423236 0.6721339  0.5        0.7153163  0.75599664 0.5\n",
      " 0.715473   0.6891533  0.6365873  0.5        0.52843535 0.78409046\n",
      " 0.8317293  0.6160657  0.62970656 0.74026054 0.5        0.5\n",
      " 0.6195824  0.5        0.5        0.80728495 0.5        0.6868998\n",
      " 0.63772005 0.5614413  0.5        0.54311645 0.5        0.5\n",
      " 0.5        0.8578128  0.5        0.5        0.5        0.7751933\n",
      " 0.5        0.5        0.58040863 0.5        0.5        0.5\n",
      " 0.5        0.5        0.5        0.5        0.5        0.51709235\n",
      " 0.9200202  0.70723546 0.5        0.6080052  0.5        0.5\n",
      " 0.5        0.5        0.6047831  0.71085566 0.83364916 0.5\n",
      " 0.6107819  0.5        0.6266222  0.7221516  0.5316333  0.5\n",
      " 0.72908306 0.8620567  0.5        0.7523823  0.58577514 0.5\n",
      " 0.5        0.54953337 0.5        0.5        0.5950116  0.51886386\n",
      " 0.5        0.5        0.5        0.8211582  0.6602638  0.51362324\n",
      " 0.5        0.62819165 0.5        0.5        0.5875233  0.5\n",
      " 0.5        0.5        0.52105945 0.65409476 0.64147645 0.6172864\n",
      " 0.5        0.5        0.5        0.5        0.5        0.5\n",
      " 0.5        0.50368595 0.63083917 0.5        0.8585174  0.8204143\n",
      " 0.6747846  0.6194516  0.6835213  0.5        0.5        0.5\n",
      " 0.6637865  0.5        0.7357559  0.5        0.5        0.5\n",
      " 0.9406553  0.72905475 0.5        0.5        0.7100714  0.5\n",
      " 0.86612886 0.5        0.5        0.7370543  0.61266637 0.5\n",
      " 0.6037361  0.84553885 0.71009386 0.5485415  0.5        0.5\n",
      " 0.6093403  0.7271276  0.5        0.5        0.5        0.5       ] [0.29389122 0.7249863  0.6366352  0.5412508  0.17056084 0.69917184\n",
      " 0.598446   0.8101272  0.33493993 0.528664   0.41140556 0.7036457\n",
      " 0.42852366 0.21382622 0.55640244 0.6848723  0.6924315  0.3676271\n",
      " 0.39629105 0.83230567 0.40230414 0.36965695 0.36472532 0.5428607\n",
      " 0.49086344 0.6737144  0.44718856 0.6159488  0.20312488 0.3399909\n",
      " 0.36936125 0.8551244  0.64492947 0.3274742  0.46744972 0.55973035\n",
      " 0.23893812 0.62111795 0.42948753 0.16051006 0.66999674 0.81591016\n",
      " 0.56626683 0.6770009  0.21758716 0.28482488 0.59579855 0.7662768\n",
      " 0.65731573 0.7749484  0.3386997  0.72690076 0.25149345 0.43965143\n",
      " 0.14244537 0.81372076 0.6651162  0.61998284 0.5374441  0.27496982\n",
      " 0.39187223 0.5462622  0.5347439  0.40771255 0.5074822  0.276617\n",
      " 0.78972775 0.054997   0.9181213  0.7841157  0.3525104  0.27197966\n",
      " 0.17510456 0.44778112 0.5929512  0.6849454  0.23474254 0.3064437\n",
      " 0.45018172 0.6592191  0.7807743  0.20234936 0.4836593  0.41123185\n",
      " 0.4077062  0.6441412  0.51957226 0.43633398 0.4394756  0.11189184\n",
      " 0.7924957  0.2124843  0.6628663  0.58303666 0.16853313 0.7173741\n",
      " 0.43861222 0.7731564  0.4565678  0.4305655  0.5950295  0.5943901\n",
      " 0.3091931  0.91303205 0.5416225  0.14694583 0.692018   0.79197896\n",
      " 0.44124678 0.48308167 0.6019778  0.5907526  0.7725815  0.5738188\n",
      " 0.6108461  0.5882936  0.8505417  0.54272777 0.7223658  0.62142295\n",
      " 0.6383754  0.501177   0.24599724 0.4942263  0.33451694 0.15155256\n",
      " 0.32501554 0.20233482 0.66004413 0.3499802  0.52082527 0.5397325\n",
      " 0.6155098  0.47307587 0.4386469  0.6995746  0.7463195  0.39564994\n",
      " 0.369874   0.51948214 0.64219767 0.46694916 0.37019038 0.5400461\n",
      " 0.29498303 0.5408101  0.55703086 0.6720402  0.66067827 0.4445311\n",
      " 0.487757   0.51237494 0.34877825 0.39470127 0.6764903  0.51051503\n",
      " 0.6631435  0.5616352  0.8687183  0.25343603 0.5785297  0.9048648\n",
      " 0.60654604 0.9069467  0.60754037 0.45148978 0.68750966 0.44177967\n",
      " 0.3642053  0.74086875 0.59144753 0.84979755 0.6928315  0.3704759\n",
      " 0.25602165 0.82104766 0.77224684 0.6348728  0.2651947  0.26622987\n",
      " 0.90420955 0.37530738 0.35550612 0.28295892 0.83570874 0.21843852\n",
      " 0.49236456 0.72493917 0.26136306 0.3597146  0.7788484  0.29176357\n",
      " 0.1294825  0.3720914  0.24408503 0.44167677 0.57632095 0.60284394\n",
      " 0.48313704 0.44680792 0.39377752 0.46489602 0.6254611  0.48172107\n",
      " 0.51012343 0.42268038 0.69342    0.7820094  0.08447166 0.33611566\n",
      " 0.5763773  0.47213918 0.26503885 0.80734754 0.38605672 0.36547068\n",
      " 0.24887016 0.5915622  0.43990517 0.31945565 0.6290685  0.38192776\n",
      " 0.60677844 0.2843161  0.10445937 0.42499667 0.28044027 0.26740393\n",
      " 0.6627621  0.51168656 0.4931899  0.6119213  0.4376387  0.51600546\n",
      " 0.25365052 0.41236866 0.7633392  0.30361187 0.33634865 0.7173719\n",
      " 0.65589833 0.7124779  0.4681982  0.784632   0.44539705 0.6049008\n",
      " 0.36330268 0.6696467  0.7820301  0.57657665 0.474368   0.6163943\n",
      " 0.2494659  0.12109013 0.57453424 0.39191037 0.4094155  0.71419674\n",
      " 0.22193362 0.31367138 0.7325244  0.4952775  0.5643106  0.63521343\n",
      " 0.38606185 0.745237   0.12592268 0.6799308  0.44305882 0.5719378\n",
      " 0.1642382  0.73093575 0.7328127  0.7219978  0.48514718 0.42049763\n",
      " 0.5285343  0.6476072  0.2964452  0.893069   0.6126328  0.2594356\n",
      " 0.6980624  0.19374356 0.2826904  0.50655943 0.5064713  0.3319981\n",
      " 0.47324258 0.7564249  0.4359465  0.31583947 0.5400369  0.3399734\n",
      " 0.18543442 0.06836835 0.17765026 0.17089482 0.4986455  0.84096706\n",
      " 0.44398865 0.5946064  0.49810612 0.21712494 0.6280099  0.6676349\n",
      " 0.35303926 0.5544897  0.7140875  0.92273575 0.10652508 0.41529897\n",
      " 0.32570192 0.05200697 0.9065336  0.6003981  0.8615523  0.18476176\n",
      " 0.20755903 0.76199347 0.41702476 0.33619675 0.3618123  0.65300816\n",
      " 0.25868106 0.5136779  0.5152971  0.2649369  0.39913046 0.7099571\n",
      " 0.74387187 0.5256815  0.35606053 0.3481848  0.21424612 0.6660862\n",
      " 0.74423236 0.6721339  0.05890102 0.7153163  0.75599664 0.35352367\n",
      " 0.715473   0.6891533  0.6365873  0.31856754 0.52843535 0.78409046\n",
      " 0.8317293  0.6160657  0.62970656 0.74026054 0.43753543 0.49138647\n",
      " 0.61958236 0.35106304 0.4252753  0.80728495 0.2879989  0.6868998\n",
      " 0.63772005 0.5614413  0.18480721 0.54311645 0.4411987  0.2628861\n",
      " 0.32823265 0.8578128  0.45677584 0.4867154  0.48078656 0.7751933\n",
      " 0.30648968 0.2738163  0.58040863 0.12065245 0.30218643 0.49317554\n",
      " 0.3271503  0.17380512 0.09179205 0.42957482 0.19302848 0.51709235\n",
      " 0.9200202  0.70723546 0.36439893 0.6080052  0.46076214 0.30328697\n",
      " 0.4232015  0.20667301 0.6047831  0.71085566 0.83364916 0.09822239\n",
      " 0.6107819  0.48494282 0.62662226 0.72215164 0.5316333  0.45047873\n",
      " 0.72908306 0.8620567  0.18171763 0.7523823  0.58577514 0.41878337\n",
      " 0.30443734 0.5495334  0.34773958 0.24816038 0.5950116  0.51886386\n",
      " 0.43975833 0.2800865  0.32379952 0.8211582  0.6602638  0.51362324\n",
      " 0.4431689  0.62819165 0.4085936  0.2382987  0.5875233  0.28059778\n",
      " 0.4957672  0.41580954 0.52105945 0.65409476 0.64147645 0.6172864\n",
      " 0.48312458 0.0675837  0.4477052  0.4674185  0.32748905 0.38929087\n",
      " 0.35933796 0.503686   0.6308392  0.21686573 0.8585174  0.8204143\n",
      " 0.6747846  0.6194516  0.6835213  0.31665155 0.49060744 0.28253523\n",
      " 0.6637865  0.15394716 0.7357559  0.25806656 0.48793915 0.22310138\n",
      " 0.9406553  0.72905475 0.28745705 0.39816794 0.7100714  0.3753715\n",
      " 0.86612886 0.28337252 0.46040624 0.7370543  0.61266637 0.38888103\n",
      " 0.6037361  0.84553885 0.71009386 0.5485415  0.263612   0.20614627\n",
      " 0.6093403  0.7271276  0.26063627 0.15379371 0.30289155 0.3719172 ]\n",
      "Outputs are equal\n",
      "my and torch input gradients:\n",
      "[-9.12172717e-02  7.63019525e-02 -8.76250422e-02  2.25300685e-01\n",
      "  1.18546268e-01  3.69945708e-01  2.88062294e-01  7.98999975e-02\n",
      " -3.30024619e-02 -3.62189709e-02  6.87723089e-02 -1.61334479e-01\n",
      "  8.63057899e-03 -3.07712417e-01 -5.82430448e-01  3.99975886e-01\n",
      "  1.60865068e-01 -2.97041566e-01 -2.35537072e-01 -1.85984822e-01\n",
      "  4.21144960e-01 -3.75363087e-02 -1.57602142e-01  2.12714216e-01\n",
      " -2.12917200e-01 -2.27990677e-02  3.46994023e-01 -1.89807493e-01\n",
      "  2.20306648e-01 -5.40395198e-01  1.41681111e-01 -1.14776396e-01\n",
      "  1.18770704e-02  1.30758192e-02  4.58273346e-01 -2.34263942e-01\n",
      "  8.65846491e-02  9.94405716e-02 -7.43422293e-01 -2.37375952e-01\n",
      " -8.56730294e-02 -2.97990846e-01 -4.02773524e-02 -2.63897737e-01\n",
      "  3.10237725e-01 -2.80875119e-01  1.75228166e-02 -5.49932892e-02\n",
      "  1.29004191e-01 -2.13361734e-01  1.36953167e-01 -2.75584819e-01\n",
      " -1.63033998e-02  1.96430152e-01  4.40969627e-02  7.86483577e-02\n",
      " -4.87618720e-01 -3.09791249e-01  1.15452253e-01 -4.34967162e-01\n",
      "  2.16168277e-01  8.34155766e-02 -1.44238040e-01  3.49975040e-01\n",
      " -1.53839288e-01  6.02158044e-01 -8.38289834e-02  3.51688657e-02\n",
      " -6.62745591e-03  1.05879035e-01  4.89015175e-02 -3.08113242e-01\n",
      " -9.52318776e-02  6.02269076e-02 -3.98105990e-02  4.28103114e-01\n",
      "  2.12249069e-01 -1.78040806e-01  5.32423660e-02 -1.55260418e-01\n",
      "  6.42020157e-02  8.56625723e-02  4.11228549e-01  5.27915594e-02\n",
      "  4.66352056e-01 -1.24783478e-01 -1.34745414e-02  1.75598476e-01\n",
      "  3.33252877e-01 -7.78837987e-02  2.04276707e-01  2.21172043e-01\n",
      " -1.04947110e-01  2.49266101e-01 -4.96212516e-01 -1.72177907e-01\n",
      " -2.86390517e-01  5.57640911e-02  5.18021388e-01 -3.05950397e-01\n",
      "  5.31441790e-01 -2.53576164e-01  1.70950347e-02 -2.64239474e-02\n",
      " -7.31742934e-03  3.79269126e-01  5.56746684e-01  1.28574108e-01\n",
      " -4.04663667e-01 -1.56528321e-01 -4.76519355e-02 -5.15247971e-02\n",
      " -2.52848211e-01 -6.61194767e-01 -3.81963106e-01  2.09968456e-01\n",
      "  8.65885308e-03 -5.07704605e-01 -9.19702024e-02  2.76926891e-01\n",
      " -4.12033073e-02 -3.52966140e-01  3.13164132e-01 -5.48128856e-02\n",
      "  3.65962790e-02 -1.97228067e-01 -4.58031319e-01 -2.17935178e-01\n",
      "  3.45460228e-02  3.23250697e-01 -6.37976789e-02  4.73080248e-01\n",
      " -1.92941955e-01  9.89499206e-02  1.27860168e-01 -1.45402900e-01\n",
      "  2.89400170e-02 -1.39134385e-01 -2.83625224e-01  1.97009969e-01\n",
      "  4.08632442e-01 -6.55129859e-02  6.06826967e-01 -2.38357679e-01\n",
      "  2.87627758e-01  3.19966301e-01  3.36790825e-01 -2.55133021e-03\n",
      "  3.59670360e-02  9.95349984e-02  3.53075853e-01 -1.06810573e-01\n",
      " -5.16271323e-01 -3.00367896e-01 -1.21663421e-01  1.23702977e-01\n",
      "  1.04784651e-01  1.91723066e-01 -1.36661940e-02 -8.48697759e-02\n",
      "  1.78862982e-01  7.20269492e-02 -3.90860124e-01 -3.27504008e-02\n",
      " -2.10606734e-01  8.09129299e-02  1.06361338e-01  3.87811993e-01\n",
      "  5.63891248e-01  6.88489568e-02 -6.05837470e-02 -1.00089726e-01\n",
      " -2.54608109e-01 -2.12657347e-01  1.31107859e-01  1.50168410e-01\n",
      "  3.25947128e-03  2.36051096e-01  1.88219675e-01  1.72502632e-01\n",
      " -2.59574243e-01 -5.01678277e-01  1.12145815e-01  5.03519692e-03\n",
      " -1.86980170e-01 -3.31009600e-01 -4.97887862e-02 -1.18591638e-01\n",
      "  1.41671081e-01 -1.87086992e-01  6.37834221e-02  2.54643617e-01\n",
      " -2.19393835e-02 -2.71446384e-01 -1.09243678e-01  3.97097178e-01\n",
      " -2.43230602e-01  3.45923351e-02 -4.12822359e-02 -3.69571397e-02\n",
      " -7.42982335e-01 -1.12623938e-01  3.83195474e-02  2.58280454e-01\n",
      "  4.40938359e-02  1.79599777e-01  3.72417405e-02  5.90200242e-02\n",
      " -2.78707075e-01  1.95969418e-01 -4.92589973e-01 -2.33825280e-01\n",
      " -9.56851607e-02 -2.80702688e-02  2.25119648e-01  5.41811357e-01\n",
      "  3.51162334e-01 -1.73990031e-01  1.92384110e-01  2.92619453e-01\n",
      "  2.12024946e-01  2.28395605e-01  3.19445181e-01  6.60782222e-02\n",
      " -3.22405000e-01 -3.53689606e-02 -3.69906554e-01 -1.92430100e-01\n",
      "  2.42890806e-01  2.87697264e-01  5.29697092e-02  2.31660661e-01\n",
      "  5.71673729e-02 -5.95267413e-02  3.07894573e-01  1.27648570e-01\n",
      " -1.38145497e-01 -1.67133427e-01  2.01752824e-02 -1.00179539e-01\n",
      " -2.02075005e-01  3.40581442e-01 -3.11844331e-01 -6.26385029e-02\n",
      "  3.32027584e-01  2.70373979e-02  1.44278400e-02 -1.53740889e-01\n",
      " -1.72888090e-01 -1.23454070e-01  1.54399857e-01  3.81666596e-01\n",
      "  1.28568855e-01 -1.49549548e-02 -1.14667053e-01  1.33183769e-01\n",
      "  1.47812739e-01  3.75611621e-01 -3.17228168e-01  3.60766345e-02\n",
      " -2.90214929e-03  3.17748588e-01 -4.00227781e-01 -3.85024862e-01\n",
      "  1.33795364e-01  2.26813360e-01  1.66280940e-01  3.52331292e-02\n",
      " -8.57238661e-02  6.94017369e-02 -1.66333852e-01  2.64237880e-01\n",
      "  3.17050799e-02  2.24617255e-01 -8.71510069e-02  2.57057705e-01\n",
      "  4.56687978e-01  7.13892770e-02  1.22863107e-01 -8.27324754e-02\n",
      "  5.74257572e-01  3.62339009e-01  1.74998407e-01  6.11806175e-02\n",
      "  3.02161921e-01 -2.48141624e-01  3.14667190e-01 -4.68385445e-01\n",
      "  1.05672180e-02  1.38377899e-01  1.90343791e-01 -3.54303644e-01\n",
      "  3.99585565e-01  6.42550025e-02 -5.15382835e-02 -2.15273443e-01\n",
      "  5.75441544e-02 -3.08402059e-01  9.87819773e-02  1.75402616e-01\n",
      "  3.32397236e-01  6.21723661e-02 -3.04904649e-02 -3.42012983e-01\n",
      " -6.08634530e-01 -5.23472613e-03 -1.71882319e-01 -1.09955553e-01\n",
      "  2.17951163e-01  8.85517258e-02 -2.37955408e-01 -1.98997179e-01\n",
      "  2.57610462e-01 -3.35528763e-01  1.25052672e-01  2.09348783e-01\n",
      "  1.41925393e-01 -2.24547862e-01  6.23447366e-02 -6.67300714e-02\n",
      " -2.93678462e-01 -1.84709802e-01  2.23358551e-01  4.57981599e-02\n",
      "  2.62572689e-02  5.18262524e-02  8.30005419e-02 -1.95961719e-01\n",
      " -2.00182573e-01 -1.97758766e-01 -1.56919078e-01 -3.47360228e-01\n",
      "  1.84064975e-01  1.69281774e-01 -2.82353311e-03  2.89075109e-02\n",
      "  1.33928693e-04  5.45253629e-02 -3.95327746e-01 -2.57215605e-01\n",
      "  4.42574489e-02  5.63516392e-01  9.28256868e-02  2.15243460e-02\n",
      "  3.04499622e-01  1.05020872e-01  7.68723679e-02 -2.79106983e-02\n",
      " -1.53073471e-01  9.30403389e-02  9.25564374e-02  3.38447254e-01\n",
      "  2.35263593e-01  1.80087349e-01  6.61588160e-01  3.65702680e-01\n",
      " -1.96017343e-01  1.57240873e-01 -1.25987662e-01 -1.75714691e-01\n",
      "  1.64995345e-01  5.68652064e-01  4.24741739e-01 -5.94747535e-03\n",
      "  4.51190946e-02  3.55152330e-01 -1.16310525e-01  4.65554092e-02\n",
      "  2.91556186e-02 -5.38777416e-04 -9.37012141e-02 -1.51857661e-02\n",
      "  2.42503210e-01  1.26963924e-01  2.08662150e-01  9.29492423e-03\n",
      "  1.49854258e-01 -1.78929488e-01 -4.09910992e-01 -3.46474589e-01\n",
      "  5.84487325e-02  3.53559113e-02 -4.84421618e-01 -2.04613484e-01\n",
      "  3.71591309e-02 -3.82484608e-01  2.18458035e-01  7.17020496e-02\n",
      " -3.66318054e-01 -1.73092224e-01  1.84499101e-01 -4.28698996e-01\n",
      " -3.47181729e-01  9.11368279e-03 -1.49249297e-01  2.85793794e-01\n",
      "  2.53632265e-01  4.73475230e-02 -1.46185910e-01  1.24268255e-01\n",
      "  5.97421911e-01  1.43573883e-01 -1.06959673e-01 -5.78928905e-02\n",
      " -7.46926312e-02  1.13665843e-01 -2.45440138e-02  7.18599382e-02\n",
      " -5.25313422e-01  1.92105773e-03  1.85136128e-02  3.36078736e-01\n",
      "  7.93590579e-03 -3.36212253e-01 -1.24335066e-01 -1.08267500e-01\n",
      "  7.22169604e-02 -2.72125587e-01 -7.72717971e-02  1.93117636e-01\n",
      " -4.13235880e-02 -4.30693774e-01 -4.70753365e-02 -2.09583897e-01\n",
      " -1.25354912e-01 -3.42200353e-01 -1.25229800e-01  4.47793739e-01\n",
      " -2.33852518e-02 -2.53181863e-01 -2.23866971e-01 -1.46489792e-02\n",
      " -3.55927068e-01 -3.86940203e-02  1.81560309e-01  4.87640089e-02\n",
      "  1.72191732e-01  3.36644733e-01 -9.79533648e-02  1.47515309e-01\n",
      "  1.96223816e-02 -2.08700869e-01 -1.53196106e-01  1.10839643e-02\n",
      "  1.72727095e-01  2.87074089e-01  7.86104588e-02 -6.67992754e-02\n",
      "  2.92463028e-02  3.67557232e-01  3.03551411e-01  3.14544087e-01\n",
      " -1.56338765e-01 -4.28122542e-02  7.63469703e-02 -1.99048896e-01\n",
      " -3.16956779e-02 -1.69372982e-01 -1.20897402e-01  7.27198238e-02\n",
      " -2.29419111e-02  1.93517168e-01  8.96998131e-02  4.15093482e-01\n",
      " -7.55050653e-02 -2.84312092e-01  2.89149984e-01 -1.85405068e-01\n",
      " -4.31231763e-01 -2.34866960e-02  1.80969346e-02  1.40103469e-01\n",
      " -1.30825036e-01 -2.28991245e-01 -5.62357438e-02  1.41502152e-01\n",
      "  3.37194395e-01 -2.34594683e-01 -7.24141614e-02  1.32137408e-01] [-7.57173300e-02  7.63019547e-02 -8.76250416e-02  2.25300685e-01\n",
      "  6.70828894e-02  3.69945705e-01  2.88062304e-01  7.98999965e-02\n",
      " -2.94058751e-02 -3.62189673e-02  6.66131377e-02 -1.61334470e-01\n",
      "  8.45420919e-03 -2.06911445e-01 -5.82430482e-01  3.99975896e-01\n",
      "  1.60865068e-01 -2.76221812e-01 -2.25403741e-01 -1.85984820e-01\n",
      "  4.05066490e-01 -3.49854454e-02 -1.46066144e-01  2.12714210e-01\n",
      " -2.12846100e-01 -2.27990672e-02  3.43122870e-01 -1.89807475e-01\n",
      "  1.42639890e-01 -4.85052437e-01  1.32009104e-01 -1.14776395e-01\n",
      "  1.18770711e-02  1.15190037e-02  4.56331164e-01 -2.34263942e-01\n",
      "  6.29805326e-02  9.94405746e-02 -7.28637099e-01 -1.27942398e-01\n",
      " -8.56730267e-02 -2.97990859e-01 -4.02773544e-02 -2.63897747e-01\n",
      "  2.11263195e-01 -2.28856683e-01  1.75228175e-02 -5.49932905e-02\n",
      "  1.29004180e-01 -2.13361725e-01  1.22700281e-01 -2.75584817e-01\n",
      " -1.22761009e-02  1.93568587e-01  2.15466022e-02  7.86483586e-02\n",
      " -4.87618715e-01 -3.09791267e-01  1.15452252e-01 -3.46862674e-01\n",
      "  2.06058845e-01  8.34155753e-02 -1.44238055e-01  3.38052124e-01\n",
      " -1.53839290e-01  4.81967390e-01 -8.38289857e-02  7.31123146e-03\n",
      " -6.62745535e-03  1.05879039e-01  4.46464606e-02 -2.44033977e-01\n",
      " -5.50222956e-02  5.95700033e-02 -3.98105979e-02  4.28103119e-01\n",
      "  1.52512401e-01 -1.51360288e-01  5.27138039e-02 -1.55260429e-01\n",
      "  6.42020181e-02  5.53051606e-02  4.10789341e-01  5.11276126e-02\n",
      "  4.50462252e-01 -1.24783486e-01 -1.34745408e-02  1.72751397e-01\n",
      "  3.28369766e-01 -3.09578925e-02  2.04276711e-01  1.48039043e-01\n",
      " -1.04947113e-01  2.49266103e-01 -2.78136462e-01 -1.72177911e-01\n",
      " -2.82073528e-01  5.57640903e-02  5.14112711e-01 -3.00050288e-01\n",
      "  5.31441808e-01 -2.53576159e-01  1.46054998e-02 -2.64239479e-02\n",
      " -7.31742894e-03  1.90169722e-01  5.56746721e-01  1.28574118e-01\n",
      " -3.99076194e-01 -1.56349108e-01 -4.76519391e-02 -5.15247993e-02\n",
      " -2.52848208e-01 -6.61194801e-01 -3.81963104e-01  2.09968463e-01\n",
      "  8.65885336e-03 -5.07704556e-01 -9.19701979e-02  2.76926875e-01\n",
      " -4.12033051e-02 -3.52966160e-01  2.32345998e-01 -5.48055731e-02\n",
      "  3.25875767e-02 -1.01441801e-01 -4.01932448e-01 -1.40694976e-01\n",
      "  3.45460214e-02  2.94150442e-01 -6.37976825e-02  4.73080248e-01\n",
      " -1.92941949e-01  9.86630097e-02  1.25935018e-01 -1.45402893e-01\n",
      "  2.89400164e-02 -1.33074299e-01 -2.64414966e-01  1.97009966e-01\n",
      "  4.08632457e-01 -6.52267262e-02  5.65925539e-01 -2.38357678e-01\n",
      "  2.39269525e-01  3.19966316e-01  3.36790830e-01 -2.55133025e-03\n",
      "  3.59670371e-02  9.83100086e-02  3.52864206e-01 -1.06810570e-01\n",
      " -4.69046891e-01 -2.87046224e-01 -1.21663421e-01  1.23702973e-01\n",
      "  1.04784653e-01  1.91723049e-01 -1.36661939e-02 -6.42315522e-02\n",
      "  1.78862989e-01  7.20269457e-02 -3.90860111e-01 -3.27504016e-02\n",
      " -2.10606754e-01  8.01512972e-02  1.06361337e-01  3.82553875e-01\n",
      "  5.22298157e-01  6.88489601e-02 -6.05837479e-02 -1.00089721e-01\n",
      " -2.54608124e-01 -1.98386759e-01  9.98908505e-02  1.50168419e-01\n",
      "  3.25947138e-03  2.36051098e-01  1.46710828e-01  1.34794623e-01\n",
      " -2.59574234e-01 -4.70477402e-01  1.02780066e-01  4.08642832e-03\n",
      " -1.86980158e-01 -2.26043999e-01 -4.97771800e-02 -1.18591629e-01\n",
      "  1.09399773e-01 -1.72359496e-01  6.37834296e-02  2.10475773e-01\n",
      " -9.89174657e-03 -2.53682256e-01 -8.06251243e-02  3.91694129e-01\n",
      " -2.43230626e-01  3.45923379e-02 -4.12352830e-02 -3.65388729e-02\n",
      " -7.09449410e-01 -1.12068795e-01  3.83195467e-02  2.57935286e-01\n",
      "  4.40938361e-02  1.75304949e-01  3.72417383e-02  5.90200238e-02\n",
      " -8.62165838e-02  1.74915969e-01 -4.92589951e-01 -2.33099282e-01\n",
      " -7.45552927e-02 -2.80702692e-02  2.13428676e-01  5.02588272e-01\n",
      "  2.62576461e-01 -1.73990026e-01  1.89605013e-01  2.54466265e-01\n",
      "  2.12024942e-01  2.15659305e-01  3.19445193e-01  5.37825078e-02\n",
      " -1.20640881e-01 -3.45730893e-02 -2.98578978e-01 -1.50787503e-01\n",
      "  2.42890805e-01  2.87697285e-01  5.29598780e-02  2.31660664e-01\n",
      "  5.62780946e-02 -5.95267378e-02  2.33152464e-01  1.23727597e-01\n",
      " -1.38145462e-01 -1.41349211e-01  1.80139635e-02 -1.00179538e-01\n",
      " -2.02075019e-01  3.40581447e-01 -3.10582787e-01 -6.26384988e-02\n",
      "  3.28067869e-01  2.70373970e-02  1.33494353e-02 -1.53740883e-01\n",
      " -1.72888085e-01 -1.23454086e-01  1.53994113e-01  3.81666601e-01\n",
      "  9.62891653e-02 -6.36646245e-03 -1.14667051e-01  1.26959622e-01\n",
      "  1.42961204e-01  3.75611603e-01 -2.19114691e-01  3.10665518e-02\n",
      " -2.90214969e-03  3.17720205e-01 -4.00227815e-01 -3.85024846e-01\n",
      "  1.26847684e-01  2.26813361e-01  7.32076168e-02  3.52331288e-02\n",
      " -8.46120939e-02  6.94017336e-02 -9.13266093e-02  2.64237940e-01\n",
      "  3.17050815e-02  2.24617243e-01 -8.70741010e-02  2.50558615e-01\n",
      "  4.56687987e-01  7.13892803e-02  1.02499992e-01 -8.27324763e-02\n",
      "  5.74257553e-01  2.78462976e-01  1.74998417e-01  3.82273607e-02\n",
      "  2.45085403e-01 -2.48141617e-01  3.14667195e-01 -4.15505409e-01\n",
      "  1.05369547e-02  1.38377890e-01  1.87219992e-01 -3.06238681e-01\n",
      "  3.99585545e-01  5.76730967e-02 -3.11391205e-02 -5.48466071e-02\n",
      "  3.36266495e-02 -1.74789682e-01  9.87812504e-02  1.75402611e-01\n",
      "  3.28225911e-01  6.21723644e-02 -3.04900277e-02 -2.32543781e-01\n",
      " -6.08634531e-01 -5.23472670e-03 -1.57033429e-01 -1.09955549e-01\n",
      "  2.17951164e-01  8.85517299e-02 -9.05919895e-02 -1.93286553e-01\n",
      "  2.26305813e-01 -6.61692843e-02  1.25052661e-01  2.09348783e-01\n",
      "  1.41925395e-01 -1.35289952e-01  4.10174206e-02 -6.67300746e-02\n",
      " -2.85590708e-01 -1.64885625e-01  2.06297681e-01  4.57981601e-02\n",
      "  2.01409180e-02  5.18262498e-02  8.30005407e-02 -1.52650520e-01\n",
      " -1.92035407e-01 -1.97758794e-01 -1.56919077e-01 -3.47360194e-01\n",
      "  1.68810725e-01  1.53675437e-01 -1.90130749e-03  2.89075114e-02\n",
      "  1.33928683e-04  5.45253605e-02 -8.76547396e-02 -2.57215619e-01\n",
      "  4.42574471e-02  5.15154719e-01  9.28256884e-02  2.15243455e-02\n",
      "  3.04499626e-01  9.11926851e-02  7.68723711e-02 -2.79106982e-02\n",
      " -1.53073460e-01  9.30403322e-02  9.25564319e-02  3.38447243e-01\n",
      "  2.31591776e-01  1.80033907e-01  6.61588192e-01  3.33254308e-01\n",
      " -1.91639259e-01  1.57240883e-01 -1.03337869e-01 -1.75714687e-01\n",
      "  1.64995342e-01  5.68652093e-01  2.55955338e-01 -5.94747532e-03\n",
      "  4.44950797e-02  2.75281399e-01 -1.02584012e-01  4.65554073e-02\n",
      "  2.89377291e-02 -5.38397057e-04 -9.35628489e-02 -1.51857659e-02\n",
      "  2.06179857e-01  1.00982502e-01  2.08662152e-01  3.94459674e-03\n",
      "  1.26398951e-01 -1.78896174e-01 -3.60923290e-01 -1.99010715e-01\n",
      "  1.94906108e-02  3.46544869e-02 -3.01830500e-01 -2.04613492e-01\n",
      "  3.71591300e-02 -3.82484585e-01  2.02390313e-01  7.17020556e-02\n",
      " -3.64062101e-01 -1.46300316e-01  1.80146396e-01 -2.81156689e-01\n",
      " -3.47181708e-01  9.11368337e-03 -1.49249285e-01  1.01256453e-01\n",
      "  2.53632277e-01  4.73045893e-02 -1.46185890e-01  1.24268241e-01\n",
      "  5.97421885e-01  1.42165512e-01 -1.06959678e-01 -5.78928888e-02\n",
      " -4.44260798e-02  1.13665842e-01 -2.45440137e-02  6.99639469e-02\n",
      " -4.44951504e-01  1.92105758e-03  1.67967919e-02  2.50817984e-01\n",
      "  7.93590583e-03 -3.36212248e-01 -1.22530200e-01 -8.73233899e-02\n",
      "  6.32485896e-02 -2.72125572e-01 -7.72717968e-02  1.93117633e-01\n",
      " -4.07897271e-02 -4.30693775e-01 -4.55020554e-02 -1.52168334e-01\n",
      " -1.25354901e-01 -2.76309878e-01 -1.25220835e-01  4.35097873e-01\n",
      " -2.33852509e-02 -2.53181875e-01 -2.23866969e-01 -1.46489786e-02\n",
      " -3.55521619e-01 -9.75339301e-03  1.79574221e-01  4.85569462e-02\n",
      "  1.51694015e-01  3.20140392e-01 -9.02010202e-02  1.47515312e-01\n",
      "  1.96223818e-02 -1.41778842e-01 -1.53196096e-01  1.10839643e-02\n",
      "  1.72727093e-01  2.87074089e-01  7.86104575e-02 -5.78169972e-02\n",
      "  2.92359814e-02  2.98028737e-01  3.03551406e-01  1.63874239e-01\n",
      " -1.56338781e-01 -3.27887461e-02  7.63025582e-02 -1.38002306e-01\n",
      " -3.16956788e-02 -1.69372976e-01 -9.90514830e-02  6.97034672e-02\n",
      " -2.29419097e-02  1.81494132e-01  8.96998197e-02  3.37176323e-01\n",
      " -7.50316009e-02 -2.84312099e-01  2.89150000e-01 -1.76247954e-01\n",
      " -4.31231767e-01 -2.34866943e-02  1.80969350e-02  1.40103459e-01\n",
      " -1.01583399e-01 -1.49897650e-01 -5.62357455e-02  1.41502157e-01\n",
      "  2.59916186e-01 -1.22121744e-01 -6.11604936e-02  1.23466440e-01]\n",
      "Input gradients are equal\n",
      "passed with atol=1 and random_sampler=randn\n"
     ]
    }
   ],
   "source": [
    "atol = 1\n",
    "random_sampler = np.random.randn\n",
    "sigmoid_test([n_samples, n_input_features], print_results=True, print_tensors=True, atol=atol, random_sampler=random_sampler)\n",
    "print(f\"passed with atol={atol} and random_sampler={random_sampler.__name__}\")\n",
    "sigmoid_test([n_samples, n_input_features, height, width],  print_results=True, print_tensors=True, atol=atol, random_sampler=random_sampler)\n",
    "print(f\"passed with atol={atol} and random_sampler={random_sampler.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5e1f6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Outputs are equal\n",
      "Input gradients are equal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Flatten test\"\"\"\n",
    "\n",
    "flatten_test(input_shape = [2, 3, 5, 5], print_results=True, random_sampler=np.random.rand)\n",
    "flatten_test(input_shape = [2, 3, 5, 5], print_results=True, random_sampler=np.random.randn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "58326a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "\n",
      "Outputs are equal\n",
      "Input gradients are equal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"MaxPool2d tests\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "n_channels = 3\n",
    "height = 16\n",
    "width = 16\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "max_pool_2d_test(batch_size, height, width, n_channels, kernel_size, stride, padding, atol=1e-6, print_results=True, random_sampler=np.random.rand)\n",
    "# max_pool_2d_test(batch_size, height, width, n_channels, kernel_size, stride, padding, atol=1e-6, print_results=True, random_sampler=np.random.randn)\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "max_pool_2d_test(batch_size = 2, height = 6, width = 4, n_channels = 3,\n",
    "                 kernel_size = 2, stride = 1, padding = 0, atol=1e-6, print_results=True, random_sampler=np.random.rand)\n",
    "# max_pool_2d_test(batch_size = 2, height = 6, width = 4, n_channels = 3,\n",
    "#                  kernel_size = 2, stride = 1, padding = 0, atol=1e-6, print_results=True, random_sampler=np.random.randn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "531d3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my and torch outputs:\n",
      "[0.81953168 1.30946732 1.30946732 ... 1.21647656 1.21647656 1.44358754] [0.8195317 1.3094673 1.3094673 ... 1.2164766 1.2164766 1.4435875]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Outputs are not equal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m max_pool_2d_test(batch_size, height, width, n_channels, kernel_size, stride, padding, atol\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m, print_results\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, random_sampler\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandn, print_tensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 18\u001b[0m in \u001b[0;36mmax_pool_2d_test\u001b[1;34m(batch_size, height, width, n_channels, kernel_size, stride, padding, atol, random_sampler, print_tensors, print_results)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m my_pool \u001b[39m=\u001b[39m MaxPool2d(\u001b[39m*\u001b[39mmy_pool_args)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m torch_pool \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMaxPool2d(\u001b[39m*\u001b[39mtorch_pool_args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m test_module(my_pool,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m             torch_pool,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m             input_shape\u001b[39m=\u001b[39;49m[batch_size, n_channels, height, width],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m             output_shape\u001b[39m=\u001b[39;49m[batch_size, n_channels, output_height, output_width],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m             atol\u001b[39m=\u001b[39;49matol,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m             random_sampler \u001b[39m=\u001b[39;49m random_sampler,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m             print_tensors\u001b[39m=\u001b[39;49mprint_tensors,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m             print_results\u001b[39m=\u001b[39;49mprint_results)\n",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 18\u001b[0m in \u001b[0;36mtest_module\u001b[1;34m(my_module, torch_module, input_shape, output_shape, atol, random_sampler, skip_parameter_copying, print_tensors, print_results)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmy and torch outputs:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mprint\u001b[39m(output_np\u001b[39m.\u001b[39mflatten(), output_torch\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mflatten())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mallclose(output_np, output_torch\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), atol\u001b[39m=\u001b[39matol), \u001b[39m\"\u001b[39m\u001b[39mOutputs are not equal\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mif\u001b[39;00m print_results:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X20sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutputs are equal\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Outputs are not equal"
     ]
    }
   ],
   "source": [
    "max_pool_2d_test(batch_size, height, width, n_channels, kernel_size, stride, padding, atol=1e-2, print_results=True, random_sampler=np.random.randn, print_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9ad76517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my and torch outputs:\n",
      "[0.         0.46690935 0.98588884 0.98588884 0.         0.46690935\n",
      " 2.7892983  2.7892983  0.         1.19887888 2.7892983  2.7892983\n",
      " 0.         1.19887888 1.19887888 0.42659062] [-1.5969588   0.46690935  0.98588884  0.98588884 -0.22935311  0.46690935\n",
      "  2.7892983   2.7892983  -0.11540441  1.1988789   2.7892983   2.7892983\n",
      " -0.11540441  1.1988789   1.1988789   0.42659062]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Outputs are not equal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m max_pool_2d_test(batch_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, height \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m, width \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m, n_channels \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, kernel_size \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, stride \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, padding \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, atol\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m, print_results\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, random_sampler\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandn, print_tensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 19\u001b[0m in \u001b[0;36mmax_pool_2d_test\u001b[1;34m(batch_size, height, width, n_channels, kernel_size, stride, padding, atol, random_sampler, print_tensors, print_results)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m my_pool \u001b[39m=\u001b[39m MaxPool2d(\u001b[39m*\u001b[39mmy_pool_args)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m torch_pool \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMaxPool2d(\u001b[39m*\u001b[39mtorch_pool_args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m test_module(my_pool,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m             torch_pool,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m             input_shape\u001b[39m=\u001b[39;49m[batch_size, n_channels, height, width],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m             output_shape\u001b[39m=\u001b[39;49m[batch_size, n_channels, output_height, output_width],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m             atol\u001b[39m=\u001b[39;49matol,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m             random_sampler \u001b[39m=\u001b[39;49m random_sampler,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m             print_tensors\u001b[39m=\u001b[39;49mprint_tensors,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m             print_results\u001b[39m=\u001b[39;49mprint_results)\n",
      "\u001b[1;32mc:\\Users\\SystemPoint\\Documents\\GitHub\\DNN_lab_1\\numpy_nn\\test\\module_tests.ipynb Cell 19\u001b[0m in \u001b[0;36mtest_module\u001b[1;34m(my_module, torch_module, input_shape, output_shape, atol, random_sampler, skip_parameter_copying, print_tensors, print_results)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmy and torch outputs:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mprint\u001b[39m(output_np\u001b[39m.\u001b[39mflatten(), output_torch\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mflatten())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mallclose(output_np, output_torch\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), atol\u001b[39m=\u001b[39matol), \u001b[39m\"\u001b[39m\u001b[39mOutputs are not equal\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mif\u001b[39;00m print_results:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SystemPoint/Documents/GitHub/DNN_lab_1/numpy_nn/test/module_tests.ipynb#X56sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutputs are equal\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Outputs are not equal"
     ]
    }
   ],
   "source": [
    "max_pool_2d_test(batch_size = 1, height = 3, width = 3, n_channels = 1, kernel_size = 2, stride = 1, padding = 1, atol=1e-2, print_results=True, random_sampler=np.random.randn, print_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "04e50bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import  os\n",
    "project_root = os.path.dirname(os.path.dirname(sys.path[0]))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "\n",
    "\n",
    "from typing import Callable, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from numpy_nn.modules.np_nn import (\n",
    "    FullyConnectedLayer,\n",
    "    BatchNormalization2d,\n",
    "    TrainableLayer,\n",
    "    Module,\n",
    ")\n",
    "\n",
    "\n",
    "def copy_trainable_parameters(my_module: Module, torch_module: torch.nn.Module) -> None:\n",
    "    if isinstance(my_module, FullyConnectedLayer):\n",
    "        my_module.weights = torch_module.weight.detach().numpy().T\n",
    "        my_module.bias = torch_module.bias.detach().numpy().reshape(-1, 1).T\n",
    "    elif isinstance(my_module, BatchNormalization2d):\n",
    "        n_channels = my_module.n_channels\n",
    "        my_module.gamma = torch_module.weight.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "        my_module.beta = torch_module.bias.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "        my_module.running_mean = torch_module.running_mean.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "        my_module.running_var = torch_module.running_var.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "    else:\n",
    "        my_module.weights = torch_module.weight.detach().numpy()\n",
    "        if my_module.bias is not None:\n",
    "            my_module.bias = torch_module.bias.detach().numpy()\n",
    "\n",
    "\n",
    "def test_module(my_module: Module,\n",
    "                torch_module: torch.nn.Module,\n",
    "                input_shape: Tuple[int, ...],\n",
    "                output_shape: Tuple[int, ...],\n",
    "                atol: float = 1e-5,\n",
    "                random_sampler: Callable = np.random.rand,\n",
    "                skip_parameter_copying: bool = False,\n",
    "                print_tensors: bool = False,\n",
    "                print_results: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Compares the output and gradients of a numpy layer and a torch layer\n",
    "\n",
    "    Args:\n",
    "        my_module: neural network layer implemented in numpy.\n",
    "        torch_module: neural network layer implemented in torch.\n",
    "        input_shape: shape of the input tensor.\n",
    "        output_shape: shape of the output tensor. It's used to generate\n",
    "            a random tensor representing partial derivative of the loss\n",
    "            function with respect to the output of the layer.\n",
    "        atol: absolute tolerance for comparing\n",
    "            numpy and torch tensors (used in np.allclose).\n",
    "        random_sampler: function that generates random tensors of the given shape.\n",
    "        skip_parameter_copying: if True, the weights and biases will be held intact.\n",
    "            By default, weights and biases are copied from torch_module to my_module.\n",
    "    \"\"\"    \n",
    "    # copy weights from torch_module to my_module\n",
    "    # if the numpy layer is trainable\n",
    "    print(\"cucucu\")\n",
    "    if not skip_parameter_copying and isinstance(my_module, TrainableLayer):\n",
    "        copy_trainable_parameters(my_module, torch_module)\n",
    "\n",
    "    input_np = random_sampler(*input_shape).astype(np.float32)\n",
    "    input_torch = torch.from_numpy(input_np)\n",
    "    input_torch.requires_grad = True\n",
    "\n",
    "    output_np = my_module.forward(input_np)\n",
    "    output_torch = torch_module(input_torch)\n",
    "\n",
    "\n",
    "    if print_tensors:\n",
    "        print(\"my and torch outputs:\")\n",
    "        print(output_np.flatten(), output_torch.detach().numpy().flatten())\n",
    "    assert np.allclose(output_np, output_torch.detach().numpy(), atol=atol), \"Outputs are not equal\"\n",
    "    if print_results:\n",
    "        print(\"Outputs are equal\")\n",
    "\n",
    "    output_grad_np = random_sampler(*output_shape)\n",
    "    output_grad_torch = torch.from_numpy(output_grad_np)\n",
    "\n",
    "    input_grad_np = my_module.backward(output_grad_np)\n",
    "    output_torch.backward(output_grad_torch)\n",
    "    input_grad_torch = input_torch.grad.detach().numpy()\n",
    "\n",
    "    if print_tensors:\n",
    "        print(\"my and torch input gradients:\")\n",
    "        print(input_grad_np.flatten(), input_grad_torch.flatten())\n",
    "    assert np.allclose(input_grad_np, input_grad_torch, atol=atol), \"Input gradients are not equal\"\n",
    "    if print_results:\n",
    "        print(\"Input gradients are equal\")\n",
    "\n",
    "\n",
    "    if not isinstance(my_module, TrainableLayer):\n",
    "        return\n",
    "    \n",
    "\n",
    "    # compare weight and bias gradients\n",
    "    if isinstance(my_module, FullyConnectedLayer):\n",
    "        weight_grad_np = my_module.weights_gradient\n",
    "        weight_grad_torch = torch_module.weight.grad.detach().numpy().T\n",
    "        bias_grad_np = my_module.bias_gradient\n",
    "        bias_grad_torch = torch_module.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "    elif isinstance(my_module, BatchNormalization2d):\n",
    "        weight_grad_np = my_module.gamma_gradient.flatten()\n",
    "        weight_grad_torch = torch_module.weight.grad.detach().numpy()\n",
    "        bias_grad_np = my_module.beta_gradient.flatten()\n",
    "        bias_grad_torch = torch_module.bias.grad.detach().numpy()\n",
    "\n",
    "        if print_tensors:\n",
    "            print(\"my and torch running means:\")\n",
    "            print(my_module.running_mean.flatten(), torch_module.running_mean.detach().numpy().flatten())\n",
    "        running_mean_close = np.allclose(\n",
    "            my_module.running_mean.flatten(), torch_module.running_mean.detach().numpy().flatten(), atol=atol)\n",
    "        assert running_mean_close, \"Running mean is not equal\"\n",
    "        if print_results:\n",
    "            print(\"Running means are equal\")\n",
    "\n",
    "        if print_tensors:\n",
    "            print(\"my and torch running vars:\")\n",
    "            print(my_module.running_var.flatten(), torch_module.running_var.detach().numpy().flatten())\n",
    "        running_var_close = np.allclose(\n",
    "            my_module.running_var.flatten(), torch_module.running_var.detach().numpy().flatten(), atol=atol)\n",
    "        assert running_var_close, \"Running var is not equal\"\n",
    "        if print_results:\n",
    "            print(\"Running vars are equal\")\n",
    "            \n",
    "    else:\n",
    "        weight_grad_np = my_module.weights_gradient\n",
    "        weight_grad_torch = torch_module.weight.grad.detach().numpy()\n",
    "        if my_module.bias is not None:\n",
    "            bias_grad_np = my_module.bias_gradient\n",
    "            bias_grad_torch = torch_module.bias.grad.detach().numpy()\n",
    "    \n",
    "    weight_grads_close = np.allclose(weight_grad_np, weight_grad_torch, atol=atol)\n",
    "\n",
    "    if print_tensors:\n",
    "        print(\"my and torch weight gradients:\")\n",
    "        print(weight_grad_np.flatten(), weight_grad_torch.flatten())\n",
    "        \n",
    "    assert weight_grads_close, \"Weight gradients are not equal\"\n",
    "    if print_results:\n",
    "        print(\"Weight gradients are equal\")\n",
    "\n",
    "    if isinstance(my_module, BatchNormalization2d) or my_module.bias is not None:\n",
    "        if print_tensors:\n",
    "            print(\"my and torch bias gradients:\")\n",
    "            print(bias_grad_np.flatten(), bias_grad_torch.flatten())\n",
    "\n",
    "        assert np.allclose(bias_grad_np, bias_grad_torch, atol=atol), \"Bias gradients are not equal\"\n",
    "        if print_results:\n",
    "            print(\"Bias gradients are equal\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_stack_of_layers(\n",
    "    my_stack_of_layers: List[Module],\n",
    "    torch_module_constructor: List[torch.nn.Module],\n",
    "    input_shape: Tuple[int, ...],\n",
    "    output_shape: Tuple[int, ...],\n",
    "    atol: float = 1e-5,\n",
    "    random_sampler: Callable = np.random.rand):\n",
    "\n",
    "    \"\"\"\n",
    "    Compares the output and all gradients of a my layer stack and torch layer stack\n",
    "    \"\"\"\n",
    "\n",
    "    input_np = random_sampler(*input_shape).astype(np.float32)\n",
    "    input_torch = torch.from_numpy(input_np)\n",
    "    input_torch.requires_grad = True\n",
    "\n",
    "    output_np = input_np\n",
    "    output_torch = input_torch\n",
    "    for my_layer, torch_layer in zip(my_stack_of_layers, torch_module_constructor):\n",
    "        output_np = my_layer.forward(output_np)\n",
    "        output_torch = torch_layer(output_torch)\n",
    "\n",
    "    assert np.allclose(output_np, output_torch.detach().numpy(), atol=atol), \"Outputs are not equal\"\n",
    "\n",
    "    output_grad_np = random_sampler(*output_shape)\n",
    "    output_grad_torch = torch.from_numpy(output_grad_np)\n",
    "\n",
    "    input_grad_np = output_grad_np\n",
    "    for my_layer, torch_layer in zip(my_stack_of_layers[::-1], torch_module_constructor[::-1]):\n",
    "        input_grad_np = my_layer.backward(input_grad_np)\n",
    "        output_torch.backward(output_grad_torch)\n",
    "        input_grad_torch = input_torch.grad.detach().numpy()\n",
    "\n",
    "    assert np.allclose(input_grad_np, input_grad_torch, atol=atol), \"Input gradients are not equal\"\n",
    "\n",
    "    # compare weight and bias gradients\n",
    "    for my_layer, torch_layer in zip(my_stack_of_layers, torch_module_constructor):\n",
    "        if isinstance(my_layer, TrainableLayer):\n",
    "            if isinstance(my_layer, FullyConnectedLayer):\n",
    "                weight_grad_np = my_layer.weights_gradient\n",
    "                weight_grad_torch = torch_layer.weight.grad.detach().numpy().T\n",
    "                bias_grad_np = my_layer.bias_gradient\n",
    "                bias_grad_torch = torch_layer.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "            elif isinstance(my_layer, BatchNormalization2d):\n",
    "                weight_grad_np = my_layer.gamma_gradient\n",
    "                weight_grad_torch = torch_layer.weight.grad.detach().numpy()\n",
    "                bias_grad_np = my_layer.beta_gradient\n",
    "                bias_grad_torch = torch_layer.bias.grad.detach().numpy()\n",
    "            else:\n",
    "                weight_grad_np = my_layer.weights_gradient\n",
    "                weight_grad_torch = torch_layer.weight.grad.detach().numpy()\n",
    "                if my_layer.bias is not None:\n",
    "                    bias_grad_np = my_layer.bias_gradient\n",
    "                    bias_grad_torch = torch_layer.bias.grad.detach().numpy()\n",
    "\n",
    "            assert np.allclose(weight_grad_np, weight_grad_torch, atol=atol), \"Weight gradients are not equal\"\n",
    "\n",
    "            if my_layer.bias is not None:\n",
    "                assert np.allclose(bias_grad_np, bias_grad_torch, atol=atol), \"Bias gradients are not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d73a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e890f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equal\n",
      "Input gradients are equal\n"
     ]
    }
   ],
   "source": [
    "max_pool_2d_test(batch_size = 2, height = 6, width = 4, n_channels = 3,\n",
    "                 kernel_size = 2, stride = 1, padding = 0, atol=1e-6, print_results=True, random_sampler=np.random.randn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5717f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride = 1\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "conv1 weights gradients all close: True\n",
      "conv2 weights gradients all close: True\n",
      "conv3 weights gradients all close: True\n",
      "bn1 gamma gradients all close: True\n",
      "bn1 beta gradients all close: True\n",
      "bn2 gamma gradients all close: True\n",
      "bn2 beta gradients all close: True\n",
      "bn3 gamma gradients all close: True\n",
      "bn3 beta gradients all close: True\n",
      "\n",
      "stride = 2\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "conv1 weights gradients all close: True\n",
      "conv2 weights gradients all close: True\n",
      "conv3 weights gradients all close: True\n",
      "conv_to_match_dimensions weights gradients all close: True\n",
      "bn_for_residual gamma gradients all close: True\n",
      "bn_for_residual beta gradients all close: True\n",
      "bn1 gamma gradients all close: True\n",
      "bn1 beta gradients all close: True\n",
      "bn2 gamma gradients all close: True\n",
      "bn2 beta gradients all close: True\n",
      "bn3 gamma gradients all close: True\n",
      "bn3 beta gradients all close: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BottleNeckLayer test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "in_channels = 8\n",
    "bottleneck_depth = 2\n",
    "width = 6\n",
    "height = 6\n",
    "\n",
    "expansion_factor = 4\n",
    "n_output_channels = bottleneck_depth * expansion_factor\n",
    "\n",
    "momentum = 0.1\n",
    "\n",
    "for stride_for_downsampling in (1, 2):  # Checking both cases: no downsampling and downsampling\n",
    "    print(f\"stride = {stride_for_downsampling}\")\n",
    "    input_data = np.random.rand(batch_size, in_channels, width, height).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "\n",
    "    output_width = width // stride_for_downsampling\n",
    "    output_height = height // stride_for_downsampling\n",
    "    output_gradient = np.random.rand(batch_size, n_output_channels, output_width, output_height).astype(np.float32)\n",
    "\n",
    "    torch_bottleneck = Bottleneck_torch(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "    my_bottleneck = Bottleneck(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "\n",
    "    conv_layer_pairs = [\n",
    "        (my_bottleneck.conv1, torch_bottleneck.conv1),\n",
    "        (my_bottleneck.conv2, torch_bottleneck.conv2),\n",
    "        (my_bottleneck.conv3, torch_bottleneck.conv3)]\n",
    "\n",
    "    for my_conv, torch_conv in conv_layer_pairs:\n",
    "        my_conv.weights = torch_conv.weight.detach().numpy() #.reshape(my_conv.weights.shape)\n",
    "    \n",
    "    bn_pairs = [\n",
    "        (my_bottleneck.bn1, torch_bottleneck.bn1),\n",
    "        (my_bottleneck.bn2, torch_bottleneck.bn2),\n",
    "        (my_bottleneck.bn3, torch_bottleneck.bn3)]\n",
    "    \n",
    "    for my_bn, torch_bn in bn_pairs:\n",
    "        my_bn.gamma = torch_bn.weight.detach().numpy().reshape(my_bn.gamma.shape)\n",
    "        my_bn.beta = torch_bn.bias.detach().numpy().reshape(my_bn.beta.shape)\n",
    "        my_bn.running_mean = torch_bn.running_mean.detach().numpy().reshape(my_bn.running_mean.shape)\n",
    "        my_bn.running_var = torch_bn.running_var.detach().numpy().reshape(my_bn.running_var.shape)\n",
    "        my_bn.momentum = torch_bn.momentum = momentum\n",
    "    \n",
    "\n",
    "    if my_bottleneck.conv_to_match_dimensions:\n",
    "        my_bottleneck.conv_to_match_dimensions.weights = torch_bottleneck.conv_to_match_dimensions.weight.detach().numpy()\n",
    "        my_bottleneck.bn_for_residual.gamma = torch_bottleneck.bn_for_residual.weight.detach().numpy().reshape(my_bottleneck.bn_for_residual.gamma.shape)\n",
    "        my_bottleneck.bn_for_residual.beta = torch_bottleneck.bn_for_residual.bias.detach().numpy().reshape(my_bottleneck.bn_for_residual.beta.shape)\n",
    "        my_bottleneck.bn_for_residual.running_mean = torch_bottleneck.bn_for_residual.running_mean.detach().numpy().reshape(my_bottleneck.bn_for_residual.running_mean.shape)\n",
    "        my_bottleneck.bn_for_residual.running_var = torch_bottleneck.bn_for_residual.running_var.detach().numpy().reshape(my_bottleneck.bn_for_residual.running_var.shape)\n",
    "        torch_bottleneck.bn_for_residual.momentum = my_bottleneck.bn_for_residual.momentum = momentum\n",
    "    \n",
    "    my_bottleneck.train()\n",
    "    torch_bottleneck.train()\n",
    "\n",
    "    my_out = my_bottleneck.forward(input_data)\n",
    "    torch_out = torch_bottleneck(input_data_torch)\n",
    "\n",
    "    torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "    my_input_g = my_bottleneck.backward(output_gradient)\n",
    "\n",
    "    atol = 1e-4\n",
    "    print(\"output all close:\", np.allclose(my_out, torch_out.detach().numpy(), atol=atol))\n",
    "    print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "    print(\"conv1 weights gradients all close:\", np.allclose(my_bottleneck.conv1.weights_gradient, torch_bottleneck.conv1.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"conv2 weights gradients all close:\", np.allclose(my_bottleneck.conv2.weights_gradient, torch_bottleneck.conv2.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"conv3 weights gradients all close:\", np.allclose(my_bottleneck.conv3.weights_gradient, torch_bottleneck.conv3.weight.grad.detach().numpy(), atol=atol))\n",
    "    if my_bottleneck.conv_to_match_dimensions:\n",
    "        print(\"conv_to_match_dimensions weights gradients all close:\", np.allclose(my_bottleneck.conv_to_match_dimensions.weights_gradient, torch_bottleneck.conv_to_match_dimensions.weight.grad.detach().numpy(), atol=atol))\n",
    "        print(\"bn_for_residual gamma gradients all close:\", np.allclose(my_bottleneck.bn_for_residual.gamma_gradient.flatten(), torch_bottleneck.bn_for_residual.weight.grad.detach().numpy(), atol=atol))\n",
    "        print(\"bn_for_residual beta gradients all close:\", np.allclose(my_bottleneck.bn_for_residual.beta_gradient.flatten(), torch_bottleneck.bn_for_residual.bias.grad.detach().numpy(), atol=atol))\n",
    "    \n",
    "\n",
    "    print(\"bn1 gamma gradients all close:\", np.allclose(my_bottleneck.bn1.gamma_gradient.flatten(), torch_bottleneck.bn1.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"bn1 beta gradients all close:\", np.allclose(my_bottleneck.bn1.beta_gradient.flatten(), torch_bottleneck.bn1.bias.grad.detach().numpy(), atol=atol))\n",
    "    print(\"bn2 gamma gradients all close:\", np.allclose(my_bottleneck.bn2.gamma_gradient.flatten(), torch_bottleneck.bn2.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"bn2 beta gradients all close:\", np.allclose(my_bottleneck.bn2.beta_gradient.flatten(), torch_bottleneck.bn2.bias.grad.detach().numpy(), atol=atol))\n",
    "    print(\"bn3 gamma gradients all close:\", np.allclose(my_bottleneck.bn3.gamma_gradient.flatten(), torch_bottleneck.bn3.weight.grad.detach().numpy(), atol=atol))  \n",
    "    print(\"bn3 beta gradients all close:\", np.allclose(my_bottleneck.bn3.beta_gradient.flatten(), torch_bottleneck.bn3.bias.grad.detach().numpy(), atol=atol))  \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b68d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4f96846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: False\n",
      "input gradients all close: False\n",
      "fc weights gradients all close: False\n",
      "fc bias gradients all close: True\n",
      "It's ok that gradients don't match in eval mode. Batch norm's backward is different in train and eval mode.\n",
      "I didn't implement eval backward since it won't ever be used in training the network\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "resnet 101 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 10).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "my_resnet = resnet101(10, 1)\n",
    "torch_resnet.train()\n",
    "my_resnet.train()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.backward(output_gradient)\n",
    "\n",
    "atol=1e-4\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g.flatten(), torch_input_g.flatten(), atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))\n",
    "\n",
    "print(\"It's ok that gradients don't match in eval mode. \"\\\n",
    "      \"Batch norm's backward is different in train and eval mode.\\n\"\\\n",
    "      \"I didn't implement eval backward since it won't ever be used in training the network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf8d0ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 23.06215228, -28.48848282,  81.80059416, ...,  -8.13147196,\n",
       "          1.08361028, -12.83009572]),\n",
       " array([ 20.20251  , -30.15444  ,  81.40647  , ...,  -7.3269043,\n",
       "          1.8909721, -12.808845 ], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input_g.flatten(), torch_input_g.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44cf20de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: False\n",
      "fc weights gradients all close: True\n",
      "fc bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "atol=1e-2\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "822c960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n",
      "fc weights gradients all close: True\n",
      "fc bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "atol=1e+2\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "511f0e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n",
      "fc weights gradients all close: True\n",
      "fc bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "resnet 101 without batchnormtest\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 10).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch_without_batchnorm(10, 1)\n",
    "torch_resnet.train()\n",
    "my_resnet = resnet101_np_without_batchnorm(10, 1)\n",
    "my_resnet.train()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14fb02cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conv1 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 64, 16, 16).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "torch_resnet.eval()\n",
    "my_resnet = resnet101(10, 1)\n",
    "my_resnet.eval()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.conv1.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet.conv1(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.conv1.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1356b5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: train\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "\n",
      "phase: eval\n",
      "output all close: True\n",
      "input gradients all close: False\n",
      "\n",
      "It's ok that gradients don't match in eval mode. Batch norm's backward is different in train and eval mode.\n",
      "I didn't implement eval backward since it won't ever be used in training the network\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "bn1 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 16\n",
    "n_channels = 64\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "for phase in ['train', 'eval']:\n",
    "    print(f\"phase: {phase}\")\n",
    "\n",
    "    # ! Has been noticed that moving 4 lines below outside the loop leads to not passing tests\n",
    "    input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "\n",
    "    torch_resnet = resnet101_torch(n_classes, n_channels)\n",
    "    my_resnet = resnet101(n_classes, n_channels)\n",
    "    if phase == 'train':\n",
    "        my_resnet.train()\n",
    "        torch_resnet.train()\n",
    "    elif phase == 'eval':\n",
    "        my_resnet.eval()\n",
    "        torch_resnet.eval()\n",
    "    else:\n",
    "        raise Exception(\"unknown phase\") \n",
    "\n",
    "    my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "    my_out = my_resnet.bn1.forward(input_data)\n",
    "\n",
    "    torch_out = torch_resnet.bn1(input_data_torch)\n",
    "    torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "    torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "    my_input_g = my_resnet.bn1.backward(output_gradient)\n",
    "\n",
    "    atol=1e-6\n",
    "\n",
    "    print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "    print(\"input gradients all close:\", np.allclose(my_input_g.flatten(), torch_input_g.flatten(), atol=atol))\n",
    "    print()\n",
    "\n",
    "print(\"It's ok that gradients don't match in eval mode. \"\\\n",
    "      \"Batch norm's backward is different in train and eval mode.\\n\"\\\n",
    "      \"I didn't implement eval backward since it won't ever be used in training the network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6997b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57fc2d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: train\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "\n",
      "phase: eval\n",
      "output all close: True\n",
      "input gradients all close: False\n",
      "\n",
      "It's ok that gradients don't match in eval mode. Batch norm's backward is different in train and eval mode.\n",
      "I didn't implement eval backward since it won't ever be used in training the network\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conv2_x test\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 8\n",
    "n_channels = 64\n",
    "\n",
    "for phase in ['train', 'eval']:\n",
    "    print(f\"phase: {phase}\")\n",
    "\n",
    "    input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, 256, 8, 8).astype(np.float32)\n",
    "\n",
    "    torch_resnet = resnet101_torch(10, 1)\n",
    "    my_resnet = resnet101(10, 1)\n",
    "\n",
    "    if phase == 'train':\n",
    "        my_resnet.train()\n",
    "        torch_resnet.train()\n",
    "    elif phase == 'eval':\n",
    "        my_resnet.eval()\n",
    "        torch_resnet.eval()\n",
    "    else:\n",
    "        raise Exception(\"unknown phase\") \n",
    "\n",
    "\n",
    "    my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "    my_out = my_resnet.conv2_x.forward(input_data)\n",
    "\n",
    "    torch_out = torch_resnet.conv2_x(input_data_torch)\n",
    "    torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "    torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "    my_input_g = my_resnet.conv2_x.backward(output_gradient)\n",
    "\n",
    "    atol=1e-3\n",
    "\n",
    "    print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "    print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "    print()\n",
    "\n",
    "print(\"It's ok that gradients don't match in eval mode. \"\\\n",
    "      \"Batch norm's backward is different in train and eval mode.\\n\"\\\n",
    "      \"I didn't implement eval backward since it won't ever be used in training the network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9234786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conv3_x test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 8\n",
    "n_channels = 256\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 512, 4, 4).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "my_resnet = resnet101(10, 1)\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.conv3_x.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet.conv3_x(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.conv3_x.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c31786ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conv4_x test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 4\n",
    "n_channels = 512\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 1024, height//2, width//2).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "my_resnet = resnet101(10, 1)\n",
    "\n",
    "my_resnet.train()\n",
    "torch_resnet.train()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.conv4_x.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet.conv4_x(input_data_torch)\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "\n",
    "my_input_g = my_resnet.conv4_x.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e639853e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 217.90011733,    0.        , -159.54153584, ...,    0.        ,\n",
       "           0.        ,    0.        ]),\n",
       " array([ 217.23167,    0.     , -159.80835, ...,    0.     ,    0.     ,\n",
       "           0.     ], dtype=float32))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input_g.flatten(), torch_input_g.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b704ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conv5_x test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 2\n",
    "n_channels = 1024\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 2048, 1, 1).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "my_resnet = resnet101(10, 1)\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.conv5_x.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet.conv5_x(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.conv5_x.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "affa6502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n",
      "before adam weights gradients all close: True\n",
      "before adam bias gradients all close: True\n",
      "[[1.3891779  1.7336051  1.2286544 ]\n",
      " [1.3048313  0.99781084 1.0630406 ]\n",
      " [1.6351756  1.4931997  1.3154104 ]\n",
      " [1.5244397  1.4477336  1.157729  ]\n",
      " [0.9432854  0.58816004 0.951926  ]\n",
      " [1.6254252  2.1075394  1.3502117 ]] \n",
      " [[1.3891779  1.733605   1.2286544 ]\n",
      " [1.3048313  0.99781084 1.0630406 ]\n",
      " [1.6351756  1.4931998  1.3154104 ]\n",
      " [1.5244397  1.4477336  1.1577291 ]\n",
      " [0.9432854  0.58816004 0.95192605]\n",
      " [1.6254252  2.1075392  1.3502117 ]]\n",
      "after adam weights gradients all close: True\n",
      "after adam bias gradients all close: True\n",
      "[[1.3891779  1.7336051  1.2286544 ]\n",
      " [1.3048313  0.99781084 1.0630406 ]\n",
      " [1.6351756  1.4931997  1.3154104 ]\n",
      " [1.5244397  1.4477336  1.157729  ]\n",
      " [0.9432854  0.58816004 0.951926  ]\n",
      " [1.6254252  2.1075394  1.3502117 ]] \n",
      " [[1.3891779  1.733605   1.2286544 ]\n",
      " [1.3048313  0.99781084 1.0630406 ]\n",
      " [1.6351756  1.4931998  1.3154104 ]\n",
      " [1.5244397  1.4477336  1.1577291 ]\n",
      " [0.9432854  0.58816004 0.95192605]\n",
      " [1.6254252  2.1075392  1.3502117 ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AdamOptimizer test\n",
    "\"\"\"\n",
    "\n",
    "from torch.optim import Adam as Adam_torch\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "batch_size = 5\n",
    "input_data = np.random.rand(batch_size, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_features).astype(np.float32)\n",
    "\n",
    "torch_fc = torch.nn.Linear(n_input_features, n_output_features)\n",
    "torch_out = torch_fc(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_wg = torch_fc.weight.grad.detach().numpy().T\n",
    "torch_bg = torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_fc = FullyConnectedLayer(n_input_features, n_output_features)\n",
    "my_fc.weights = torch_fc.weight.detach().numpy().T\n",
    "my_fc.bias = torch_fc.bias.detach().numpy().reshape(-1, 1).T\n",
    "my_out = my_fc.forward(input_data)\n",
    "my_input_g = my_fc.backward(output_gradient)\n",
    "my_wg = my_fc.weights_gradient\n",
    "my_bg = my_fc.bias_gradient\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "print(\"before adam weights gradients all close:\", np.allclose(my_wg, torch_wg, atol=atol))\n",
    "print(\"before adam bias gradients all close:\", np.allclose(my_bg, torch_bg, atol=atol))\n",
    "print(my_wg, \"\\n\", torch_wg)\n",
    "\n",
    "my_adam = AdamOptimizer(my_fc.get_trainable_layers(), 0.001, 0.9, 0.999, 1e-8)\n",
    "my_adam.step()\n",
    "\n",
    "torch_adam = Adam_torch(torch_fc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n",
    "torch_adam.step()\n",
    "\n",
    "print(\"after adam weights gradients all close:\", np.allclose(my_fc.weights_gradient, torch_fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"after adam bias gradients all close:\", np.allclose(my_fc.bias_gradient, torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T, atol=atol))\n",
    "print(my_fc.weights_gradient, \"\\n\", torch_fc.weight.grad.detach().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bca9fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "my_conv forward time: 0.5448141098022461\n",
      "my_conv_with_loops forward time: 3.00417423248291\n",
      "torch_conv forward time: 0.07560348510742188\n",
      "my_conv backward time: 0.684333324432373\n",
      "my_conv_with_loops backward time: 2.6092746257781982\n",
      "torch_conv backward time: 0.17493200302124023\n",
      "\n",
      "batch_size: 2\n",
      "my_conv forward time: 0.5803859233856201\n",
      "my_conv_with_loops forward time: 2.1322405338287354\n",
      "torch_conv forward time: 0.15914392471313477\n",
      "my_conv backward time: 1.183729648590088\n",
      "my_conv_with_loops backward time: 4.955396413803101\n",
      "torch_conv backward time: 0.365201473236084\n",
      "\n",
      "batch_size: 4\n",
      "my_conv forward time: 1.063096284866333\n",
      "my_conv_with_loops forward time: 2.2665672302246094\n",
      "torch_conv forward time: 0.16541314125061035\n",
      "my_conv backward time: 2.5409464836120605\n",
      "my_conv_with_loops backward time: 9.793752670288086\n",
      "torch_conv backward time: 0.414461612701416\n",
      "\n",
      "batch_size: 8\n",
      "my_conv forward time: 1.992255687713623\n",
      "my_conv_with_loops forward time: 2.455413341522217\n",
      "torch_conv forward time: 0.20453190803527832\n",
      "my_conv backward time: 4.4292519092559814\n",
      "my_conv_with_loops backward time: 21.643410205841064\n",
      "torch_conv backward time: 0.5241467952728271\n",
      "\n",
      "batch_size: 16\n",
      "my_conv forward time: 4.281964540481567\n",
      "my_conv_with_loops forward time: 3.1336183547973633\n",
      "torch_conv forward time: 0.3031580448150635\n",
      "my_conv backward time: 9.075878858566284\n",
      "my_conv_with_loops backward time: 41.86888933181763\n",
      "torch_conv backward time: 0.500030517578125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d vs Conv2dWithLoops vs torch.nn.Conv2d time comparison forward and backward\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 3\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "for batch_size in [1, 2, 4, 8, 16]:\n",
    "    print(\"batch_size:\", batch_size)\n",
    "\n",
    "    input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "    torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "    my_conv_with_loops = Conv2dWithLoops(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv_with_loops.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    n_iterations = 1000\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out = my_conv.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out_with_loops = my_conv_with_loops.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out = torch_conv(input_data_torch)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g = my_conv.backward_as_matrix_multiplication(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g_with_loops = my_conv_with_loops.backward(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv backward time: {end - start}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07955d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bbf79ea567ebf64b92b9cf68f2b08a0cf8db5ff8a7f29f56d99d802810464d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
