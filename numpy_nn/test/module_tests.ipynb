{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427c3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to project's root directory to make\n",
    "# numpy_nn and pytorch_nn packages accessable \n",
    "# %cd ../..\n",
    "\n",
    "\n",
    "# # Another possible solution is appending to the sys.path\n",
    "\n",
    "import sys\n",
    "import  os\n",
    "project_root = os.path.dirname(os.path.dirname(sys.path[0]))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a56421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SystemPoint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from test_layer import test_module\n",
    "\n",
    "from numpy_nn.modules.np_nn import (\n",
    "    FullyConnectedLayer,\n",
    "    ReLULayer,\n",
    "    SigmoidLayer,\n",
    "    ReLULayer,\n",
    "    AdamOptimizer,\n",
    "    CrossEntropyLoss,\n",
    "    LinearActivation,\n",
    "    Sequential,\n",
    "    Optimizer,\n",
    "    SoftMaxLayer,\n",
    "    GradientDescentOptimizer,\n",
    "    CrossEntropyLossWithSoftMax,\n",
    "    Conv2d,\n",
    "    Conv2dWithLoops,\n",
    "    Flatten,\n",
    "    MaxPool2d,\n",
    "    AdamOptimizer,\n",
    "    BatchNormalization2d,\n",
    "    ActivationLayer,\n",
    "    TrainableLayer,\n",
    ")\n",
    "\n",
    "from pytorch_nn.models.resnet import (\n",
    "    Bottleneck as Bottleneck_torch,\n",
    "    resnet101 as resnet101_torch\n",
    ")\n",
    "\n",
    "from pytorch_nn.models.resnet_without_batchnorm import (\n",
    "    Bottleneck as Bottleneck_torch_without_batchnorm,\n",
    "    resnet101 as resnet101_torch_without_batchnorm\n",
    ")\n",
    "\n",
    "from numpy_nn.models.resnet import Bottleneck, resnet101\n",
    "\n",
    "from numpy_nn.models.resnet_without_batchnorm import (\n",
    "    Bottleneck as Bottleneck_np_without_batchnorm,\n",
    "    resnet101 as resnet101_np_without_batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38f73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload a user's module test_layer\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c80e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_test(my_conv2d_constructor: Callable, batch_size: int,\n",
    "                input_height: int, input_width: int, n_input_channels,\n",
    "                n_output_channels, kernel_size: int, stride: int, padding: int,\n",
    "                bias: bool, atol: float = 1e-5, random_sampler: Callable = np.random.rand) -> None:\n",
    "    \n",
    "    my_conv2d_kwargs = torch_conv2d_kwargs = {\n",
    "        \"in_channels\": n_input_channels,\n",
    "        \"out_channels\": n_output_channels,\n",
    "        \"kernel_size\": kernel_size,\n",
    "        \"stride\": stride,\n",
    "        \"padding\": padding,\n",
    "        \"bias\": bias\n",
    "    }\n",
    "\n",
    "    output_height = (input_height + 2 * padding - kernel_size) // stride + 1\n",
    "    output_width = (input_width + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "    input_shape = (batch_size, n_input_channels, input_height, input_width)\n",
    "    output_shape = (batch_size, n_output_channels, output_height, output_width)\n",
    "\n",
    "    my_conv2d = my_conv2d_constructor(**my_conv2d_kwargs)\n",
    "    torch_conv2d = torch.nn.Conv2d(**torch_conv2d_kwargs)\n",
    "\n",
    "    test_module(my_conv2d, torch_conv2d, input_shape,\n",
    "                output_shape, atol=atol, random_sampler=random_sampler)\n",
    "    \n",
    "\n",
    "\n",
    "def max_pool_2d_test(batch_size: int, height: int, width: int, n_channels: int, kernel_size: int,\n",
    "                     stride: int, padding: int, atol: float = 1e-5, random_sampler: Callable = np.random.rand):\n",
    "    \n",
    "    output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "    output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "    my_pool_args = torch_pool_args = [kernel_size, stride, padding]\n",
    "\n",
    "    my_pool = MaxPool2d(*my_pool_args)\n",
    "    torch_pool = torch.nn.MaxPool2d(*torch_pool_args)\n",
    "\n",
    "    test_module(my_pool, torch_pool, input_shape=[batch_size, n_channels, height, width],\n",
    "               output_shape=[batch_size, n_channels, output_height, output_width],\n",
    "               atol=atol, random_sampler = random_sampler)\n",
    "\n",
    "\n",
    "\n",
    "def activation_test(my_activation: Callable,\n",
    "                    torch_activation: Callable,\n",
    "                    input_dim: List[int],\n",
    "                    atol: float = 1e-5):\n",
    "    \"\"\"\n",
    "    Samples input data and output gradient from a uniform\n",
    "    distribution and tests if the output and input gradients\n",
    "    are close to the ones computed by pytorch\n",
    "    \"\"\"\n",
    "    test_module(my_activation(), torch_activation(), input_shape=input_dim, output_shape=input_dim, atol=atol)\n",
    "\n",
    "def relu_test(input_dim: List[int], atol: float = 1e-5):\n",
    "    activation_test(ReLULayer, torch.nn.ReLU, input_dim, atol=atol)\n",
    "\n",
    "def sigmoid_test(input_dim: List[int], atol: float = 1e-5):\n",
    "    activation_test(SigmoidLayer, torch.nn.Sigmoid, input_dim, atol=atol)\n",
    "\n",
    "\n",
    "\n",
    "def flatten_test(input_shape: List[int], atol: float = 1e-5,\n",
    "                 random_sampler: Callable = np.random.rand):\n",
    "    batch_size, *rest_input_dim = input_shape\n",
    "    output_shape = [batch_size, np.prod(rest_input_dim)]\n",
    "\n",
    "    test_module(Flatten(), torch.nn.Flatten(), input_shape=input_shape,\n",
    "               output_shape=output_shape, atol=atol, random_sampler=random_sampler)\n",
    "\n",
    "\n",
    "\n",
    "def batchnorm2d_iterative_test(n_channels: int, batch_size: int, height: int,\n",
    "                     width: int, n_iter: int, phase: str = \"train\",\n",
    "                     momentum: float = 0.1, atol: float = 1e-5,\n",
    "                     random_sampler: Callable = np.random.rand, print_tensors = False) -> None:\n",
    "    \n",
    "    # since each iteration changes the running mean, running variance, mean and variance\n",
    "    # this test runs several iterations and checks that the results are the same\n",
    "\n",
    "    # maybe there should be a custom test that calls\n",
    "    # forward multiple times and calls backward only once\n",
    "    \n",
    "    input_shape = output_shape = [batch_size, n_channels, height, width]\n",
    "\n",
    "    my_bn = BatchNormalization2d(n_channels, momentum=momentum)\n",
    "    torch_bn = torch.nn.BatchNorm2d(n_channels, momentum=momentum)\n",
    "\n",
    "    if phase == \"eval\":\n",
    "        my_bn.eval()\n",
    "        torch_bn.eval()\n",
    "    else:\n",
    "        my_bn.train()\n",
    "        torch_bn.train()\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        print(\"Iteration:\", i)\n",
    "        print(my_bn.training)\n",
    "        test_module(my_bn, torch_bn, input_shape, output_shape,\n",
    "                    atol=atol, random_sampler=random_sampler,\n",
    "                    skip_parameter_copying=bool(i), print_tensors=print_tensors)\n",
    "        # reset torch gradients\n",
    "        torch_bn.weight.grad = None\n",
    "        torch_bn.bias.grad = None\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bd5af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "True\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Running means are equal\n",
      "Running vars are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n",
      "Iteration: 1\n",
      "True\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Running means are equal\n",
      "Running vars are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n",
      "Iteration: 2\n",
      "True\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Running means are equal\n",
      "Running vars are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batchnorm2d_iterative_test(n_channels=5, batch_size=4, height=8, width=8,\n",
    "                           n_iter=3, phase=\"train\", momentum=0.1,\n",
    "                           atol=1e-4, random_sampler=np.random.rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059674e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "True\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Running means are equal\n",
      "Running vars are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n",
      "Iteration: 1\n",
      "True\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Running means are equal\n",
      "Running vars are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n",
      "Iteration: 2\n",
      "True\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Running means are equal\n",
      "Running vars are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batchnorm2d_iterative_test(n_channels=5, batch_size=4, height=8, width=8,\n",
    "                           n_iter=3, phase=\"train\", momentum=0.8,\n",
    "                           atol=1e-4, random_sampler=np.random.rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54353c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2dWithLoops test\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n",
      "Conv2d test\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n",
      "\n",
      "Conv2d test without bias\n",
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Weight gradients are equal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2dWithLoops and Conv2d tests\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "print(\"Conv2dWithLoops test\")\n",
    "conv2d_test(Conv2dWithLoops, batch_size, height, width, n_input_channels, n_output_channels, kernel_size, stride, padding, bias=True)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Conv2d test\")\n",
    "conv2d_test(Conv2d, batch_size, height, width, n_input_channels, n_output_channels, kernel_size, stride, padding, bias=True)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Conv2d test without bias\")\n",
    "conv2d_test(Conv2d, batch_size, height, width, n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af989443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "Weight gradients are equal\n",
      "Bias gradients are equal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FullyConnectedLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_neurons = 6\n",
    "n_output_neurons = 3\n",
    "n_samples = 5\n",
    "\n",
    "my_fc_params = torch_fc_params = [n_input_neurons, n_output_neurons]\n",
    "\n",
    "test_module(FullyConnectedLayer(*my_fc_params), torch.nn.Linear(*torch_fc_params),\n",
    "            input_shape=[n_samples, n_input_neurons], output_shape=[n_samples, n_output_neurons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba7f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_val all close: True\n",
      "loss gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CrossEntropyLoss test\n",
    "\"\"\"\n",
    "\n",
    "def one_hot(y: np.ndarray, n_classes: int):\n",
    "    encoded = np.zeros((y.size, n_classes))\n",
    "    encoded[np.arange(y.size), y] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "n_classes = 3\n",
    "pred = np.random.rand(batch_size, n_classes).astype(np.float32)\n",
    "true = one_hot(np.random.randint(0, n_classes, batch_size), n_classes)\n",
    "pred_torch = torch.from_numpy(pred).float()\n",
    "true_torch = torch.from_numpy(true).float()\n",
    "pred_torch.requires_grad = True\n",
    "\n",
    "torch_loss  = torch.nn.CrossEntropyLoss()\n",
    "torch_loss_val = torch_loss(pred_torch, true_torch)\n",
    "torch_loss_val.backward()\n",
    "\n",
    "my_loss = CrossEntropyLossWithSoftMax()\n",
    "my_loss_val = my_loss.forward(pred, true)\n",
    "my_loss.backward()\n",
    "\n",
    "print(\"loss_val all close:\", np.allclose(my_loss_val, torch_loss_val.detach().numpy()))\n",
    "print(\"loss gradients all close:\", np.allclose(my_loss.backward(), pred_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a8b413b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0581,  0.0729, -0.1309],\n",
       "        [ 0.0631,  0.0617, -0.1248],\n",
       "        [ 0.0558,  0.0446, -0.1004],\n",
       "        [ 0.0668,  0.0803, -0.1471],\n",
       "        [-0.1166,  0.0491,  0.0675]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_torch.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6887e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equal\n",
      "Input gradients are equal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ReLU test\"\"\"\n",
    "n_input_features = 6\n",
    "n_samples = 5\n",
    "\n",
    "relu_test([n_samples, n_input_features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0083991a",
   "metadata": {},
   "source": [
    "If we sample random numbers from normanl distribution instead of uniform distribution (randn instead of rand) sigmoid layer won't pass tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "481886fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "\n",
      "Outputs are equal\n",
      "Input gradients are equal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"SigmoidLayer test\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_samples = 5\n",
    "height = 4\n",
    "width = 4\n",
    "\n",
    "sigmoid_test([n_samples, n_input_features])\n",
    "print()\n",
    "sigmoid_test([n_samples, n_input_features, height, width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1f6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equal\n",
      "Input gradients are equal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Flatten test\"\"\"\n",
    "\n",
    "flatten_test(input_shape = [2, 3, 5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58326a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are equal\n",
      "Input gradients are equal\n",
      "\n",
      "Outputs are equal\n",
      "Input gradients are equal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"MaxPool2d tests\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "n_channels = 3\n",
    "height = 16\n",
    "width = 16\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "max_pool_2d_test(batch_size, height, width, n_channels, kernel_size, stride, padding, atol=1e-5)\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "max_pool_2d_test(batch_size = 2, height = 6, width = 4, n_channels = 3,\n",
    "                 kernel_size = 2, stride = 1, padding = 0, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d3319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890f501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b5717f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride = 1\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "conv1 weights gradients all close: True\n",
      "conv2 weights gradients all close: True\n",
      "conv3 weights gradients all close: True\n",
      "bn1 gamma gradients all close: True\n",
      "bn1 beta gradients all close: True\n",
      "bn2 gamma gradients all close: True\n",
      "bn2 beta gradients all close: True\n",
      "bn3 gamma gradients all close: True\n",
      "bn3 beta gradients all close: True\n",
      "\n",
      "stride = 2\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "conv1 weights gradients all close: True\n",
      "conv2 weights gradients all close: True\n",
      "conv3 weights gradients all close: True\n",
      "conv_to_match_dimensions weights gradients all close: True\n",
      "bn_for_residual gamma gradients all close: True\n",
      "bn_for_residual beta gradients all close: True\n",
      "bn1 gamma gradients all close: True\n",
      "bn1 beta gradients all close: True\n",
      "bn2 gamma gradients all close: True\n",
      "bn2 beta gradients all close: True\n",
      "bn3 gamma gradients all close: True\n",
      "bn3 beta gradients all close: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BottleNeckLayer test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "in_channels = 8\n",
    "bottleneck_depth = 2\n",
    "width = 6\n",
    "height = 6\n",
    "\n",
    "expansion_factor = 4\n",
    "n_output_channels = bottleneck_depth * expansion_factor\n",
    "\n",
    "momentum = 0.1\n",
    "\n",
    "for stride_for_downsampling in (1, 2):  # Checking both cases: no downsampling and downsampling\n",
    "    print(f\"stride = {stride_for_downsampling}\")\n",
    "    input_data = np.random.rand(batch_size, in_channels, width, height).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "\n",
    "    output_width = width // stride_for_downsampling\n",
    "    output_height = height // stride_for_downsampling\n",
    "    output_gradient = np.random.rand(batch_size, n_output_channels, output_width, output_height).astype(np.float32)\n",
    "\n",
    "    torch_bottleneck = Bottleneck_torch(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "    my_bottleneck = Bottleneck(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "\n",
    "    conv_layer_pairs = [\n",
    "        (my_bottleneck.conv1, torch_bottleneck.conv1),\n",
    "        (my_bottleneck.conv2, torch_bottleneck.conv2),\n",
    "        (my_bottleneck.conv3, torch_bottleneck.conv3)]\n",
    "\n",
    "    for my_conv, torch_conv in conv_layer_pairs:\n",
    "        my_conv.weights = torch_conv.weight.detach().numpy() #.reshape(my_conv.weights.shape)\n",
    "    \n",
    "    bn_pairs = [\n",
    "        (my_bottleneck.bn1, torch_bottleneck.bn1),\n",
    "        (my_bottleneck.bn2, torch_bottleneck.bn2),\n",
    "        (my_bottleneck.bn3, torch_bottleneck.bn3)]\n",
    "    \n",
    "    for my_bn, torch_bn in bn_pairs:\n",
    "        my_bn.gamma = torch_bn.weight.detach().numpy().reshape(my_bn.gamma.shape)\n",
    "        my_bn.beta = torch_bn.bias.detach().numpy().reshape(my_bn.beta.shape)\n",
    "        my_bn.running_mean = torch_bn.running_mean.detach().numpy().reshape(my_bn.running_mean.shape)\n",
    "        my_bn.running_var = torch_bn.running_var.detach().numpy().reshape(my_bn.running_var.shape)\n",
    "        my_bn.momentum = torch_bn.momentum = momentum\n",
    "    \n",
    "\n",
    "    if my_bottleneck.conv_to_match_dimensions:\n",
    "        my_bottleneck.conv_to_match_dimensions.weights = torch_bottleneck.conv_to_match_dimensions.weight.detach().numpy()\n",
    "        my_bottleneck.bn_for_residual.gamma = torch_bottleneck.bn_for_residual.weight.detach().numpy().reshape(my_bottleneck.bn_for_residual.gamma.shape)\n",
    "        my_bottleneck.bn_for_residual.beta = torch_bottleneck.bn_for_residual.bias.detach().numpy().reshape(my_bottleneck.bn_for_residual.beta.shape)\n",
    "        my_bottleneck.bn_for_residual.running_mean = torch_bottleneck.bn_for_residual.running_mean.detach().numpy().reshape(my_bottleneck.bn_for_residual.running_mean.shape)\n",
    "        my_bottleneck.bn_for_residual.running_var = torch_bottleneck.bn_for_residual.running_var.detach().numpy().reshape(my_bottleneck.bn_for_residual.running_var.shape)\n",
    "        torch_bottleneck.bn_for_residual.momentum = my_bottleneck.bn_for_residual.momentum = momentum\n",
    "    \n",
    "    my_bottleneck.train()\n",
    "    torch_bottleneck.train()\n",
    "\n",
    "    my_out = my_bottleneck.forward(input_data)\n",
    "    torch_out = torch_bottleneck(input_data_torch)\n",
    "\n",
    "    torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "    my_input_g = my_bottleneck.backward(output_gradient)\n",
    "\n",
    "    atol = 1e-4\n",
    "    print(\"output all close:\", np.allclose(my_out, torch_out.detach().numpy(), atol=atol))\n",
    "    print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "    print(\"conv1 weights gradients all close:\", np.allclose(my_bottleneck.conv1.weights_gradient, torch_bottleneck.conv1.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"conv2 weights gradients all close:\", np.allclose(my_bottleneck.conv2.weights_gradient, torch_bottleneck.conv2.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"conv3 weights gradients all close:\", np.allclose(my_bottleneck.conv3.weights_gradient, torch_bottleneck.conv3.weight.grad.detach().numpy(), atol=atol))\n",
    "    if my_bottleneck.conv_to_match_dimensions:\n",
    "        print(\"conv_to_match_dimensions weights gradients all close:\", np.allclose(my_bottleneck.conv_to_match_dimensions.weights_gradient, torch_bottleneck.conv_to_match_dimensions.weight.grad.detach().numpy(), atol=atol))\n",
    "        print(\"bn_for_residual gamma gradients all close:\", np.allclose(my_bottleneck.bn_for_residual.gamma_gradient.flatten(), torch_bottleneck.bn_for_residual.weight.grad.detach().numpy(), atol=atol))\n",
    "        print(\"bn_for_residual beta gradients all close:\", np.allclose(my_bottleneck.bn_for_residual.beta_gradient.flatten(), torch_bottleneck.bn_for_residual.bias.grad.detach().numpy(), atol=atol))\n",
    "    \n",
    "\n",
    "    print(\"bn1 gamma gradients all close:\", np.allclose(my_bottleneck.bn1.gamma_gradient.flatten(), torch_bottleneck.bn1.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"bn1 beta gradients all close:\", np.allclose(my_bottleneck.bn1.beta_gradient.flatten(), torch_bottleneck.bn1.bias.grad.detach().numpy(), atol=atol))\n",
    "    print(\"bn2 gamma gradients all close:\", np.allclose(my_bottleneck.bn2.gamma_gradient.flatten(), torch_bottleneck.bn2.weight.grad.detach().numpy(), atol=atol))\n",
    "    print(\"bn2 beta gradients all close:\", np.allclose(my_bottleneck.bn2.beta_gradient.flatten(), torch_bottleneck.bn2.bias.grad.detach().numpy(), atol=atol))\n",
    "    print(\"bn3 gamma gradients all close:\", np.allclose(my_bottleneck.bn3.gamma_gradient.flatten(), torch_bottleneck.bn3.weight.grad.detach().numpy(), atol=atol))  \n",
    "    print(\"bn3 beta gradients all close:\", np.allclose(my_bottleneck.bn3.beta_gradient.flatten(), torch_bottleneck.bn3.bias.grad.detach().numpy(), atol=atol))  \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b68d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4f96846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: False\n",
      "input gradients all close: False\n",
      "fc weights gradients all close: False\n",
      "fc bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "resnet 101 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 10).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "my_resnet = resnet101(10, 1)\n",
    "torch_resnet.train()\n",
    "my_resnet.train()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "822c960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: False\n",
      "fc weights gradients all close: True\n",
      "fc bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "atol=1e-0\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "511f0e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n",
      "fc weights gradients all close: True\n",
      "fc bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "resnet 101 without batchnormtest\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 10).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch_without_batchnorm(10, 1)\n",
    "torch_resnet.train()\n",
    "my_resnet = resnet101_np_without_batchnorm(10, 1)\n",
    "my_resnet.train()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.backward(output_gradient)\n",
    "\n",
    "atol=1e-4\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "\n",
    "print(\"fc weights gradients all close:\", np.allclose(my_resnet.fc.weights_gradient, torch_resnet.fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "print(\"fc bias gradients all close:\", np.allclose(my_resnet.fc.bias_gradient, torch_resnet.fc.bias.grad.detach().numpy(), atol=atol))\n",
    "\n",
    "\n",
    "print(\"It's ok that gradients don't match in eval mode. \"\\\n",
    "      \"Batch norm's backward is different in train and eval mode.\\n\"\\\n",
    "      \"I didn't implement eval backward since it won't ever be used in training the network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14fb02cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conv1 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 64, 16, 16).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "torch_resnet.eval()\n",
    "my_resnet = resnet101(10, 1)\n",
    "my_resnet.eval()\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.conv1.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet.conv1(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.conv1.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1356b5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: train\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "\n",
      "phase: eval\n",
      "output all close: True\n",
      "input gradients all close: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "bn1 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 16\n",
    "n_channels = 64\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "for phase in ['train', 'eval']:\n",
    "    print(f\"phase: {phase}\")\n",
    "\n",
    "    # ! Has been noticed that moving 4 lines below outside the loop leads to not passing tests\n",
    "    input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "\n",
    "    torch_resnet = resnet101_torch(n_classes, n_channels)\n",
    "    my_resnet = resnet101(n_classes, n_channels)\n",
    "    if phase == 'train':\n",
    "        my_resnet.train()\n",
    "        torch_resnet.train()\n",
    "    elif phase == 'eval':\n",
    "        my_resnet.eval()\n",
    "        torch_resnet.eval()\n",
    "    else:\n",
    "        raise Exception(\"unknown phase\") \n",
    "\n",
    "    my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "    my_out = my_resnet.bn1.forward(input_data)\n",
    "\n",
    "    torch_out = torch_resnet.bn1(input_data_torch)\n",
    "    torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "    torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "    my_input_g = my_resnet.bn1.backward(output_gradient)\n",
    "\n",
    "    atol=1e-3\n",
    "\n",
    "    print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "    print(\"input gradients all close:\", np.allclose(my_input_g.flatten(), torch_input_g.flatten(), atol=atol))\n",
    "    print()\n",
    "\n",
    "print(\"It's ok that gradients don't match in eval mode. \"\\\n",
    "      \"Batch norm's backward is different in train and eval mode.\\n\"\\\n",
    "      \"I didn't implement eval backward since it won't ever be used in training the network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6997b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57fc2d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: train\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "\n",
      "phase: eval\n",
      "output all close: True\n",
      "input gradients all close: False\n",
      "\n",
      "it's ok that gradients don't match in eval mode. Batch norm's backward is different in train and eval mode.\n",
      "I didn't implement eval backward since it won't ever be used in training the network\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "conv2_x test\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 8\n",
    "n_channels = 64\n",
    "\n",
    "for phase in ['train', 'eval']:\n",
    "    print(f\"phase: {phase}\")\n",
    "\n",
    "    input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, 256, 8, 8).astype(np.float32)\n",
    "\n",
    "    torch_resnet = resnet101_torch(10, 1)\n",
    "    my_resnet = resnet101(10, 1)\n",
    "\n",
    "    if phase == 'train':\n",
    "        my_resnet.train()\n",
    "        torch_resnet.train()\n",
    "    elif phase == 'eval':\n",
    "        my_resnet.eval()\n",
    "        torch_resnet.eval()\n",
    "    else:\n",
    "        raise Exception(\"unknown phase\") \n",
    "\n",
    "\n",
    "    my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "    my_out = my_resnet.conv2_x.forward(input_data)\n",
    "\n",
    "    torch_out = torch_resnet.conv2_x(input_data_torch)\n",
    "    torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "    torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "    my_input_g = my_resnet.conv2_x.backward(output_gradient)\n",
    "\n",
    "    atol=1e-3\n",
    "\n",
    "    print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "    print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "    print()\n",
    "\n",
    "print(\"It's ok that gradients don't match in eval mode. \"\\\n",
    "      \"Batch norm's backward is different in train and eval mode.\\n\"\\\n",
    "      \"I didn't implement eval backward since it won't ever be used in training the network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24faafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv3_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 8\n",
    "# n_channels = 256\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 512, 4, 4).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv3_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv3_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv3_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31786ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv4_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 4\n",
    "# n_channels = 512\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 1024, 2, 2).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv4_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv4_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv4_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv5_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 2\n",
    "# n_channels = 1024\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 2048, 1, 1).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv5_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv5_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv5_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# AdamOptimizer test\n",
    "# \"\"\"\n",
    "\n",
    "# from torch.optim import Adam as Adam_torch\n",
    "\n",
    "# n_input_features = 6\n",
    "# n_output_features = 3\n",
    "# batch_size = 5\n",
    "# input_data = np.random.rand(batch_size, n_input_features).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, n_output_features).astype(np.float32)\n",
    "\n",
    "# torch_fc = torch.nn.Linear(n_input_features, n_output_features)\n",
    "# torch_out = torch_fc(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_wg = torch_fc.weight.grad.detach().numpy().T\n",
    "# torch_bg = torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "# my_fc = FullyConnectedLayer(n_input_features, n_output_features)\n",
    "# my_fc.weights = torch_fc.weight.detach().numpy().T\n",
    "# my_fc.bias = torch_fc.bias.detach().numpy().reshape(-1, 1).T\n",
    "# my_out = my_fc.forward(input_data)\n",
    "# my_input_g = my_fc.backward(output_gradient)\n",
    "# my_wg = my_fc.weights_gradient\n",
    "# my_bg = my_fc.bias_gradient\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "# print(\"before adam weights gradients all close:\", np.allclose(my_wg, torch_wg, atol=atol))\n",
    "# print(\"before adam bias gradients all close:\", np.allclose(my_bg, torch_bg, atol=atol))\n",
    "# print(my_wg, \"\\n\", torch_wg)\n",
    "\n",
    "# my_adam = AdamOptimizer(my_fc.get_trainable_layers(), 0.001, 0.9, 0.999, 1e-8)\n",
    "# my_adam.step()\n",
    "\n",
    "# torch_adam = Adam_torch(torch_fc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n",
    "# torch_adam.step()\n",
    "\n",
    "# print(\"after adam weights gradients all close:\", np.allclose(my_fc.weights_gradient, torch_fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "# print(\"after adam bias gradients all close:\", np.allclose(my_fc.bias_gradient, torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T, atol=atol))\n",
    "# print(my_fc.weights_gradient, \"\\n\", torch_fc.weight.grad.detach().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca9fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "my_conv forward time: 0.5312726497650146\n",
      "my_conv_with_loops forward time: 2.215890645980835\n",
      "torch_conv forward time: 0.06669497489929199\n",
      "my_conv backward time: 0.7423074245452881\n",
      "my_conv_with_loops backward time: 2.638596773147583\n",
      "torch_conv backward time: 0.1759324073791504\n",
      "\n",
      "batch_size: 2\n",
      "my_conv forward time: 0.5586578845977783\n",
      "my_conv_with_loops forward time: 2.2504141330718994\n",
      "torch_conv forward time: 0.17436599731445312\n",
      "my_conv backward time: 1.2079191207885742\n",
      "my_conv_with_loops backward time: 5.122586965560913\n",
      "torch_conv backward time: 0.3848881721496582\n",
      "\n",
      "batch_size: 4\n",
      "my_conv forward time: 1.064781904220581\n",
      "my_conv_with_loops forward time: 2.2000110149383545\n",
      "torch_conv forward time: 0.1567244529724121\n",
      "my_conv backward time: 2.2330682277679443\n",
      "my_conv_with_loops backward time: 9.913957118988037\n",
      "torch_conv backward time: 0.3933844566345215\n",
      "\n",
      "batch_size: 8\n",
      "my_conv forward time: 1.9885218143463135\n",
      "my_conv_with_loops forward time: 2.341416597366333\n",
      "torch_conv forward time: 0.200042724609375\n",
      "my_conv backward time: 4.398773908615112\n",
      "my_conv_with_loops backward time: 20.821265697479248\n",
      "torch_conv backward time: 0.4874694347381592\n",
      "\n",
      "batch_size: 16\n",
      "my_conv forward time: 4.403775453567505\n",
      "my_conv_with_loops forward time: 3.899850606918335\n",
      "torch_conv forward time: 0.6100683212280273\n",
      "my_conv backward time: 10.690370321273804\n",
      "my_conv_with_loops backward time: 45.32488679885864\n",
      "torch_conv backward time: 0.542504072189331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d vs Conv2dWithLoops vs torch.nn.Conv2d time comparison forward and backward\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 3\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "for batch_size in [1, 2, 4, 8, 16]:\n",
    "    print(\"batch_size:\", batch_size)\n",
    "\n",
    "    input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "    torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "    my_conv_with_loops = Conv2dWithLoops(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv_with_loops.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    n_iterations = 1000\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out = my_conv.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out_with_loops = my_conv_with_loops.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out = torch_conv(input_data_torch)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g = my_conv.backward_as_matrix_multiplication(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g_with_loops = my_conv_with_loops.backward(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv backward time: {end - start}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07955d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bbf79ea567ebf64b92b9cf68f2b08a0cf8db5ff8a7f29f56d99d802810464d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
