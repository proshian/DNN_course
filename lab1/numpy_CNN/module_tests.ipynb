{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a56421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SystemPoint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from NumpyNN.NN_np import (\n",
    "    FullyConnectedLayer,\n",
    "    ReLULayer,\n",
    "    SigmoidLayer,\n",
    "    ReLULayer,\n",
    "    AdamOptimizer,\n",
    "    CrossEntropyLoss,\n",
    "    LinearActivation,\n",
    "    Sequential,\n",
    "    Optimizer,\n",
    "    SoftMaxLayer,\n",
    "    GradientDescentOptimizer,\n",
    "    CrossEntropyLossWithSoftMax,\n",
    "    Conv2d,\n",
    "    Conv2dWithLoops,\n",
    "    Flatten,\n",
    "    MaxPool2d,\n",
    "    AdamOptimizer,\n",
    "    BatchNormalization2d,\n",
    ")\n",
    "\n",
    "from numpy_resnet import Bottleneck, resnet101\n",
    "\n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12d3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(\n",
    "    sys.path[0].removesuffix(\"numpy_CNN\") + \"pytorch_implementations\"\n",
    ")\n",
    "\n",
    "from resnet import Bottleneck as Bottleneck_torch\n",
    "from resnet import resnet101 as resnet101_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e86a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "w gradients all close: True\n",
      "b gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FullyConnectedLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_output_features).astype(np.float32)\n",
    "\n",
    "torch_fc = torch.nn.Linear(n_input_features, n_output_features)\n",
    "torch_out = torch_fc(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_wg = torch_fc.weight.grad.detach().numpy().T\n",
    "torch_bg = torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_fc = FullyConnectedLayer(n_input_features, n_output_features)\n",
    "my_fc.weights = torch_fc.weight.detach().numpy().T\n",
    "my_fc.bias = torch_fc.bias.detach().numpy().reshape(-1, 1).T\n",
    "my_out = my_fc.forward(input_data)\n",
    "my_input_g = my_fc.backward(output_gradient)\n",
    "my_wg = my_fc.weights_gradient\n",
    "my_bg = my_fc.bias_gradient\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"w gradients all close:\", np.allclose(my_wg, torch_wg))\n",
    "print(\"b gradients all close:\", np.allclose(my_bg, torch_bg))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, input_data_torch.grad))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_bg))\n",
    "\n",
    "# print(\"all parameters shape same: \", my_fc.weights.shape == torch_fc.weight.T.shape and my_fc.bias.shape == torch.unsqueeze(torch_fc.bias, 1).shape)\n",
    "# print(\"output sum of square dif:\", np.square(my_out - torch_out_np).sum())\n",
    "# print(torch_wg.sum(), my_wg.sum())\n",
    "# print(\"w gradient sum of square dif:\", np.square(my_wg - torch_wg).sum())\n",
    "\n",
    "#print()\n",
    "#print(my_wg)\n",
    "#print()\n",
    "#print(torch_wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba7f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_val all close: True\n",
      "loss gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CrossEntropyLoss test\n",
    "\"\"\"\n",
    "def one_hot(y: np.ndarray, n_classes: int):\n",
    "    encoded = np.zeros((y.size, n_classes))\n",
    "    encoded[np.arange(y.size), y] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "n_classes = 3\n",
    "pred = np.random.rand(batch_size, n_classes).astype(np.float32)\n",
    "true = one_hot(np.random.randint(0, n_classes, batch_size), n_classes)\n",
    "pred_torch = torch.from_numpy(pred).float()\n",
    "true_torch = torch.from_numpy(true).float()\n",
    "pred_torch.requires_grad = True\n",
    "\n",
    "torch_loss  = torch.nn.CrossEntropyLoss()\n",
    "torch_loss_val = torch_loss(pred_torch, true_torch)\n",
    "torch_loss_val.backward()\n",
    "\n",
    "my_loss = CrossEntropyLossWithSoftMax()\n",
    "my_loss_val = my_loss.forward(pred, true)\n",
    "my_loss.backward()\n",
    "\n",
    "print(\"loss_val all close:\", np.allclose(my_loss_val, torch_loss_val.detach().numpy()))\n",
    "print(\"loss gradients all close:\", np.allclose(my_loss.backward(), pred_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99dabc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ReLULayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_relu = torch.nn.ReLU()\n",
    "torch_out = torch_relu(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_relu = ReLULayer()\n",
    "my_out = my_relu.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_relu.backward(output_gradient), input_data_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461e801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SigmoidLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "torch_out = torch_sigmoid(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_sigmoid = SigmoidLayer()\n",
    "my_out = my_sigmoid.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_sigmoid.backward(output_gradient), input_data_torch.grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1d5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SigmoidLayer test on a 4D tensor\n",
    "\"\"\"\n",
    "\n",
    "n_input_channels = 3\n",
    "n_samples = 2\n",
    "height = 5\n",
    "width = 5\n",
    "input_data = np.random.rand(n_samples, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_channels, height, width).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "torch_out = torch_sigmoid(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_sigmoid = SigmoidLayer()\n",
    "my_out = my_sigmoid.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_sigmoid.backward(output_gradient), input_data_torch.grad))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_bg))\n",
    "\n",
    "# print(\"all parameters shape same: \", my_fc.weights.shape == torch_fc.weight.T.shape and my_fc.bias.shape == torch.unsqueeze(torch_fc.bias, 1).shape)\n",
    "# print(\"output sum of square dif:\", np.square(my_out - torch_out_np).sum())\n",
    "# print(torch_wg.sum(), my_wg.sum())\n",
    "# print(\"w gradient sum of square dif:\", np.square(my_wg - torch_wg).sum())\n",
    "\n",
    "#print()\n",
    "#print(my_wg)\n",
    "#print()\n",
    "#print(torch_wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f81748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FlattenLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_channels = 3\n",
    "n_samples = 2\n",
    "height = 5\n",
    "width = 5\n",
    "\n",
    "input_data = np.random.rand(n_samples, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_channels * height * width).astype(np.float32)\n",
    "\n",
    "my_flatten = Flatten()\n",
    "my_out = my_flatten.forward(input_data)\n",
    "my_out_g = my_flatten.backward(output_gradient)\n",
    "\n",
    "torch_flatten = torch.nn.Flatten()\n",
    "torch_out = torch_flatten(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_out_g, torch_input_g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea63232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2dWithLoops test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, width, height).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, output_width, output_height).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "my_conv = Conv2dWithLoops(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "my_conv.bias = torch_conv.bias.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "torch_bg = torch_conv.bias.grad.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "\n",
    "print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba16408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 2\n",
    "n_input_channels = 2\n",
    "n_output_channels = 2\n",
    "width = 4\n",
    "height = 4\n",
    "\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    my_conv.bias = torch_conv.bias.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    torch_bg = torch_conv.bias.grad.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward(output_gradient)\n",
    "\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "if my_conv.bias is not None:\n",
    "    print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ccdb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d test 2\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 3\n",
    "n_input_channels = 1\n",
    "n_output_channels = 64\n",
    "width = 32\n",
    "height = 32\n",
    "\n",
    "kernel_size = 7\n",
    "stride = 2\n",
    "padding = 3\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False)\n",
    "\n",
    "my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    my_conv.bias = torch_conv.bias.detach().numpy().reshape(my_conv.bias.shape)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    torch_bg = torch_conv.bias.grad.detach().numpy().reshape(my_conv.bias.shape)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward(output_gradient)\n",
    "\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "if my_conv.bias is not None:\n",
    "    print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910f5e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "n_input_channels = 4\n",
    "n_output_channels = 5\n",
    "width = 32\n",
    "height = 32\n",
    "\n",
    "kernel_size = 5\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False)\n",
    "\n",
    "big_input = np.random.rand(2, 5, 10, 10).astype(np.float32)   \n",
    "\n",
    "converted_input = my_conv._convert_input(big_input)\n",
    "\n",
    "restored_input = my_conv._restore_input(converted_input, big_input.shape)\n",
    "\n",
    "np.allclose(restored_input, big_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "970ee26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MaxPool2d test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 2\n",
    "n_channels = 3\n",
    "height = 6\n",
    "width = 4\n",
    "\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_pool = torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_pool = MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_out = my_pool.forward(input_data)\n",
    "\n",
    "torch_out = torch_pool(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_pool.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47856b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MaxPool2d test 2\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "n_channels = 3\n",
    "height = 16\n",
    "width = 16\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_pool = torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_pool = MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_out = my_pool.forward(input_data)\n",
    "\n",
    "torch_out = torch_pool(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_pool.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b5717f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# BottleNeckLayer test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 5\n",
    "# in_channels = 8\n",
    "# bottleneck_depth = 2\n",
    "# width = 6\n",
    "# height = 6\n",
    "\n",
    "# expansion_factor = 4\n",
    "# n_output_channels = bottleneck_depth * expansion_factor\n",
    "\n",
    "# for stride_for_downsampling in (1, 2):  # Checking both cases: no downsampling and downsampling\n",
    "#     print(f\"stride = {stride_for_downsampling}\")\n",
    "#     input_data = np.random.rand(batch_size, in_channels, width, height).astype(np.float32)\n",
    "#     input_data_torch = torch.from_numpy(input_data).float()\n",
    "#     input_data_torch.requires_grad = True\n",
    "\n",
    "#     if stride_for_downsampling == 1:\n",
    "#         output_width = width\n",
    "#         output_height = height\n",
    "#     if stride_for_downsampling == 2:\n",
    "#         output_width = width // stride_for_downsampling\n",
    "#         output_height = height // stride_for_downsampling\n",
    "#     output_gradient = np.random.rand(batch_size, n_output_channels, output_width, output_height).astype(np.float32)\n",
    "\n",
    "#     torch_bottleneck = Bottleneck_torch(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "#     my_bottleneck = Bottleneck(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "\n",
    "#     my_bottleneck.conv1.weights = torch_bottleneck.conv1.weight.detach().numpy()\n",
    "#     my_bottleneck.conv2.weights = torch_bottleneck.conv2.weight.detach().numpy()\n",
    "#     my_bottleneck.conv3.weights = torch_bottleneck.conv3.weight.detach().numpy()\n",
    "\n",
    "#     if my_bottleneck.conv_to_match_dimensions:\n",
    "#         my_bottleneck.conv_to_match_dimensions.weights = torch_bottleneck.conv_to_match_dimensions.weight.detach().numpy()\n",
    "    \n",
    "#     my_out = my_bottleneck.forward(input_data)\n",
    "#     torch_out = torch_bottleneck(input_data_torch)\n",
    "\n",
    "#     torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "#     torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "#     my_input_g = my_bottleneck.backward(output_gradient)\n",
    "\n",
    "#     atol = 1e-6\n",
    "#     print(\"output all close:\", np.allclose(my_out, torch_out.detach().numpy(), atol=atol))\n",
    "#     print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "#     print(\"conv1 weights gradients all close:\", np.allclose(my_bottleneck.conv1.weights_gradient, torch_bottleneck.conv1.weight.grad.detach().numpy(), atol=atol))\n",
    "#     print(\"conv2 weights gradients all close:\", np.allclose(my_bottleneck.conv2.weights_gradient, torch_bottleneck.conv2.weight.grad.detach().numpy(), atol=atol))\n",
    "#     print(\"conv3 weights gradients all close:\", np.allclose(my_bottleneck.conv3.weights_gradient, torch_bottleneck.conv3.weight.grad.detach().numpy(), atol=atol))\n",
    "#     if my_bottleneck.conv_to_match_dimensions:\n",
    "#         print(\"conv_to_match_dimensions weights gradients all close:\", np.allclose(my_bottleneck.conv_to_match_dimensions.weights_gradient, torch_bottleneck.conv_to_match_dimensions.weight.grad.detach().numpy(), atol=atol))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4f96846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "resnet 101 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 10).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "my_resnet = resnet101(10, 1)\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14fb02cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv1 test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 32\n",
    "# n_channels = 1\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 64, 16, 16).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv1.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv1(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv1.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57fc2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv2_x test\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 8\n",
    "# n_channels = 64\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 256, 8, 8).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv2_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv2_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv2_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9234786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv3_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 8\n",
    "# n_channels = 256\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 512, 4, 4).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv3_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv3_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv3_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31786ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv4_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 4\n",
    "# n_channels = 512\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 1024, 2, 2).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv4_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv4_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv4_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b704ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv5_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 2\n",
    "# n_channels = 1024\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 2048, 1, 1).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv5_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv5_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv5_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affa6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# AdamOptimizer test\n",
    "# \"\"\"\n",
    "\n",
    "# from torch.optim import Adam as Adam_torch\n",
    "\n",
    "# n_input_features = 6\n",
    "# n_output_features = 3\n",
    "# batch_size = 5\n",
    "# input_data = np.random.rand(batch_size, n_input_features).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, n_output_features).astype(np.float32)\n",
    "\n",
    "# torch_fc = torch.nn.Linear(n_input_features, n_output_features)\n",
    "# torch_out = torch_fc(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_wg = torch_fc.weight.grad.detach().numpy().T\n",
    "# torch_bg = torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "# my_fc = FullyConnectedLayer(n_input_features, n_output_features)\n",
    "# my_fc.weights = torch_fc.weight.detach().numpy().T\n",
    "# my_fc.bias = torch_fc.bias.detach().numpy().reshape(-1, 1).T\n",
    "# my_out = my_fc.forward(input_data)\n",
    "# my_input_g = my_fc.backward(output_gradient)\n",
    "# my_wg = my_fc.weights_gradient\n",
    "# my_bg = my_fc.bias_gradient\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "# print(\"before adam weights gradients all close:\", np.allclose(my_wg, torch_wg, atol=atol))\n",
    "# print(\"before adam bias gradients all close:\", np.allclose(my_bg, torch_bg, atol=atol))\n",
    "# print(my_wg, \"\\n\", torch_wg)\n",
    "\n",
    "# my_adam = AdamOptimizer(my_fc.get_trainable_layers(), 0.001, 0.9, 0.999, 1e-8)\n",
    "# my_adam.step()\n",
    "\n",
    "# torch_adam = Adam_torch(torch_fc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n",
    "# torch_adam.step()\n",
    "\n",
    "# print(\"after adam weights gradients all close:\", np.allclose(my_fc.weights_gradient, torch_fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "# print(\"after adam bias gradients all close:\", np.allclose(my_fc.bias_gradient, torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T, atol=atol))\n",
    "# print(my_fc.weights_gradient, \"\\n\", torch_fc.weight.grad.detach().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e7675aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: False\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BatchNorm test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 4\n",
    "n_channels = 5\n",
    "height = 8\n",
    "width = 8\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "\n",
    "torch_bn = torch.nn.BatchNorm2d(n_channels)\n",
    "\n",
    "my_bn = BatchNormalization2d(n_channels)\n",
    "\n",
    "my_bn.gamma = torch_bn.weight.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "my_bn.beta = torch_bn.bias.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "\n",
    "my_bn.mean = torch_bn.running_mean.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "my_bn.var = torch_bn.running_var.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "\n",
    "my_out = my_bn.forward(input_data)\n",
    "\n",
    "torch_out = torch_bn(input_data_torch)\n",
    "\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_bn.backward(output_gradient)\n",
    "\n",
    "atol=1e-2\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "print(\"weights gradients all close:\", np.allclose(my_bn.gamma_gradient, torch_bn.weight.grad.detach().numpy().reshape(1, n_channels, 1, 1), atol=atol))\n",
    "print(\"bias gradients all close:\", np.allclose(my_bn.beta_gradient, torch_bn.bias.grad.detach().numpy().reshape(1, n_channels, 1, 1), atol=atol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "126d38d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-119.58441 , -117.38979 , -116.06575 , ..., -118.8053  ,\n",
       "          -114.74184 , -107.76805 ],\n",
       "         [-111.328545, -113.101036, -115.94552 , ..., -110.956665,\n",
       "          -113.052216, -115.82398 ],\n",
       "         [-112.05172 , -112.13679 , -111.54582 , ..., -111.00725 ,\n",
       "          -118.87325 , -117.77903 ],\n",
       "         ...,\n",
       "         [-115.48353 , -108.375725, -115.52752 , ..., -109.12556 ,\n",
       "          -117.114815, -106.96927 ],\n",
       "         [-116.99163 , -113.27758 , -109.78651 , ..., -110.11395 ,\n",
       "          -114.615234, -119.966385],\n",
       "         [-112.057434, -118.51246 , -114.00641 , ..., -112.66236 ,\n",
       "          -115.02709 , -116.5347  ]],\n",
       "\n",
       "        [[-109.99663 , -108.14228 , -107.84375 , ..., -107.301956,\n",
       "          -108.09628 , -107.55227 ],\n",
       "         [-106.89328 , -108.81617 , -107.89132 , ..., -108.356865,\n",
       "          -109.381165, -109.56656 ],\n",
       "         [-108.08066 , -108.165596, -109.70267 , ..., -109.06108 ,\n",
       "          -108.868225, -107.60681 ],\n",
       "         ...,\n",
       "         [-108.41136 , -109.93217 , -107.46672 , ..., -106.87159 ,\n",
       "          -106.98362 , -107.0953  ],\n",
       "         [-108.42547 , -106.621864, -108.24232 , ..., -108.39183 ,\n",
       "          -108.211784, -107.832726],\n",
       "         [-108.94346 , -109.78879 , -108.69442 , ..., -107.90162 ,\n",
       "          -108.34739 , -106.83414 ]],\n",
       "\n",
       "        [[-104.3318  , -103.42227 , -105.56198 , ..., -104.98711 ,\n",
       "          -104.86577 , -104.48496 ],\n",
       "         [-104.31152 , -105.15355 , -102.59158 , ..., -102.75322 ,\n",
       "          -103.548454, -105.26454 ],\n",
       "         [-104.2555  , -104.462204, -104.13656 , ..., -104.77103 ,\n",
       "          -103.60517 , -102.76168 ],\n",
       "         ...,\n",
       "         [-103.18219 , -103.142075, -103.0897  , ..., -102.60003 ,\n",
       "          -103.29358 , -103.181625],\n",
       "         [-103.93901 , -105.72066 , -104.23187 , ..., -105.43619 ,\n",
       "          -104.90255 , -103.054344],\n",
       "         [-104.50052 , -103.80786 , -105.2726  , ..., -102.46059 ,\n",
       "          -105.4371  , -102.703835]],\n",
       "\n",
       "        [[-111.2599  , -114.361755, -109.83292 , ..., -110.660095,\n",
       "          -112.93153 , -112.574   ],\n",
       "         [-111.509224, -109.66058 , -111.974174, ..., -110.49694 ,\n",
       "          -110.66821 , -114.317474],\n",
       "         [-110.80158 , -110.026596, -111.64173 , ..., -110.12132 ,\n",
       "          -114.16397 , -108.5048  ],\n",
       "         ...,\n",
       "         [-108.31813 , -109.77515 , -112.156204, ..., -111.76903 ,\n",
       "          -109.06922 , -112.95685 ],\n",
       "         [-111.68003 , -114.02699 , -111.78394 , ..., -112.48058 ,\n",
       "          -111.630325, -112.567505],\n",
       "         [-111.14337 , -110.43015 , -111.32233 , ..., -112.14439 ,\n",
       "          -110.61594 , -110.995285]],\n",
       "\n",
       "        [[-118.10912 , -118.14002 , -118.31267 , ..., -121.20184 ,\n",
       "          -124.33745 , -118.06197 ],\n",
       "         [-122.84897 , -122.22928 , -115.51329 , ..., -118.053955,\n",
       "          -119.35275 , -111.53319 ],\n",
       "         [-115.2962  , -118.242546, -119.01472 , ..., -113.04451 ,\n",
       "          -116.65787 , -117.59317 ],\n",
       "         ...,\n",
       "         [-117.7959  , -115.56719 , -112.56817 , ..., -125.36387 ,\n",
       "          -124.66698 , -125.14933 ],\n",
       "         [-116.03133 , -121.961754, -122.375824, ..., -116.12768 ,\n",
       "          -114.58794 , -119.32037 ],\n",
       "         [-121.04667 , -113.490906, -109.93756 , ..., -115.95196 ,\n",
       "          -121.06997 , -117.069725]]],\n",
       "\n",
       "\n",
       "       [[[-112.53627 , -111.211586, -112.95369 , ..., -118.32182 ,\n",
       "          -115.03545 , -108.049126],\n",
       "         [-118.78856 , -116.17121 , -114.96853 , ..., -115.55498 ,\n",
       "          -111.93158 , -110.73146 ],\n",
       "         [-113.66534 , -109.72375 , -110.34074 , ..., -113.37496 ,\n",
       "          -117.49182 , -116.42369 ],\n",
       "         ...,\n",
       "         [-109.26024 , -114.63302 , -113.173645, ..., -111.81496 ,\n",
       "          -112.9632  , -115.515656],\n",
       "         [-118.6543  , -117.329254, -115.28978 , ..., -117.583405,\n",
       "          -114.36693 , -117.50673 ],\n",
       "         [-110.68875 , -117.44874 , -112.1338  , ..., -115.57436 ,\n",
       "          -108.30206 , -109.50687 ]],\n",
       "\n",
       "        [[-107.509155, -109.23385 , -107.75389 , ..., -109.39384 ,\n",
       "          -109.16097 , -106.8332  ],\n",
       "         [-108.42603 , -107.46742 , -109.86305 , ..., -107.78502 ,\n",
       "          -106.96816 , -109.24272 ],\n",
       "         [-108.37028 , -107.34536 , -109.36798 , ..., -109.11909 ,\n",
       "          -108.83972 , -106.88301 ],\n",
       "         ...,\n",
       "         [-109.738174, -109.4238  , -108.40957 , ..., -106.7404  ,\n",
       "          -109.384735, -109.85714 ],\n",
       "         [-107.76032 , -107.91587 , -108.7127  , ..., -109.15615 ,\n",
       "          -108.113205, -109.742325],\n",
       "         [-107.94886 , -109.350685, -107.77758 , ..., -106.96351 ,\n",
       "          -107.0924  , -108.955864]],\n",
       "\n",
       "        [[-103.5553  , -104.39446 , -103.66021 , ..., -105.84751 ,\n",
       "          -104.345215, -105.801476],\n",
       "         [-104.4785  , -102.81587 , -105.28901 , ..., -103.20013 ,\n",
       "          -105.66317 , -105.22328 ],\n",
       "         [-103.14373 , -105.14789 , -102.550896, ..., -105.03869 ,\n",
       "          -103.920135, -102.89928 ],\n",
       "         ...,\n",
       "         [-103.80004 , -102.61702 , -104.372665, ..., -103.01999 ,\n",
       "          -103.09007 , -103.68109 ],\n",
       "         [-105.90425 , -103.68628 , -103.61177 , ..., -102.655785,\n",
       "          -103.78274 , -102.90809 ],\n",
       "         [-105.235886, -105.387474, -105.0449  , ..., -105.277405,\n",
       "          -104.43346 , -102.98732 ]],\n",
       "\n",
       "        [[-113.890396, -109.95744 , -112.12786 , ..., -110.529785,\n",
       "          -112.020966, -111.09553 ],\n",
       "         [-113.06032 , -112.262   , -111.68204 , ..., -112.558525,\n",
       "          -112.91276 , -113.27878 ],\n",
       "         [-109.96498 , -111.3209  , -109.33621 , ..., -112.05503 ,\n",
       "          -112.20129 , -109.241806],\n",
       "         ...,\n",
       "         [-112.61465 , -110.39603 , -111.898285, ..., -110.241516,\n",
       "          -110.29796 , -111.86719 ],\n",
       "         [-109.49392 , -110.829056, -112.900986, ..., -112.2979  ,\n",
       "          -111.5261  , -111.675964],\n",
       "         [-109.23286 , -110.1232  , -113.68086 , ..., -109.179565,\n",
       "          -112.49624 , -112.9701  ]],\n",
       "\n",
       "        [[-113.1538  , -119.880325, -111.35173 , ..., -114.57456 ,\n",
       "          -115.844124, -122.22219 ],\n",
       "         [-117.63173 , -114.20916 , -111.9565  , ..., -113.148254,\n",
       "          -116.46171 , -114.069664],\n",
       "         [-122.631096, -116.908066, -124.66786 , ..., -115.54172 ,\n",
       "          -110.2538  , -123.14891 ],\n",
       "         ...,\n",
       "         [-123.372215, -121.85401 , -110.92502 , ..., -113.74179 ,\n",
       "          -115.0226  , -113.49081 ],\n",
       "         [-123.35129 , -125.24121 , -114.82544 , ..., -112.80842 ,\n",
       "          -121.021835, -110.66855 ],\n",
       "         [-118.94841 , -113.36004 , -111.31574 , ..., -116.7082  ,\n",
       "          -122.38498 , -122.18436 ]]],\n",
       "\n",
       "\n",
       "       [[[-114.79292 , -109.1124  , -115.02293 , ..., -110.50527 ,\n",
       "          -112.040344, -109.92071 ],\n",
       "         [-114.7156  , -116.05321 , -109.772156, ..., -118.909874,\n",
       "          -111.35239 , -111.225555],\n",
       "         [-110.71965 , -113.44764 , -108.23365 , ..., -119.45127 ,\n",
       "          -112.227615, -110.51111 ],\n",
       "         ...,\n",
       "         [-115.88306 , -112.45953 , -111.14156 , ..., -109.10149 ,\n",
       "          -118.53996 , -114.51619 ],\n",
       "         [-114.36116 , -109.74763 , -116.77302 , ..., -111.49951 ,\n",
       "          -114.79577 , -115.15188 ],\n",
       "         [-117.58364 , -107.75377 , -118.34509 , ..., -118.60657 ,\n",
       "          -116.27767 , -117.35881 ]],\n",
       "\n",
       "        [[-109.86378 , -107.385666, -108.33768 , ..., -107.645226,\n",
       "          -107.838715, -108.96074 ],\n",
       "         [-109.25187 , -107.83285 , -106.8674  , ..., -108.08373 ,\n",
       "          -107.24073 , -107.52809 ],\n",
       "         [-106.83534 , -107.38964 , -107.13488 , ..., -108.052895,\n",
       "          -107.93018 , -109.05071 ],\n",
       "         ...,\n",
       "         [-109.40769 , -109.20888 , -107.20769 , ..., -106.67441 ,\n",
       "          -107.96048 , -108.22693 ],\n",
       "         [-108.73122 , -108.55966 , -108.333694, ..., -107.99311 ,\n",
       "          -108.378525, -107.98896 ],\n",
       "         [-107.95723 , -107.21909 , -108.85883 , ..., -107.41846 ,\n",
       "          -109.45692 , -108.1845  ]],\n",
       "\n",
       "        [[-104.95078 , -105.17233 , -104.11679 , ..., -105.22282 ,\n",
       "          -105.67355 , -103.419334],\n",
       "         [-102.17654 , -102.6974  , -103.821556, ..., -105.728966,\n",
       "          -105.30529 , -103.66153 ],\n",
       "         [-105.572975, -105.21889 , -105.30892 , ..., -104.133736,\n",
       "          -105.38271 , -105.23489 ],\n",
       "         ...,\n",
       "         [-104.22599 , -104.78244 , -103.57028 , ..., -104.01543 ,\n",
       "          -105.50161 , -102.45057 ],\n",
       "         [-104.857025, -104.65494 , -105.61551 , ..., -104.1491  ,\n",
       "          -103.66692 , -104.830414],\n",
       "         [-102.73819 , -103.85068 , -104.21232 , ..., -102.883125,\n",
       "          -102.605125, -102.49991 ]],\n",
       "\n",
       "        [[-114.6033  , -110.01639 , -110.02759 , ..., -112.52884 ,\n",
       "          -111.58323 , -113.4448  ],\n",
       "         [-109.50707 , -113.098305, -111.56887 , ..., -112.55042 ,\n",
       "          -113.09574 , -112.53553 ],\n",
       "         [-111.44184 , -109.87239 , -111.18806 , ..., -110.472015,\n",
       "          -113.80895 , -113.22405 ],\n",
       "         ...,\n",
       "         [-114.08588 , -110.12979 , -111.11298 , ..., -111.713646,\n",
       "          -110.8111  , -110.81925 ],\n",
       "         [-113.754486, -110.10922 , -111.751686, ..., -112.03677 ,\n",
       "          -111.49231 , -108.87397 ],\n",
       "         [-112.21892 , -110.0515  , -110.135216, ..., -111.44965 ,\n",
       "          -110.913574, -112.33825 ]],\n",
       "\n",
       "        [[-115.69313 , -116.53586 , -124.586044, ..., -114.21446 ,\n",
       "          -124.44935 , -112.56433 ],\n",
       "         [-115.61981 , -120.19827 , -116.178734, ..., -120.94016 ,\n",
       "          -115.58736 , -119.939964],\n",
       "         [-122.9747  , -115.537224, -115.663246, ..., -117.29077 ,\n",
       "          -112.605064, -119.70278 ],\n",
       "         ...,\n",
       "         [-109.37009 , -114.52507 , -118.53206 , ..., -123.50067 ,\n",
       "          -112.69271 , -124.1548  ],\n",
       "         [-122.87614 , -113.80559 , -109.9837  , ..., -121.745705,\n",
       "          -116.68046 , -118.99601 ],\n",
       "         [-118.2429  , -111.3221  , -114.07763 , ..., -123.50395 ,\n",
       "          -117.72283 , -112.60939 ]]],\n",
       "\n",
       "\n",
       "       [[[-116.97535 , -116.8449  , -115.409744, ..., -113.41177 ,\n",
       "          -118.4997  , -114.97056 ],\n",
       "         [-108.88174 , -111.7068  , -113.90317 , ..., -110.29317 ,\n",
       "          -112.253235, -117.20555 ],\n",
       "         [-113.3885  , -118.35645 , -109.16893 , ..., -108.898834,\n",
       "          -113.98729 , -115.057465],\n",
       "         ...,\n",
       "         [-109.158394, -113.43491 , -115.44542 , ..., -109.32443 ,\n",
       "          -115.52857 , -106.73861 ],\n",
       "         [-109.69431 , -117.029366, -115.320076, ..., -116.00625 ,\n",
       "          -114.777115, -110.961044],\n",
       "         [-109.34777 , -110.80589 , -111.062775, ..., -112.266716,\n",
       "          -118.97877 , -118.33271 ]],\n",
       "\n",
       "        [[-108.846115, -109.225   , -109.904   , ..., -108.075584,\n",
       "          -107.46932 , -106.65583 ],\n",
       "         [-106.62853 , -107.69854 , -109.67468 , ..., -106.91563 ,\n",
       "          -108.55256 , -109.12555 ],\n",
       "         [-108.17094 , -109.10533 , -109.60095 , ..., -109.877174,\n",
       "          -108.79578 , -108.733315],\n",
       "         ...,\n",
       "         [-108.60889 , -108.8925  , -106.673134, ..., -108.331894,\n",
       "          -109.540565, -107.3938  ],\n",
       "         [-108.4074  , -108.80975 , -109.475136, ..., -108.625275,\n",
       "          -108.03331 , -109.46046 ],\n",
       "         [-106.73358 , -109.71704 , -109.0046  , ..., -108.187836,\n",
       "          -108.50457 , -109.832596]],\n",
       "\n",
       "        [[-102.79785 , -104.130936, -104.32535 , ..., -102.13332 ,\n",
       "          -102.8702  , -105.114784],\n",
       "         [-103.66976 , -102.99511 , -105.52934 , ..., -102.608765,\n",
       "          -105.55155 , -102.482574],\n",
       "         [-103.49676 , -102.06151 , -105.68825 , ..., -105.852646,\n",
       "          -102.81838 , -105.93448 ],\n",
       "         ...,\n",
       "         [-104.148186, -105.45221 , -104.81147 , ..., -103.16629 ,\n",
       "          -103.83037 , -103.19267 ],\n",
       "         [-103.25782 , -105.126526, -103.82406 , ..., -102.98451 ,\n",
       "          -105.57708 , -104.388084],\n",
       "         [-102.98923 , -104.01201 , -105.01447 , ..., -102.64937 ,\n",
       "          -105.02796 , -104.50275 ]],\n",
       "\n",
       "        [[-113.82662 , -111.33791 , -110.646454, ..., -108.99539 ,\n",
       "          -111.327484, -109.73553 ],\n",
       "         [-108.36021 , -113.91683 , -110.37149 , ..., -108.913734,\n",
       "          -112.91334 , -111.73226 ],\n",
       "         [-109.314224, -114.005325, -112.26902 , ..., -110.69355 ,\n",
       "          -112.2274  , -113.069244],\n",
       "         ...,\n",
       "         [-111.83971 , -109.99322 , -112.47754 , ..., -113.57423 ,\n",
       "          -112.35544 , -112.3648  ],\n",
       "         [-110.683304, -113.9375  , -112.847275, ..., -113.03321 ,\n",
       "          -110.47348 , -111.58103 ],\n",
       "         [-110.27664 , -111.28772 , -115.15975 , ..., -109.708534,\n",
       "          -112.78499 , -112.6718  ]],\n",
       "\n",
       "        [[-119.04309 , -122.14772 , -123.92724 , ..., -119.62445 ,\n",
       "          -118.85714 , -117.20015 ],\n",
       "         [-120.21637 , -124.78045 , -124.02269 , ..., -119.84464 ,\n",
       "          -113.76397 , -109.872086],\n",
       "         [-111.51159 , -123.448105, -110.97728 , ..., -118.30024 ,\n",
       "          -122.02934 , -116.946144],\n",
       "         ...,\n",
       "         [-115.77078 , -119.363106, -112.39024 , ..., -109.31099 ,\n",
       "          -118.65251 , -117.51935 ],\n",
       "         [-123.49333 , -125.936844, -109.63539 , ..., -120.92493 ,\n",
       "          -123.14664 , -114.36844 ],\n",
       "         [-121.63284 , -124.11202 , -126.355644, ..., -119.68396 ,\n",
       "          -122.29595 , -113.67636 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd77f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1.4604646 , -1.6258209 , -0.46832883, ..., -0.65056187,\n",
       "           1.4689349 ,  1.5229833 ],\n",
       "         [-1.5541439 ,  1.1766549 ,  0.33189905, ...,  0.70651054,\n",
       "          -1.1455977 ,  0.7864988 ],\n",
       "         [ 0.11715477, -0.68132675,  0.65488607, ...,  1.6007843 ,\n",
       "          -0.6948455 ,  0.2621041 ],\n",
       "         ...,\n",
       "         [-1.0062634 ,  0.1385234 , -0.02918611, ...,  0.52856183,\n",
       "           1.4011326 ,  1.3812448 ],\n",
       "         [ 1.4241978 , -1.75277   ,  0.63789666, ..., -0.7553188 ,\n",
       "          -1.6918747 , -1.1367693 ],\n",
       "         [-1.2775362 , -1.7074631 ,  0.69975036, ..., -0.9538025 ,\n",
       "          -1.7584723 ,  0.07514569]],\n",
       "\n",
       "        [[-1.6987466 ,  0.0845708 ,  0.46471477, ...,  0.98437905,\n",
       "           0.1494384 ,  0.6443455 ],\n",
       "         [ 1.3062056 , -0.5410769 ,  0.3069672 , ..., -0.1535215 ,\n",
       "          -1.1490119 , -1.2601275 ],\n",
       "         [ 0.18453379,  0.02986067, -1.4887841 , ..., -0.82146645,\n",
       "          -0.6127944 ,  0.626997  ],\n",
       "         ...,\n",
       "         [-0.19450997, -1.6893635 ,  0.75615686, ...,  1.3261474 ,\n",
       "           1.2604125 ,  1.1269767 ],\n",
       "         [-0.12360431,  1.6553779 , -0.0328236 , ..., -0.18258129,\n",
       "           0.04880017,  0.45956078],\n",
       "         [-0.716265  , -1.5831454 , -0.4233159 , ...,  0.3643027 ,\n",
       "          -0.05080476,  1.3749226 ]],\n",
       "\n",
       "        [[ 0.05085447,  0.9419736 , -0.96025   , ..., -1.204882  ,\n",
       "          -0.5531716 , -0.21169202],\n",
       "         [ 0.09438793, -0.5152315 ,  1.4549183 , ...,  1.1614014 ,\n",
       "           0.37525377, -1.5383312 ],\n",
       "         [-0.10267552,  0.14777824,  0.31614292, ..., -0.84350526,\n",
       "           1.0311278 ,  1.4242826 ],\n",
       "         ...,\n",
       "         [ 1.4625758 ,  1.2232616 ,  1.5584728 , ...,  1.7368774 ,\n",
       "           1.0629498 ,  1.3141582 ],\n",
       "         [ 0.6961561 , -1.5966591 , -0.04077217, ..., -1.0328254 ,\n",
       "          -0.991989  ,  0.66001374],\n",
       "         [-0.70454407,  0.8451343 , -1.4129726 , ...,  1.7778906 ,\n",
       "          -1.3560425 ,  1.0742748 ]],\n",
       "\n",
       "        [[-0.827099  , -1.1027528 ,  1.5242553 , ..., -0.4396512 ,\n",
       "          -0.96299136,  0.8603423 ],\n",
       "         [ 0.6040904 ,  0.16275208, -0.42614743, ..., -0.43995038,\n",
       "           0.05011771, -1.0480363 ],\n",
       "         [ 0.50402117,  0.61540645, -1.1831745 , ...,  1.3142802 ,\n",
       "          -1.672089  ,  1.4225177 ],\n",
       "         ...,\n",
       "         [ 1.4827384 ,  1.0021418 ,  0.5212376 , ...,  1.1253222 ,\n",
       "           0.787584  ,  0.32613254],\n",
       "         [ 0.7753497 , -0.9888489 ,  0.15785182, ...,  0.25301123,\n",
       "          -0.7167295 , -1.055657  ],\n",
       "         [ 0.09347096,  0.7192078 , -0.49236783, ..., -0.43572363,\n",
       "          -0.2725587 ,  0.9466692 ]],\n",
       "\n",
       "        [[ 0.8805554 ,  0.3190982 , -0.12417897, ...,  0.398511  ,\n",
       "          -0.8130083 ,  1.3603258 ],\n",
       "         [-0.29551762,  1.3831749 , -0.805894  , ...,  0.4999173 ,\n",
       "          -0.3183696 ,  0.8864349 ],\n",
       "         [ 0.61260784,  0.21710101, -1.1782776 , ..., -1.3708609 ,\n",
       "           1.5995847 , -1.1448501 ],\n",
       "         ...,\n",
       "         [ 0.1040127 , -0.20609984,  1.6220928 , ..., -1.8753024 ,\n",
       "          -0.92660546, -0.17850748],\n",
       "         [ 0.2536226 , -0.6020368 ,  0.8736709 , ...,  0.1561903 ,\n",
       "           0.98120767, -1.1247835 ],\n",
       "         [ 0.03720055,  0.2189814 ,  1.6535989 , ...,  1.380079  ,\n",
       "           1.3021421 , -0.51530516]]],\n",
       "\n",
       "\n",
       "       [[[ 1.3444983 , -1.5518135 ,  1.6039112 , ..., -1.7493144 ,\n",
       "           1.0845017 ,  1.0751493 ],\n",
       "         [-0.04882329,  1.1269921 , -0.7381018 , ..., -0.63444877,\n",
       "           1.3641552 , -0.12926456],\n",
       "         [ 0.96017927,  1.1296664 ,  0.30752242, ..., -0.04021576,\n",
       "           1.0312567 , -0.04169315],\n",
       "         ...,\n",
       "         [-0.13130568,  1.4523022 ,  0.1781298 , ...,  1.1029751 ,\n",
       "          -0.33680886, -1.6047363 ],\n",
       "         [-1.4968585 ,  0.57497346, -1.4959264 , ..., -0.4049866 ,\n",
       "          -1.5458733 ,  0.61520404],\n",
       "         [-0.04522157,  1.3147509 , -0.18042353, ..., -1.7768993 ,\n",
       "           0.19781676, -0.20549768]],\n",
       "\n",
       "        [[ 0.704425  , -1.030711  ,  0.5449266 , ..., -1.1737131 ,\n",
       "          -0.93758315,  1.4248098 ],\n",
       "         [-0.16499601,  0.782468  , -1.5669552 , ...,  0.49572438,\n",
       "           1.2409608 , -1.0259144 ],\n",
       "         [-0.09336989,  0.94203573, -1.1629335 , ..., -0.83030534,\n",
       "          -0.6429142 ,  1.3563286 ],\n",
       "         ...,\n",
       "         [-1.4523914 , -1.140663  , -0.10864136, ...,  1.4782975 ,\n",
       "          -1.1191624 , -1.5577531 ],\n",
       "         [ 0.48658782,  0.3850591 , -0.49553   , ..., -0.8449691 ,\n",
       "           0.12470368, -1.5168637 ],\n",
       "         [ 0.2762068 , -1.146239  ,  0.5147559 , ...,  1.276     ,\n",
       "           1.1623524 , -0.67967296]],\n",
       "\n",
       "        [[ 0.43442672, -0.23508194,  0.35632107, ..., -1.3464612 ,\n",
       "          -0.08792605, -1.3194911 ],\n",
       "         [-0.3537965 ,  1.1260326 , -0.9122525 , ...,  0.8895111 ,\n",
       "          -1.0428604 , -0.773001  ],\n",
       "         [ 1.4142958 , -0.98480856,  1.1760355 , ..., -0.6957207 ,\n",
       "          -0.13265294,  0.9634036 ],\n",
       "         ...,\n",
       "         [ 0.06497412,  1.4633337 , -0.11634162, ...,  1.2106004 ,\n",
       "           1.005274  ,  0.1227606 ],\n",
       "         [-1.6480756 ,  0.4995337 ,  0.45002934, ...,  1.2198821 ,\n",
       "           0.00506437,  0.95103323],\n",
       "         [-1.4650146 , -1.2110276 , -0.67235047, ..., -0.8483762 ,\n",
       "          -0.19100565,  0.94479716]],\n",
       "\n",
       "        [[-0.39906847,  1.2141725 , -1.0625976 , ..., -0.85792696,\n",
       "          -1.3547884 ,  1.01068   ],\n",
       "         [ 0.1586272 , -0.28261018, -1.0831192 , ..., -1.3422697 ,\n",
       "          -0.7510202 , -0.7603232 ],\n",
       "         [ 0.86922264, -1.2291828 ,  0.8231659 , ..., -1.4349623 ,\n",
       "           1.2499167 ,  1.3581394 ],\n",
       "         ...,\n",
       "         [ 0.11446548, -0.162522  ,  0.5201113 , ...,  0.6827843 ,\n",
       "           0.18500146,  0.4739764 ],\n",
       "         [ 1.0460974 , -0.85435635, -0.14629635, ...,  1.2537678 ,\n",
       "           0.26844323, -0.20419443],\n",
       "         [ 0.93487495,  0.41462606, -1.4546967 , ...,  0.8029411 ,\n",
       "           0.38530618, -1.3316426 ]],\n",
       "\n",
       "        [[ 0.95392895, -0.40718138,  1.6459537 , ..., -0.62997556,\n",
       "           0.8307223 , -0.3426117 ],\n",
       "         [-0.1588068 ,  0.39546892, -1.2003827 , ...,  0.3139763 ,\n",
       "           0.03906658, -1.6836004 ],\n",
       "         [ 0.5052055 ,  1.0095483 ,  0.7463557 , ...,  1.0263096 ,\n",
       "           0.8059882 , -0.3529847 ],\n",
       "         ...,\n",
       "         [ 0.72268754,  1.2171997 , -0.01105087, ...,  1.5365736 ,\n",
       "           0.31824985,  0.35359284],\n",
       "         [ 0.9389866 , -1.0634391 , -0.58199537, ...,  0.19414034,\n",
       "           0.09584859,  1.6396259 ],\n",
       "         [-0.13811128, -1.0795065 , -0.67830545, ...,  1.125046  ,\n",
       "          -0.81953007, -0.8956626 ]]],\n",
       "\n",
       "\n",
       "       [[[-0.9674682 ,  0.6710503 , -0.53844464, ..., -0.8810457 ,\n",
       "          -1.6549048 ,  0.11066736],\n",
       "         [-0.70684177,  1.4291906 ,  1.594327  , ..., -0.1441214 ,\n",
       "          -1.6567892 ,  1.5528809 ],\n",
       "         [ 0.26296806,  1.0876428 ,  0.589354  , ..., -1.4396665 ,\n",
       "           1.2752783 , -0.18524568],\n",
       "         ...,\n",
       "         [ 0.8676829 , -0.35425705,  0.24022643, ...,  1.1588176 ,\n",
       "          -1.4936426 ,  1.1761897 ],\n",
       "         [ 0.37096027,  1.6236444 , -1.0034008 , ...,  1.148493  ,\n",
       "           0.5504323 , -1.6012021 ],\n",
       "         [-1.488735  ,  0.86644936, -1.6379912 , ...,  0.21645117,\n",
       "           0.47744298,  0.8061935 ]],\n",
       "\n",
       "        [[-1.5988611 ,  0.9099758 , -0.13409778, ...,  0.6054268 ,\n",
       "           0.39639306, -0.69437075],\n",
       "         [-0.9865808 ,  0.45787045,  1.4367383 , ...,  0.14748003,\n",
       "           1.0124159 ,  0.77866125],\n",
       "         [ 1.3756176 ,  0.8977823 ,  1.0811495 , ...,  0.23123387,\n",
       "           0.37772903, -0.74508846],\n",
       "         ...,\n",
       "         [-1.1813382 , -0.9709094 ,  1.0281925 , ...,  1.5661259 ,\n",
       "           0.34979463,  0.05264017],\n",
       "         [-0.4784017 , -0.26046738, -0.05119734, ...,  0.2681258 ,\n",
       "          -0.09653684,  0.28849947],\n",
       "         [ 0.2679282 ,  0.9800623 , -0.5918984 , ...,  0.8576913 ,\n",
       "          -1.1902063 ,  0.10041942]],\n",
       "\n",
       "        [[-0.35649693, -1.0617312 ,  0.5327214 , ..., -0.8649362 ,\n",
       "          -1.2886207 ,  1.1821764 ],\n",
       "         [ 1.8219197 ,  1.3045256 ,  0.05716473, ..., -1.6381811 ,\n",
       "          -1.3412067 ,  0.77724093],\n",
       "         [-1.5281398 , -0.8970769 , -0.7495753 , ...,  0.27908292,\n",
       "          -1.2179977 , -1.0131588 ],\n",
       "         ...,\n",
       "         [-0.03389515, -0.9918386 ,  1.054813  , ..., -0.3033259 ,\n",
       "          -0.9268031 ,  1.5579729 ],\n",
       "         [-1.1459451 , -0.87576735, -1.2764213 , ...,  0.21276633,\n",
       "           0.3419378 , -0.8291457 ],\n",
       "         [ 1.4619213 ,  0.6063422 , -0.49376357, ...,  1.6328765 ,\n",
       "           1.3736589 ,  1.6401042 ]],\n",
       "\n",
       "        [[-1.7681884 ,  1.3928574 ,  0.553965  , ...,  0.5677788 ,\n",
       "           0.73638856, -0.22897004],\n",
       "         [ 1.0012336 , -0.3491083 , -1.5526913 , ..., -0.07360721,\n",
       "          -0.8799015 , -1.5321912 ],\n",
       "         [ 0.17445613,  0.5910498 ,  0.5260295 , ...,  0.7607404 ,\n",
       "          -1.0684026 , -0.19052105],\n",
       "         ...,\n",
       "         [-1.3161234 ,  1.4382193 , -0.9942444 , ...,  0.14787185,\n",
       "          -0.94371504,  1.0550364 ],\n",
       "         [-1.0480471 , -0.18608531,  0.24115896, ...,  0.1784741 ,\n",
       "           1.1598457 ,  0.7815823 ],\n",
       "         [-0.31505245,  1.2250847 , -0.45098084, ...,  1.1498009 ,\n",
       "           1.0652403 ,  1.0457557 ]],\n",
       "\n",
       "        [[-1.680034  , -1.4420375 ,  0.61555   , ..., -0.8820627 ,\n",
       "          -1.41774   ,  1.5104355 ],\n",
       "         [-1.0347008 , -1.3470936 ,  0.5711382 , ...,  0.2872913 ,\n",
       "           0.46650672, -0.40321824],\n",
       "         [ 0.5773153 ,  0.45154932,  1.1434509 , ...,  1.3195553 ,\n",
       "          -1.7307082 ,  0.14754672],\n",
       "         ...,\n",
       "         [ 1.1220473 , -1.5670555 , -1.3424543 , ..., -0.7508234 ,\n",
       "          -0.93048596, -0.99461406],\n",
       "         [-0.4645335 , -1.0732784 ,  0.38921982, ..., -0.132141  ,\n",
       "          -0.02079936, -1.7049494 ],\n",
       "         [ 0.19661783, -0.8780616 , -0.00195771, ..., -0.01628379,\n",
       "          -0.11557789,  1.7375783 ]]],\n",
       "\n",
       "\n",
       "       [[[ 1.2826353 , -0.9834252 , -0.42934126, ...,  1.5774006 ,\n",
       "          -1.3034986 ,  1.3393186 ],\n",
       "         [ 0.01554206,  1.5347956 , -1.4558234 , ...,  0.4211917 ,\n",
       "          -1.5778629 ,  0.6387628 ],\n",
       "         [ 0.5489637 , -1.0801997 , -0.33403018, ...,  1.5177225 ,\n",
       "          -1.6271025 ,  1.3521334 ],\n",
       "         ...,\n",
       "         [ 1.5522114 ,  0.07142557,  0.2750452 , ...,  0.00303386,\n",
       "          -0.87756884,  1.7086253 ],\n",
       "         [-0.6889758 ,  0.3129742 , -1.4424129 , ...,  0.83290255,\n",
       "          -0.37650883,  0.52487546],\n",
       "         [-0.55024165,  1.5179888 , -0.7695734 , ...,  0.8996371 ,\n",
       "          -0.70712435,  0.12832797]],\n",
       "\n",
       "        [[-0.64234066, -0.9725341 , -1.6976948 , ...,  0.22315046,\n",
       "           0.735523  ,  1.5407162 ],\n",
       "         [ 1.6363256 ,  0.56210595, -1.4695736 , ...,  1.3677953 ,\n",
       "          -0.35426864, -0.8793583 ],\n",
       "         [ 0.12159702, -0.7984085 , -1.3803256 , ..., -1.6687413 ,\n",
       "          -0.50074595, -0.46649343],\n",
       "         ...,\n",
       "         [-0.37376374, -0.6149255 ,  1.5987307 , ..., -0.104562  ,\n",
       "          -1.308325  ,  0.90143114],\n",
       "         [-0.1203243 , -0.5375985 , -1.2023548 , ..., -0.35040924,\n",
       "           0.25423908, -1.2372663 ],\n",
       "         [ 1.5520997 , -1.424585  , -0.73328346, ...,  0.08840818,\n",
       "          -0.29680097, -1.5859967 ]],\n",
       "\n",
       "        [[ 1.1187147 ,  0.08675339,  0.04913217, ...,  1.7568389 ,\n",
       "           0.8974435 , -0.7386145 ],\n",
       "         [ 0.3512493 ,  0.7688476 , -1.237216  , ...,  1.7945732 ,\n",
       "          -1.3442705 ,  1.7971275 ],\n",
       "         [ 1.0849422 ,  1.648279  , -1.3329432 , ..., -1.4385583 ,\n",
       "           1.5214044 , -1.6209028 ],\n",
       "         ...,\n",
       "         [-0.03742633, -1.3152442 , -0.78545403, ...,  1.0955638 ,\n",
       "           0.5536616 ,  1.4331691 ],\n",
       "         [ 0.96801907, -1.2750763 ,  0.24399371, ...,  1.511545  ,\n",
       "          -1.3448133 , -0.4948575 ],\n",
       "         [ 0.8907411 , -0.0321342 , -0.43057197, ...,  1.2010324 ,\n",
       "          -0.75995743, -0.2989168 ]],\n",
       "\n",
       "        [[-0.83059996, -1.6760392 , -0.7502553 , ...,  1.0507342 ,\n",
       "          -0.5593588 ,  1.4748415 ],\n",
       "         [ 1.3240285 , -1.071794  , -0.5038777 , ...,  0.84649944,\n",
       "           0.6687143 , -1.5682418 ],\n",
       "         [ 0.91644883, -0.4132921 , -1.6637573 , ...,  0.426005  ,\n",
       "           0.55257773, -0.5770134 ],\n",
       "         ...,\n",
       "         [-0.8837425 ,  0.01530822, -1.4840664 , ..., -0.5633454 ,\n",
       "           1.0146307 , -1.600427  ],\n",
       "         [-0.56702965, -0.8778851 ,  0.11597715, ..., -1.7495362 ,\n",
       "           1.1937894 ,  0.6872725 ],\n",
       "         [-0.11431924,  0.4951746 , -1.6057799 , ...,  1.4656962 ,\n",
       "           0.2427315 ,  0.21482678]],\n",
       "\n",
       "        [[-0.02022143, -0.01812903,  0.18645772, ...,  0.47030985,\n",
       "          -1.5403057 ,  1.0090578 ],\n",
       "         [-1.590679  ,  0.40463415, -0.7020784 , ...,  0.20049466,\n",
       "           0.72669756,  0.5250138 ],\n",
       "         [ 0.43253693,  1.3315741 ,  0.6461073 , ..., -0.7269277 ,\n",
       "           1.532079  , -1.2904066 ],\n",
       "         ...,\n",
       "         [ 1.5917509 ,  0.1361503 , -1.1897244 , ...,  1.730201  ,\n",
       "           0.5746915 , -0.24499415],\n",
       "         [-0.96250415, -0.90853053,  1.3380853 , ..., -1.266931  ,\n",
       "          -0.6556299 ,  0.73308194],\n",
       "         [ 0.71809494,  0.13142626, -1.6333385 , ...,  0.79740655,\n",
       "           1.3296527 , -0.10054268]]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca9fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "my_conv forward time: 0.33388376235961914\n",
      "my_conv_with_loops forward time: 2.353210210800171\n",
      "torch_conv forward time: 0.08339810371398926\n",
      "my_conv fully matrix multiplication: 0.28624987602233887\n",
      "my_conv semi_matrix_backward time: 1.4046761989593506\n",
      "my_conv_with_loops backward time: 2.42439603805542\n",
      "torch_conv backward time: 0.15736699104309082\n",
      "batch_size: 2\n",
      "my_conv forward time: 0.5330967903137207\n",
      "my_conv_with_loops forward time: 2.108637809753418\n",
      "torch_conv forward time: 0.16659021377563477\n",
      "my_conv fully matrix multiplication: 0.4738328456878662\n",
      "my_conv semi_matrix_backward time: 2.6497817039489746\n",
      "my_conv_with_loops backward time: 4.740177631378174\n",
      "torch_conv backward time: 0.3679213523864746\n",
      "batch_size: 4\n",
      "my_conv forward time: 1.0229949951171875\n",
      "my_conv_with_loops forward time: 2.224562644958496\n",
      "torch_conv forward time: 0.1580660343170166\n",
      "my_conv fully matrix multiplication: 0.8497607707977295\n",
      "my_conv semi_matrix_backward time: 5.198260545730591\n",
      "my_conv_with_loops backward time: 9.5380277633667\n",
      "torch_conv backward time: 0.37662506103515625\n",
      "batch_size: 8\n",
      "my_conv forward time: 1.939718246459961\n",
      "my_conv_with_loops forward time: 2.3732821941375732\n",
      "torch_conv forward time: 0.17607378959655762\n",
      "my_conv fully matrix multiplication: 1.6494240760803223\n",
      "my_conv semi_matrix_backward time: 10.129782915115356\n",
      "my_conv_with_loops backward time: 18.280099391937256\n",
      "torch_conv backward time: 0.5119674205780029\n",
      "batch_size: 16\n",
      "my_conv forward time: 3.8003203868865967\n",
      "my_conv_with_loops forward time: 3.2648208141326904\n",
      "torch_conv forward time: 0.24799227714538574\n",
      "my_conv fully matrix multiplication: 3.595266103744507\n",
      "my_conv semi_matrix_backward time: 21.00715708732605\n",
      "my_conv_with_loops backward time: 41.56549143791199\n",
      "torch_conv backward time: 0.45488643646240234\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d vs Conv2dWithLoops vs torch.nn.Conv2d time comparison forward and backward\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 3\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "for batch_size in [1, 2, 4, 8, 16]:\n",
    "    print(\"batch_size:\", batch_size)\n",
    "\n",
    "    input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "    torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "    my_conv_with_loops = Conv2dWithLoops(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv_with_loops.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    n_iterations = 1000\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out = my_conv.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out_with_loops = my_conv_with_loops.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out = torch_conv(input_data_torch)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g = my_conv.backward_as_matrix_multiplication(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv fully matrix multiplication: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g = my_conv.semi_matrix_backward(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv semi_matrix_backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g_with_loops = my_conv_with_loops.backward(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv backward time: {end - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bbf79ea567ebf64b92b9cf68f2b08a0cf8db5ff8a7f29f56d99d802810464d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
