{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a56421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SystemPoint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from NumpyNN.NN_np import (\n",
    "    FullyConnectedLayer,\n",
    "    ReLULayer,\n",
    "    SigmoidLayer,\n",
    "    ReLULayer,\n",
    "    AdamOptimizer,\n",
    "    CrossEntropyLoss,\n",
    "    LinearActivation,\n",
    "    Sequential,\n",
    "    Optimizer,\n",
    "    SoftMaxLayer,\n",
    "    GradientDescentOptimizer,\n",
    "    CrossEntropyLossWithSoftMax,\n",
    "    Conv2d,\n",
    ")\n",
    "\n",
    "from numpy_resnet import Bottleneck\n",
    "\n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12d3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(\n",
    "    sys.path[0].removesuffix(\"numpy_CNN\") + \"pytorch_implementations\"\n",
    ")\n",
    "\n",
    "from resnet import Bottleneck as Bottleneck_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e86a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "w gradients all close: True\n",
      "b gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FullyConnectedLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_output_features).astype(np.float32)\n",
    "\n",
    "torch_fc = torch.nn.Linear(n_input_features, n_output_features)\n",
    "torch_out = torch_fc(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_wg = torch_fc.weight.grad.detach().numpy().T\n",
    "torch_bg = torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_fc = FullyConnectedLayer(n_input_features, n_output_features)\n",
    "my_fc.weights = torch_fc.weight.detach().numpy().T\n",
    "my_fc.bias = torch_fc.bias.detach().numpy().reshape(-1, 1).T\n",
    "my_out = my_fc.forward(input_data)\n",
    "my_input_g = my_fc.backward(output_gradient)\n",
    "my_wg = my_fc.weights_gradient\n",
    "my_bg = my_fc.bias_gradient\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"w gradients all close:\", np.allclose(my_wg, torch_wg))\n",
    "print(\"b gradients all close:\", np.allclose(my_bg, torch_bg))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, input_data_torch.grad))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_bg))\n",
    "\n",
    "# print(\"all parameters shape same: \", my_fc.weights.shape == torch_fc.weight.T.shape and my_fc.bias.shape == torch.unsqueeze(torch_fc.bias, 1).shape)\n",
    "# print(\"output sum of square dif:\", np.square(my_out - torch_out_np).sum())\n",
    "# print(torch_wg.sum(), my_wg.sum())\n",
    "# print(\"w gradient sum of square dif:\", np.square(my_wg - torch_wg).sum())\n",
    "\n",
    "#print()\n",
    "#print(my_wg)\n",
    "#print()\n",
    "#print(torch_wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba7f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_val all close: True\n",
      "loss gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CrossEntropyLoss test\n",
    "\"\"\"\n",
    "def one_hot(y: np.ndarray, n_classes: int):\n",
    "    encoded = np.zeros((y.size, n_classes))\n",
    "    encoded[np.arange(y.size), y] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "n_classes = 3\n",
    "pred = np.random.rand(batch_size, n_classes).astype(np.float32)\n",
    "true = one_hot(np.random.randint(0, n_classes, batch_size), n_classes)\n",
    "pred_torch = torch.from_numpy(pred).float()\n",
    "true_torch = torch.from_numpy(true).float()\n",
    "pred_torch.requires_grad = True\n",
    "\n",
    "torch_loss  = torch.nn.CrossEntropyLoss()\n",
    "torch_loss_val = torch_loss(pred_torch, true_torch)\n",
    "torch_loss_val.backward()\n",
    "\n",
    "my_loss = CrossEntropyLossWithSoftMax()\n",
    "my_loss_val = my_loss.forward(pred, true)\n",
    "my_loss.backward()\n",
    "\n",
    "print(\"loss_val all close:\", np.allclose(my_loss_val, torch_loss_val.detach().numpy()))\n",
    "print(\"loss gradients all close:\", np.allclose(my_loss.backward(), pred_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99dabc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ReLULayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_relu = torch.nn.ReLU()\n",
    "torch_out = torch_relu(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_relu = ReLULayer()\n",
    "my_out = my_relu.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_relu.backward(output_gradient), input_data_torch.grad))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_bg))\n",
    "\n",
    "# print(\"all parameters shape same: \", my_fc.weights.shape == torch_fc.weight.T.shape and my_fc.bias.shape == torch.unsqueeze(torch_fc.bias, 1).shape)\n",
    "# print(\"output sum of square dif:\", np.square(my_out - torch_out_np).sum())\n",
    "# print(torch_wg.sum(), my_wg.sum())\n",
    "# print(\"w gradient sum of square dif:\", np.square(my_wg - torch_wg).sum())\n",
    "\n",
    "#print()\n",
    "#print(my_wg)\n",
    "#print()\n",
    "#print(torch_wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1d5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SigmoidLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "torch_out = torch_sigmoid(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_sigmoid = SigmoidLayer()\n",
    "my_out = my_sigmoid.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_sigmoid.backward(output_gradient), input_data_torch.grad))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_bg))\n",
    "\n",
    "# print(\"all parameters shape same: \", my_fc.weights.shape == torch_fc.weight.T.shape and my_fc.bias.shape == torch.unsqueeze(torch_fc.bias, 1).shape)\n",
    "# print(\"output sum of square dif:\", np.square(my_out - torch_out_np).sum())\n",
    "# print(torch_wg.sum(), my_wg.sum())\n",
    "# print(\"w gradient sum of square dif:\", np.square(my_wg - torch_wg).sum())\n",
    "\n",
    "#print()\n",
    "#print(my_wg)\n",
    "#print()\n",
    "#print(torch_wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea63232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2DLayer test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 5\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, width, height).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, width, height).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "my_conv.bias = torch_conv.bias.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "torch_bg = torch_conv.bias.grad.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "\n",
    "print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5717f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BottleNeckLayer test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "n_input_channels = 4\n",
    "bottleneck_depth = 2\n",
    "width = 5\n",
    "height = 5\n",
    "# ! Better to check both cases: for stride_for_downsampling in (1, 2):\n",
    "stride_for_downsampling = 2\n",
    "expansion_factor = 4\n",
    "n_output_channels = n_input_channels * expansion_factor\n",
    "\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, width, height).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, width, height).astype(np.float32)\n",
    "\n",
    "torch_bottleneck = Bottleneck_torch(n_input_channels, bottleneck_depth, stride_for_downsampling)\n",
    "my_bottleneck = Bottleneck(n_input_channels, bottleneck_depth, stride_for_downsampling)\n",
    "\n",
    "my_bottleneck.conv1.weights = torch_bottleneck.conv1.weight.detach().numpy()\n",
    "my_bottleneck.conv2.weights = torch_bottleneck.conv2.weight.detach().numpy()\n",
    "my_bottleneck.conv3.weights = torch_bottleneck.conv3.weight.detach().numpy()\n",
    "\n",
    "my_out = my_bottleneck.forward(input_data)\n",
    "torch_out = torch_bottleneck(input_data_torch)\n",
    "\n",
    "atol = 1e-6\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out.detach().numpy(), atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ca4980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.06232188, 0.05193541, 0.00277902],\n",
       "         [0.01627721, 0.04940979, 0.        ],\n",
       "         [0.02522602, 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00176693],\n",
       "         [0.06547843, 0.        , 0.11744102],\n",
       "         [0.01468656, 0.14748987, 0.06019311]],\n",
       "\n",
       "        [[0.06378714, 0.06039981, 0.00921874],\n",
       "         [0.08654278, 0.04993292, 0.07810593],\n",
       "         [0.05751046, 0.08303892, 0.00818532]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.04368356, 0.        , 0.1065679 ],\n",
       "         [0.00139902, 0.13335638, 0.05540168]],\n",
       "\n",
       "        [[0.00707051, 0.02321516, 0.00345614],\n",
       "         [0.12365073, 0.00326096, 0.19838589],\n",
       "         [0.04233807, 0.21866306, 0.0697446 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.01657281],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.01310711, 0.00777732, 0.        ],\n",
       "         [0.        , 0.00779931, 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.06900226, 0.04755107, 0.00161616],\n",
       "         [0.01727548, 0.05641142, 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00586285],\n",
       "         [0.04912194, 0.        , 0.07629832],\n",
       "         [0.08074122, 0.09181188, 0.10924006]],\n",
       "\n",
       "        [[0.080908  , 0.05249661, 0.        ],\n",
       "         [0.08764911, 0.0783786 , 0.0302687 ],\n",
       "         [0.06589229, 0.07630257, 0.05870566]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00120443],\n",
       "         [0.0371921 , 0.        , 0.07264133],\n",
       "         [0.06463392, 0.06987252, 0.08881648]],\n",
       "\n",
       "        [[0.01060662, 0.01343986, 0.        ],\n",
       "         [0.11714766, 0.04111079, 0.12196491],\n",
       "         [0.13183444, 0.14693473, 0.16491602]],\n",
       "\n",
       "        [[0.        , 0.00225618, 0.00769037],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.01228041, 0.00585397, 0.        ],\n",
       "         [0.        , 0.01590957, 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.05696993, 0.02147202, 0.        ],\n",
       "         [0.05218261, 0.04909469, 0.        ],\n",
       "         [0.02520621, 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.0003896 ],\n",
       "         [0.        , 0.00378589, 0.06653304],\n",
       "         [0.06660734, 0.13085934, 0.05189075]],\n",
       "\n",
       "        [[0.05575438, 0.01974876, 0.        ],\n",
       "         [0.05447123, 0.08041149, 0.05131374],\n",
       "         [0.11159627, 0.02464152, 0.00528619]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00145317],\n",
       "         [0.        , 0.        , 0.05475515],\n",
       "         [0.04577031, 0.1205504 , 0.04596525]],\n",
       "\n",
       "        [[0.00942416, 0.00459565, 0.00130196],\n",
       "         [0.01232544, 0.04525214, 0.11349579],\n",
       "         [0.15642767, 0.17759301, 0.05516662]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00778091],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.01359367, 0.00031549, 0.        ],\n",
       "         [0.00674668, 0.00971814, 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00046897],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.0722389 , 0.00995654, 0.01211047],\n",
       "         [0.09798153, 0.        , 0.00205436],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.00253704, 0.00334443],\n",
       "         [0.        , 0.08411192, 0.00092918],\n",
       "         [0.11410225, 0.04393792, 0.14043138]],\n",
       "\n",
       "        [[0.07332925, 0.01273212, 0.02614367],\n",
       "         [0.10061241, 0.09398853, 0.00285742],\n",
       "         [0.09318298, 0.05217556, 0.07303417]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.07144662, 0.        ],\n",
       "         [0.09764593, 0.03296227, 0.12204692]],\n",
       "\n",
       "        [[0.01265246, 0.00762548, 0.01824846],\n",
       "         [0.01754713, 0.16332017, 0.00328161],\n",
       "         [0.19811958, 0.0824459 , 0.21006106]],\n",
       "\n",
       "        [[0.        , 0.01758534, 0.00775904],\n",
       "         [0.        , 0.        , 0.00719809],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.02419231, 0.        , 0.        ],\n",
       "         [0.03111765, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.0728239 , 0.11715282, 0.        ],\n",
       "         [0.04401208, 0.04572214, 0.        ],\n",
       "         [0.        , 0.00969641, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00148107],\n",
       "         [0.04649786, 0.01277966, 0.0893361 ],\n",
       "         [0.06615442, 0.10031954, 0.14830931]],\n",
       "\n",
       "        [[0.07757105, 0.12490137, 0.        ],\n",
       "         [0.12094883, 0.0889726 , 0.02112814],\n",
       "         [0.06823707, 0.12840126, 0.06565744]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00321762],\n",
       "         [0.02834177, 0.00042285, 0.08410005],\n",
       "         [0.05480311, 0.07899483, 0.13481343]],\n",
       "\n",
       "        [[0.00428762, 0.02165367, 0.00121641],\n",
       "         [0.1332172 , 0.07141896, 0.1280277 ],\n",
       "         [0.11899353, 0.18736983, 0.22463938]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00431476],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.01457406, 0.03528171, 0.        ],\n",
       "         [0.00755475, 0.00802785, 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cfadfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.00000000e+00, 1.90505728e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 3.80296558e-02, 3.70121717e-01],\n",
       "         [1.67176425e-02, 7.41421282e-02, 0.00000000e+00]],\n",
       "\n",
       "        [[4.37806815e-01, 2.87687868e-01, 1.26285627e-01],\n",
       "         [4.70723450e-01, 3.69594574e-01, 0.00000000e+00],\n",
       "         [1.96808189e-01, 3.99078727e-01, 3.79391253e-01]],\n",
       "\n",
       "        [[4.40131202e-02, 1.29432663e-01, 0.00000000e+00],\n",
       "         [2.24688083e-01, 0.00000000e+00, 3.00731480e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[4.89399046e-01, 5.21139085e-01, 1.80581450e-01],\n",
       "         [5.70149362e-01, 4.57961649e-01, 2.62002051e-01],\n",
       "         [2.70077497e-01, 5.59989095e-01, 4.32028204e-01]],\n",
       "\n",
       "        [[6.86284781e-01, 7.94498920e-01, 4.86240476e-01],\n",
       "         [8.75468016e-01, 5.71602106e-01, 8.61682236e-01],\n",
       "         [5.97804904e-01, 8.03985357e-01, 5.34447491e-01]],\n",
       "\n",
       "        [[2.39965301e-02, 8.94389227e-02, 6.63924590e-02],\n",
       "         [0.00000000e+00, 1.66198537e-01, 0.00000000e+00],\n",
       "         [1.43853381e-01, 1.10537946e-01, 1.37102202e-01]],\n",
       "\n",
       "        [[0.00000000e+00, 8.79357755e-02, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.26783982e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[3.33441556e-01, 1.52778178e-01, 0.00000000e+00],\n",
       "         [3.63458991e-01, 2.04537958e-01, 2.96288133e-01],\n",
       "         [4.00360860e-03, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[2.24292278e-04, 1.51988029e-01, 4.28246379e-01],\n",
       "         [0.00000000e+00, 6.18058071e-02, 0.00000000e+00],\n",
       "         [3.37060064e-01, 4.81157422e-01, 4.50808287e-01]],\n",
       "\n",
       "        [[0.00000000e+00, 1.78198114e-01, 0.00000000e+00],\n",
       "         [2.52687365e-01, 1.45829827e-01, 2.36884654e-01],\n",
       "         [1.08019210e-01, 1.03054054e-01, 2.39791811e-01]],\n",
       "\n",
       "        [[2.63817519e-01, 4.11223382e-01, 4.56899375e-01],\n",
       "         [1.51271835e-01, 2.19607428e-01, 1.28706038e-01],\n",
       "         [4.87204254e-01, 4.62806165e-01, 5.88300586e-01]],\n",
       "\n",
       "        [[7.40402877e-01, 9.85848069e-01, 4.97712910e-01],\n",
       "         [6.35185421e-01, 4.20043111e-01, 6.28379166e-01],\n",
       "         [8.66017163e-01, 7.55332887e-01, 1.10614657e+00]],\n",
       "\n",
       "        [[3.24269712e-01, 0.00000000e+00, 1.49404123e-01],\n",
       "         [0.00000000e+00, 1.17603652e-02, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [1.25160962e-01, 1.09149002e-01, 9.52434018e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[2.36717910e-01, 4.06380743e-02, 2.02176690e-01],\n",
       "         [6.28270805e-02, 3.38646054e-01, 2.18329392e-02],\n",
       "         [2.74820089e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[1.66765675e-01, 2.35835969e-01, 3.59819159e-02],\n",
       "         [4.28476155e-01, 0.00000000e+00, 2.10651100e-01],\n",
       "         [0.00000000e+00, 1.80533320e-01, 5.26911914e-01]],\n",
       "\n",
       "        [[1.82639554e-01, 9.52358693e-02, 1.04883276e-02],\n",
       "         [8.26564878e-02, 0.00000000e+00, 2.88712651e-01],\n",
       "         [5.27638733e-01, 1.55403271e-01, 0.00000000e+00]],\n",
       "\n",
       "        [[4.18979019e-01, 3.67892414e-01, 2.42378995e-01],\n",
       "         [5.93216658e-01, 1.58675924e-01, 3.41979563e-01],\n",
       "         [2.78535545e-01, 3.94600272e-01, 5.84193289e-01]],\n",
       "\n",
       "        [[7.12855756e-01, 5.78711689e-01, 4.47456747e-01],\n",
       "         [8.32502902e-01, 2.26037055e-01, 5.54657996e-01],\n",
       "         [9.04182553e-01, 9.64190006e-01, 6.67403460e-01]],\n",
       "\n",
       "        [[4.28235829e-02, 0.00000000e+00, 1.65966943e-01],\n",
       "         [7.86651745e-02, 3.34049731e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.67760789e-01]],\n",
       "\n",
       "        [[1.38646528e-01, 1.11111589e-02, 4.18622568e-02],\n",
       "         [1.98436156e-03, 8.28185752e-02, 1.43231243e-01],\n",
       "         [2.18061388e-01, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[0.00000000e+00, 0.00000000e+00, 5.14267422e-02],\n",
       "         [0.00000000e+00, 4.05754596e-01, 0.00000000e+00],\n",
       "         [1.77216992e-01, 8.95055532e-02, 0.00000000e+00]],\n",
       "\n",
       "        [[2.25867197e-01, 2.94421345e-01, 7.43619725e-02],\n",
       "         [3.73957217e-01, 0.00000000e+00, 7.08225220e-02],\n",
       "         [0.00000000e+00, 3.39785703e-02, 4.09234196e-01]],\n",
       "\n",
       "        [[2.28288561e-01, 9.70675871e-02, 3.50327343e-02],\n",
       "         [2.90630281e-01, 2.92868316e-01, 5.68204857e-02],\n",
       "         [2.71171480e-01, 3.52048352e-02, 1.58882529e-01]],\n",
       "\n",
       "        [[3.00729603e-01, 3.74097288e-01, 2.30637535e-01],\n",
       "         [4.49121118e-01, 3.45377892e-01, 1.27617836e-01],\n",
       "         [2.13449746e-01, 1.71817631e-01, 6.10546887e-01]],\n",
       "\n",
       "        [[5.67039728e-01, 7.94105172e-01, 7.29180932e-01],\n",
       "         [8.49104881e-01, 8.43992233e-01, 3.06506217e-01],\n",
       "         [7.31754124e-01, 5.80252886e-01, 1.09966874e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 1.63314976e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 5.93699515e-03, 0.00000000e+00]],\n",
       "\n",
       "        [[2.15772297e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 1.95980310e-01, 0.00000000e+00],\n",
       "         [1.71712078e-02, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[4.11555544e-02, 2.76221961e-01, 2.29187548e-01],\n",
       "         [3.60829026e-01, 3.82683307e-01, 2.22219154e-01],\n",
       "         [7.19644651e-02, 1.18972637e-01, 1.42874092e-01]],\n",
       "\n",
       "        [[4.05047268e-01, 1.10528752e-01, 1.22342907e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 6.91743344e-02],\n",
       "         [1.19556651e-01, 2.84421265e-01, 1.79362163e-01]],\n",
       "\n",
       "        [[0.00000000e+00, 2.91223615e-01, 9.98602211e-02],\n",
       "         [3.69284034e-01, 2.01017439e-01, 1.91185787e-01],\n",
       "         [1.37764066e-01, 5.83009422e-02, 3.08341414e-01]],\n",
       "\n",
       "        [[4.64723021e-01, 3.99196893e-01, 3.80488306e-01],\n",
       "         [1.76480070e-01, 1.41854495e-01, 4.04879868e-01],\n",
       "         [2.11167336e-01, 4.16856647e-01, 4.69747126e-01]],\n",
       "\n",
       "        [[5.86701930e-01, 1.01103246e+00, 5.52320004e-01],\n",
       "         [6.73525691e-01, 6.38061404e-01, 9.27264512e-01],\n",
       "         [3.67029190e-01, 6.91554070e-01, 9.68346238e-01]],\n",
       "\n",
       "        [[2.67525464e-01, 0.00000000e+00, 1.15879357e-01],\n",
       "         [0.00000000e+00, 2.14719027e-02, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.72425413e-02, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 2.80567016e-02, 1.68270886e-01],\n",
       "         [1.95231959e-01, 8.23085457e-02, 3.57892886e-02],\n",
       "         [5.84627688e-02, 0.00000000e+00, 8.42403844e-02]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_out.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b915fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bbf79ea567ebf64b92b9cf68f2b08a0cf8db5ff8a7f29f56d99d802810464d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
