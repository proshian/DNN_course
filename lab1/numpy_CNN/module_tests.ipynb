{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a56421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SystemPoint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from NumpyNN.NN_np import (\n",
    "    FullyConnectedLayer,\n",
    "    ReLULayer,\n",
    "    SigmoidLayer,\n",
    "    ReLULayer,\n",
    "    AdamOptimizer,\n",
    "    CrossEntropyLoss,\n",
    "    LinearActivation,\n",
    "    Sequential,\n",
    "    Optimizer,\n",
    "    SoftMaxLayer,\n",
    "    GradientDescentOptimizer,\n",
    "    CrossEntropyLossWithSoftMax,\n",
    "    Conv2d,\n",
    "    Conv2dWithLoops,\n",
    "    Flatten,\n",
    "    MaxPool2d,\n",
    "    AdamOptimizer,\n",
    "    BatchNormalization2d,\n",
    ")\n",
    "\n",
    "from numpy_resnet import Bottleneck, resnet101\n",
    "\n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12d3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(\n",
    "    sys.path[0].removesuffix(\"numpy_CNN\") + \"pytorch_implementations\"\n",
    ")\n",
    "\n",
    "from resnet import Bottleneck as Bottleneck_torch\n",
    "from resnet import resnet101 as resnet101_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e86a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "w gradients all close: True\n",
      "b gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FullyConnectedLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_output_features).astype(np.float32)\n",
    "\n",
    "torch_fc = torch.nn.Linear(n_input_features, n_output_features)\n",
    "torch_out = torch_fc(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_wg = torch_fc.weight.grad.detach().numpy().T\n",
    "torch_bg = torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_fc = FullyConnectedLayer(n_input_features, n_output_features)\n",
    "my_fc.weights = torch_fc.weight.detach().numpy().T\n",
    "my_fc.bias = torch_fc.bias.detach().numpy().reshape(-1, 1).T\n",
    "my_out = my_fc.forward(input_data)\n",
    "my_input_g = my_fc.backward(output_gradient)\n",
    "my_wg = my_fc.weights_gradient\n",
    "my_bg = my_fc.bias_gradient\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"w gradients all close:\", np.allclose(my_wg, torch_wg))\n",
    "print(\"b gradients all close:\", np.allclose(my_bg, torch_bg))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, input_data_torch.grad))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_bg))\n",
    "\n",
    "# print(\"all parameters shape same: \", my_fc.weights.shape == torch_fc.weight.T.shape and my_fc.bias.shape == torch.unsqueeze(torch_fc.bias, 1).shape)\n",
    "# print(\"output sum of square dif:\", np.square(my_out - torch_out_np).sum())\n",
    "# print(torch_wg.sum(), my_wg.sum())\n",
    "# print(\"w gradient sum of square dif:\", np.square(my_wg - torch_wg).sum())\n",
    "\n",
    "#print()\n",
    "#print(my_wg)\n",
    "#print()\n",
    "#print(torch_wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba7f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_val all close: True\n",
      "loss gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CrossEntropyLoss test\n",
    "\"\"\"\n",
    "def one_hot(y: np.ndarray, n_classes: int):\n",
    "    encoded = np.zeros((y.size, n_classes))\n",
    "    encoded[np.arange(y.size), y] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "n_classes = 3\n",
    "pred = np.random.rand(batch_size, n_classes).astype(np.float32)\n",
    "true = one_hot(np.random.randint(0, n_classes, batch_size), n_classes)\n",
    "pred_torch = torch.from_numpy(pred).float()\n",
    "true_torch = torch.from_numpy(true).float()\n",
    "pred_torch.requires_grad = True\n",
    "\n",
    "torch_loss  = torch.nn.CrossEntropyLoss()\n",
    "torch_loss_val = torch_loss(pred_torch, true_torch)\n",
    "torch_loss_val.backward()\n",
    "\n",
    "my_loss = CrossEntropyLossWithSoftMax()\n",
    "my_loss_val = my_loss.forward(pred, true)\n",
    "my_loss.backward()\n",
    "\n",
    "print(\"loss_val all close:\", np.allclose(my_loss_val, torch_loss_val.detach().numpy()))\n",
    "print(\"loss gradients all close:\", np.allclose(my_loss.backward(), pred_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99dabc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ReLULayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_relu = torch.nn.ReLU()\n",
    "torch_out = torch_relu(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_relu = ReLULayer()\n",
    "my_out = my_relu.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_relu.backward(output_gradient), input_data_torch.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461e801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SigmoidLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_features = 6\n",
    "n_output_features = 3\n",
    "n_samples = 5\n",
    "input_data = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_features).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "torch_out = torch_sigmoid(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_sigmoid = SigmoidLayer()\n",
    "my_out = my_sigmoid.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_sigmoid.backward(output_gradient), input_data_torch.grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1d5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SigmoidLayer test on a 4D tensor\n",
    "\"\"\"\n",
    "\n",
    "n_input_channels = 3\n",
    "n_samples = 2\n",
    "height = 5\n",
    "width = 5\n",
    "input_data = np.random.rand(n_samples, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_channels, height, width).astype(np.float32)\n",
    "\n",
    "\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "torch_out = torch_sigmoid(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "my_sigmoid = SigmoidLayer()\n",
    "my_out = my_sigmoid.forward(input_data)\n",
    "\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_sigmoid.backward(output_gradient), input_data_torch.grad))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_bg))\n",
    "\n",
    "# print(\"all parameters shape same: \", my_fc.weights.shape == torch_fc.weight.T.shape and my_fc.bias.shape == torch.unsqueeze(torch_fc.bias, 1).shape)\n",
    "# print(\"output sum of square dif:\", np.square(my_out - torch_out_np).sum())\n",
    "# print(torch_wg.sum(), my_wg.sum())\n",
    "# print(\"w gradient sum of square dif:\", np.square(my_wg - torch_wg).sum())\n",
    "\n",
    "#print()\n",
    "#print(my_wg)\n",
    "#print()\n",
    "#print(torch_wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f81748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FlattenLayer test\n",
    "\"\"\"\n",
    "\n",
    "n_input_channels = 3\n",
    "n_samples = 2\n",
    "height = 5\n",
    "width = 5\n",
    "\n",
    "input_data = np.random.rand(n_samples, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(n_samples, n_input_channels * height * width).astype(np.float32)\n",
    "\n",
    "my_flatten = Flatten()\n",
    "my_out = my_flatten.forward(input_data)\n",
    "my_out_g = my_flatten.backward(output_gradient)\n",
    "\n",
    "torch_flatten = torch.nn.Flatten()\n",
    "torch_out = torch_flatten(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np))\n",
    "print(\"input gradients all close:\", np.allclose(my_out_g, torch_input_g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea63232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2dWithLoops test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 5\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, width, height).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, output_width, output_height).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "my_conv = Conv2dWithLoops(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "my_conv.bias = torch_conv.bias.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "torch_bg = torch_conv.bias.grad.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "\n",
    "print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba16408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 2\n",
    "n_input_channels = 2\n",
    "n_output_channels = 2\n",
    "width = 4\n",
    "height = 4\n",
    "\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    my_conv.bias = torch_conv.bias.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    torch_bg = torch_conv.bias.grad.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward(output_gradient)\n",
    "\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "if my_conv.bias is not None:\n",
    "    print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ccdb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "weights gradients all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d test 2\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 3\n",
    "n_input_channels = 1\n",
    "n_output_channels = 64\n",
    "width = 32\n",
    "height = 32\n",
    "\n",
    "kernel_size = 7\n",
    "stride = 2\n",
    "padding = 3\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False)\n",
    "\n",
    "my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False)\n",
    "my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    my_conv.bias = torch_conv.bias.detach().numpy().reshape(my_conv.bias.shape)\n",
    "\n",
    "my_out = my_conv.forward(input_data)\n",
    "\n",
    "torch_out = torch_conv(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "torch_wg = torch_conv.weight.grad.detach().numpy()\n",
    "if my_conv.bias is not None:\n",
    "    torch_bg = torch_conv.bias.grad.detach().numpy().reshape(my_conv.bias.shape)\n",
    "\n",
    "# print(torch_conv.weight.shape, torch_conv.bias.shape)\n",
    "# print(my_conv.weights.shape, my_conv.bias.shape)\n",
    "my_input_g = my_conv.backward(output_gradient)\n",
    "\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "\n",
    "print(\"weights gradients all close:\", np.allclose(my_conv.weights_gradient, torch_wg, atol=atol ))\n",
    "if my_conv.bias is not None:\n",
    "    print(\"bias gradients all close:\", np.allclose(my_conv.bias_gradient, torch_bg, atol=atol))\n",
    "\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910f5e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "n_input_channels = 4\n",
    "n_output_channels = 5\n",
    "width = 32\n",
    "height = 32\n",
    "\n",
    "kernel_size = 5\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False)\n",
    "\n",
    "big_input = np.random.rand(2, 5, 10, 10).astype(np.float32)   \n",
    "\n",
    "converted_input = my_conv._convert_input(big_input)\n",
    "\n",
    "restored_input = my_conv._restore_input(converted_input, big_input.shape)\n",
    "\n",
    "np.allclose(restored_input, big_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "970ee26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MaxPool2d test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 2\n",
    "n_channels = 3\n",
    "height = 6\n",
    "width = 4\n",
    "\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_pool = torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_pool = MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_out = my_pool.forward(input_data)\n",
    "\n",
    "torch_out = torch_pool(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_pool.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47856b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MaxPool2d test 2\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "n_channels = 3\n",
    "height = 16\n",
    "width = 16\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "torch_pool = torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_pool = MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "my_out = my_pool.forward(input_data)\n",
    "\n",
    "torch_out = torch_pool(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_pool.backward(output_gradient)\n",
    "\n",
    "atol=1e-6\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5717f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride = 1\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "conv1 weights gradients all close: True\n",
      "conv2 weights gradients all close: True\n",
      "conv3 weights gradients all close: True\n",
      "\n",
      "stride = 2\n",
      "output all close: True\n",
      "input gradients all close: True\n",
      "conv1 weights gradients all close: True\n",
      "conv2 weights gradients all close: True\n",
      "conv3 weights gradients all close: True\n",
      "conv_to_match_dimensions weights gradients all close: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# BottleNeckLayer test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 5\n",
    "# in_channels = 8\n",
    "# bottleneck_depth = 2\n",
    "# width = 6\n",
    "# height = 6\n",
    "\n",
    "# expansion_factor = 4\n",
    "# n_output_channels = bottleneck_depth * expansion_factor\n",
    "\n",
    "# for stride_for_downsampling in (1, 2):  # Checking both cases: no downsampling and downsampling\n",
    "#     print(f\"stride = {stride_for_downsampling}\")\n",
    "#     input_data = np.random.rand(batch_size, in_channels, width, height).astype(np.float32)\n",
    "#     input_data_torch = torch.from_numpy(input_data).float()\n",
    "#     input_data_torch.requires_grad = True\n",
    "\n",
    "#     if stride_for_downsampling == 1:\n",
    "#         output_width = width\n",
    "#         output_height = height\n",
    "#     if stride_for_downsampling == 2:\n",
    "#         output_width = width // stride_for_downsampling\n",
    "#         output_height = height // stride_for_downsampling\n",
    "#     output_gradient = np.random.rand(batch_size, n_output_channels, output_width, output_height).astype(np.float32)\n",
    "\n",
    "#     torch_bottleneck = Bottleneck_torch(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "#     my_bottleneck = Bottleneck(in_channels, bottleneck_depth, stride_for_downsampling)\n",
    "\n",
    "#     my_bottleneck.conv1.weights = torch_bottleneck.conv1.weight.detach().numpy()\n",
    "#     my_bottleneck.conv2.weights = torch_bottleneck.conv2.weight.detach().numpy()\n",
    "#     my_bottleneck.conv3.weights = torch_bottleneck.conv3.weight.detach().numpy()\n",
    "\n",
    "#     if my_bottleneck.conv_to_match_dimensions:\n",
    "#         my_bottleneck.conv_to_match_dimensions.weights = torch_bottleneck.conv_to_match_dimensions.weight.detach().numpy()\n",
    "    \n",
    "#     my_out = my_bottleneck.forward(input_data)\n",
    "#     torch_out = torch_bottleneck(input_data_torch)\n",
    "\n",
    "#     torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "#     torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "#     my_input_g = my_bottleneck.backward(output_gradient)\n",
    "\n",
    "#     atol = 1e-6\n",
    "#     print(\"output all close:\", np.allclose(my_out, torch_out.detach().numpy(), atol=atol))\n",
    "#     print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "#     print(\"conv1 weights gradients all close:\", np.allclose(my_bottleneck.conv1.weights_gradient, torch_bottleneck.conv1.weight.grad.detach().numpy(), atol=atol))\n",
    "#     print(\"conv2 weights gradients all close:\", np.allclose(my_bottleneck.conv2.weights_gradient, torch_bottleneck.conv2.weight.grad.detach().numpy(), atol=atol))\n",
    "#     print(\"conv3 weights gradients all close:\", np.allclose(my_bottleneck.conv3.weights_gradient, torch_bottleneck.conv3.weight.grad.detach().numpy(), atol=atol))\n",
    "#     if my_bottleneck.conv_to_match_dimensions:\n",
    "#         print(\"conv_to_match_dimensions weights gradients all close:\", np.allclose(my_bottleneck.conv_to_match_dimensions.weights_gradient, torch_bottleneck.conv_to_match_dimensions.weight.grad.detach().numpy(), atol=atol))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4f96846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "resnet 101 test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "height = width = 32\n",
    "n_channels = 1\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, 10).astype(np.float32)\n",
    "\n",
    "torch_resnet = resnet101_torch(10, 1)\n",
    "my_resnet = resnet101(10, 1)\n",
    "\n",
    "my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "my_out = my_resnet.forward(input_data)\n",
    "\n",
    "torch_out = torch_resnet(input_data_torch)\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_resnet.backward(output_gradient)\n",
    "\n",
    "atol=1e-3\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14fb02cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv1 test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 32\n",
    "# n_channels = 1\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 64, 16, 16).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv1.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv1(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv1.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fc2d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# conv2_x test\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 8\n",
    "# n_channels = 64\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 256, 8, 8).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv2_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv2_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv2_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9234786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# conv3_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 8\n",
    "# n_channels = 256\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 512, 4, 4).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv3_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv3_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv3_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31786ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# conv4_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 4\n",
    "# n_channels = 512\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 1024, 2, 2).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv4_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv4_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv4_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# conv5_x test\n",
    "# \"\"\"\n",
    "\n",
    "# batch_size = 10\n",
    "# height = width = 2\n",
    "# n_channels = 1024\n",
    "\n",
    "# input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, 2048, 1, 1).astype(np.float32)\n",
    "\n",
    "# torch_resnet = resnet101_torch(10, 1)\n",
    "# my_resnet = resnet101(10, 1)\n",
    "\n",
    "# my_resnet.clone_weights_from_torch(torch_resnet)\n",
    "\n",
    "# my_out = my_resnet.conv5_x.forward(input_data)\n",
    "\n",
    "# torch_out = torch_resnet.conv5_x(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "# my_input_g = my_resnet.conv5_x.backward(output_gradient)\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "affa6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# AdamOptimizer test\n",
    "# \"\"\"\n",
    "\n",
    "# from torch.optim import Adam as Adam_torch\n",
    "\n",
    "# n_input_features = 6\n",
    "# n_output_features = 3\n",
    "# batch_size = 5\n",
    "# input_data = np.random.rand(batch_size, n_input_features).astype(np.float32)\n",
    "# input_data_torch = torch.from_numpy(input_data).float()\n",
    "# input_data_torch.requires_grad = True\n",
    "# output_gradient = np.random.rand(batch_size, n_output_features).astype(np.float32)\n",
    "\n",
    "# torch_fc = torch.nn.Linear(n_input_features, n_output_features)\n",
    "# torch_out = torch_fc(input_data_torch)\n",
    "# torch_out_np = torch_out.detach().numpy()\n",
    "# torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "# torch_wg = torch_fc.weight.grad.detach().numpy().T\n",
    "# torch_bg = torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T\n",
    "# torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "\n",
    "# my_fc = FullyConnectedLayer(n_input_features, n_output_features)\n",
    "# my_fc.weights = torch_fc.weight.detach().numpy().T\n",
    "# my_fc.bias = torch_fc.bias.detach().numpy().reshape(-1, 1).T\n",
    "# my_out = my_fc.forward(input_data)\n",
    "# my_input_g = my_fc.backward(output_gradient)\n",
    "# my_wg = my_fc.weights_gradient\n",
    "# my_bg = my_fc.bias_gradient\n",
    "\n",
    "# atol=1e-3\n",
    "\n",
    "# print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "# print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "# print(\"before adam weights gradients all close:\", np.allclose(my_wg, torch_wg, atol=atol))\n",
    "# print(\"before adam bias gradients all close:\", np.allclose(my_bg, torch_bg, atol=atol))\n",
    "# print(my_wg, \"\\n\", torch_wg)\n",
    "\n",
    "# my_adam = AdamOptimizer(my_fc.get_trainable_layers(), 0.001, 0.9, 0.999, 1e-8)\n",
    "# my_adam.step()\n",
    "\n",
    "# torch_adam = Adam_torch(torch_fc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n",
    "# torch_adam.step()\n",
    "\n",
    "# print(\"after adam weights gradients all close:\", np.allclose(my_fc.weights_gradient, torch_fc.weight.grad.detach().numpy().T, atol=atol))\n",
    "# print(\"after adam bias gradients all close:\", np.allclose(my_fc.bias_gradient, torch_fc.bias.grad.detach().numpy().reshape(-1, 1).T, atol=atol))\n",
    "# print(my_fc.weights_gradient, \"\\n\", torch_fc.weight.grad.detach().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7675aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output all close: True\n",
      "input gradients all close: False\n",
      "weights gradients all close: True\n",
      "bias gradients all close: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BatchNorm test\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 4\n",
    "n_channels = 5\n",
    "height = 8\n",
    "width = 8\n",
    "\n",
    "input_data = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "input_data_torch = torch.from_numpy(input_data).float()\n",
    "input_data_torch.requires_grad = True\n",
    "output_gradient = np.random.rand(batch_size, n_channels, height, width).astype(np.float32)\n",
    "\n",
    "torch_bn = torch.nn.BatchNorm2d(n_channels)\n",
    "\n",
    "my_bn = BatchNormalization2d(n_channels)\n",
    "\n",
    "my_bn.gamma = torch_bn.weight.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "my_bn.beta = torch_bn.bias.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "\n",
    "my_bn.mean = torch_bn.running_mean.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "my_bn.var = torch_bn.running_var.detach().numpy().reshape(1, n_channels, 1, 1)\n",
    "\n",
    "my_out = my_bn.forward(input_data)\n",
    "\n",
    "torch_out = torch_bn(input_data_torch)\n",
    "\n",
    "torch_out_np = torch_out.detach().numpy()\n",
    "\n",
    "torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "\n",
    "torch_input_g = input_data_torch.grad.detach().numpy()\n",
    "\n",
    "my_input_g = my_bn.backward(output_gradient)\n",
    "\n",
    "atol=1e-2\n",
    "\n",
    "print(\"output all close:\", np.allclose(my_out, torch_out_np, atol=atol))\n",
    "print(\"input gradients all close:\", np.allclose(my_input_g, torch_input_g, atol=atol))\n",
    "print(\"weights gradients all close:\", np.allclose(my_bn.gamma_gradient, torch_bn.weight.grad.detach().numpy().reshape(1, n_channels, 1, 1), atol=atol))\n",
    "print(\"bias gradients all close:\", np.allclose(my_bn.beta_gradient, torch_bn.bias.grad.detach().numpy().reshape(1, n_channels, 1, 1), atol=atol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126d38d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ -96.69165 , -102.50775 , -118.53161 , ..., -103.550674,\n",
       "           -96.96536 , -112.12092 ],\n",
       "         [ -95.801544,  -98.363396, -111.3897  , ..., -104.034256,\n",
       "           -99.73653 , -106.02681 ],\n",
       "         [-109.72674 ,  -95.53466 ,  -94.23132 , ...,  -98.246994,\n",
       "          -110.09017 , -118.87275 ],\n",
       "         ...,\n",
       "         [ -96.58109 , -120.28922 , -109.24911 , ..., -105.864784,\n",
       "           -94.068184, -111.03255 ],\n",
       "         [ -95.05327 , -113.320175, -113.666214, ..., -101.68167 ,\n",
       "           -93.22583 ,  -95.58209 ],\n",
       "         [-114.210556, -101.79008 , -107.87765 , ..., -117.61133 ,\n",
       "          -112.086586, -104.878654]],\n",
       "\n",
       "        [[-100.911385, -103.09064 , -105.54439 , ..., -102.68937 ,\n",
       "          -102.33201 , -104.93354 ],\n",
       "         [-100.818306, -103.87654 , -104.293106, ..., -102.49354 ,\n",
       "          -104.273605, -103.11555 ],\n",
       "         [-104.11635 , -104.8842  , -105.349594, ..., -103.90023 ,\n",
       "          -102.2732  , -103.225945],\n",
       "         ...,\n",
       "         [-104.30669 , -105.48772 , -103.39641 , ..., -100.87638 ,\n",
       "          -101.58056 , -102.064026],\n",
       "         [-100.89711 , -101.66501 , -101.27463 , ..., -101.94262 ,\n",
       "          -101.20251 , -103.57704 ],\n",
       "         [-104.14283 , -104.71979 , -101.653114, ..., -102.60057 ,\n",
       "          -105.605255, -103.41701 ]],\n",
       "\n",
       "        [[-107.71066 , -112.06612 , -113.77083 , ..., -110.47832 ,\n",
       "          -113.93008 , -107.81532 ],\n",
       "         [-110.724625, -111.56138 , -111.60478 , ..., -109.2298  ,\n",
       "          -109.68216 , -114.882195],\n",
       "         [-108.478424, -111.64521 , -110.464874, ..., -113.35505 ,\n",
       "          -111.03377 , -108.02115 ],\n",
       "         ...,\n",
       "         [-108.15954 , -111.14594 , -109.40733 , ..., -110.12232 ,\n",
       "          -110.80173 , -110.31013 ],\n",
       "         [-115.32    , -112.13248 , -110.21676 , ..., -106.62693 ,\n",
       "          -111.39195 , -113.92547 ],\n",
       "         [-111.74619 , -112.57173 , -111.295334, ..., -111.09835 ,\n",
       "          -108.179924, -110.36693 ]],\n",
       "\n",
       "        [[ -97.236725, -100.32694 , -103.521126, ..., -105.32672 ,\n",
       "           -96.884926, -101.61785 ],\n",
       "         [ -96.933495, -104.33991 ,  -98.96157 , ..., -101.331116,\n",
       "          -104.850876, -100.19021 ],\n",
       "         [-102.799126, -105.812126,  -99.82767 , ..., -103.75891 ,\n",
       "          -107.30183 , -106.56825 ],\n",
       "         ...,\n",
       "         [-104.23552 ,  -96.234566,  -98.550156, ..., -102.66502 ,\n",
       "          -102.39667 , -101.232254],\n",
       "         [-101.72381 ,  -95.8903  ,  -96.04123 , ..., -103.10535 ,\n",
       "           -99.10614 ,  -99.60094 ],\n",
       "         [ -99.413765, -104.95758 ,  -96.43516 , ..., -106.31397 ,\n",
       "           -99.93274 , -101.556   ]],\n",
       "\n",
       "        [[-111.66065 , -116.37723 , -114.426   , ..., -111.39498 ,\n",
       "          -111.99325 , -113.71854 ],\n",
       "         [-112.342354, -112.41533 , -115.315605, ..., -113.059006,\n",
       "          -117.001434, -113.6458  ],\n",
       "         [-112.718704, -112.872   , -112.43708 , ..., -115.74849 ,\n",
       "          -115.28179 , -115.03383 ],\n",
       "         ...,\n",
       "         [-114.319084, -115.33025 , -116.08722 , ..., -111.23685 ,\n",
       "          -113.18026 , -113.22677 ],\n",
       "         [-113.480385, -111.81854 , -115.46519 , ..., -114.31199 ,\n",
       "          -115.53094 , -115.099915],\n",
       "         [-112.473785, -115.81392 , -117.92527 , ..., -111.41496 ,\n",
       "          -112.70188 , -116.79048 ]]],\n",
       "\n",
       "\n",
       "       [[[-110.777824,  -95.61485 , -110.57648 , ..., -114.404335,\n",
       "           -97.53913 , -110.33235 ],\n",
       "         [-110.180145, -112.40569 ,  -94.85275 , ...,  -99.35908 ,\n",
       "          -109.13355 , -118.649796],\n",
       "         [ -94.468575, -114.304085, -118.22814 , ...,  -97.61642 ,\n",
       "           -94.84467 , -100.14899 ],\n",
       "         ...,\n",
       "         [ -97.01611 , -102.7635  ,  -96.299095, ..., -120.37114 ,\n",
       "          -117.045166, -104.97842 ],\n",
       "         [-111.880486,  -93.852646,  -94.2996  , ..., -108.75081 ,\n",
       "          -104.490875,  -97.73971 ],\n",
       "         [-101.1792  , -109.71258 , -101.45242 , ..., -102.053375,\n",
       "          -113.52383 , -114.88238 ]],\n",
       "\n",
       "        [[-105.190765, -102.37609 , -103.56585 , ..., -103.21661 ,\n",
       "          -101.37151 , -103.25089 ],\n",
       "         [-103.56682 , -104.39073 , -103.69124 , ..., -103.85233 ,\n",
       "          -103.372894, -101.91635 ],\n",
       "         [-105.34021 , -104.52129 , -102.594696, ..., -105.59988 ,\n",
       "          -103.57251 , -102.16442 ],\n",
       "         ...,\n",
       "         [-104.99787 , -104.9521  , -101.65234 , ..., -102.18412 ,\n",
       "          -105.8259  , -104.122246],\n",
       "         [-102.83432 , -102.040726, -101.37381 , ..., -105.11976 ,\n",
       "          -103.32942 , -101.68447 ],\n",
       "         [-101.57107 , -103.89044 , -105.0215  , ..., -104.16546 ,\n",
       "          -102.51178 , -103.73971 ]],\n",
       "\n",
       "        [[-109.783455, -115.17288 , -115.593765, ..., -112.77775 ,\n",
       "          -109.9546  , -112.64199 ],\n",
       "         [-111.9123  , -115.3074  , -110.38551 , ..., -112.07728 ,\n",
       "          -109.09226 , -113.93749 ],\n",
       "         [-109.85014 , -113.510254, -109.01943 , ..., -111.79471 ,\n",
       "          -112.75214 , -110.50062 ],\n",
       "         ...,\n",
       "         [-109.50621 , -112.300285, -111.07779 , ..., -110.79313 ,\n",
       "          -112.01906 , -111.44706 ],\n",
       "         [-114.37668 , -114.98046 , -112.9131  , ..., -111.302635,\n",
       "          -107.54443 , -112.84018 ],\n",
       "         [-111.87062 , -114.94847 , -107.70647 , ..., -107.356636,\n",
       "          -110.59163 , -112.112434]],\n",
       "\n",
       "        [[-103.54916 ,  -97.642365, -104.063965, ...,  -95.67575 ,\n",
       "           -94.73832 , -105.19377 ],\n",
       "         [-102.3842  , -103.770195,  -97.8977  , ..., -100.55137 ,\n",
       "          -103.53414 , -104.66039 ],\n",
       "         [ -97.81953 , -105.19376 , -104.99664 , ...,  -97.9996  ,\n",
       "          -102.73311 , -105.84934 ],\n",
       "         ...,\n",
       "         [-101.49323 , -105.034225, -100.10367 , ..., -105.20078 ,\n",
       "           -99.031715, -105.99917 ],\n",
       "         [-102.960815, -105.37739 , -102.09263 , ..., -103.332146,\n",
       "          -102.58112 , -105.563614],\n",
       "         [-102.95859 ,  -97.30732 ,  -95.27874 , ..., -107.2316  ,\n",
       "          -105.62241 , -103.38605 ]],\n",
       "\n",
       "        [[-112.31709 , -116.02727 , -114.20085 , ..., -113.14959 ,\n",
       "          -112.40984 , -115.009544],\n",
       "         [-111.19505 , -116.66921 , -116.71449 , ..., -113.488075,\n",
       "          -114.21916 , -111.80226 ],\n",
       "         [-114.35779 , -114.988235, -114.15463 , ..., -114.24734 ,\n",
       "          -111.05389 , -115.86017 ],\n",
       "         ...,\n",
       "         [-116.78275 , -113.0903  , -110.284706, ..., -116.30021 ,\n",
       "          -113.11751 , -116.20234 ],\n",
       "         [-109.475006, -111.65122 , -113.86549 , ..., -110.105286,\n",
       "          -116.395706, -111.78412 ],\n",
       "         [-110.42864 , -111.59242 , -115.28524 , ..., -112.018456,\n",
       "          -114.19538 , -112.8689  ]]],\n",
       "\n",
       "\n",
       "       [[[ -99.516266, -113.37121 , -118.67217 , ..., -116.807625,\n",
       "          -104.21288 , -111.89092 ],\n",
       "         [ -96.06742 , -110.38077 , -102.88091 , ..., -111.429886,\n",
       "           -93.473976, -104.58101 ],\n",
       "         [-103.29458 , -119.09761 , -102.463036, ..., -107.96044 ,\n",
       "          -113.36861 ,  -96.332565],\n",
       "         ...,\n",
       "         [-107.93581 , -108.42357 , -114.918465, ..., -101.20095 ,\n",
       "          -109.11843 ,  -97.05133 ],\n",
       "         [ -94.374954, -115.39225 , -118.371056, ..., -112.51592 ,\n",
       "          -119.22971 ,  -98.11252 ],\n",
       "         [-103.88051 , -104.89227 ,  -95.60751 , ..., -113.83563 ,\n",
       "           -99.156006,  -94.647964]],\n",
       "\n",
       "        [[-104.880646, -102.46861 , -103.95184 , ..., -101.50636 ,\n",
       "          -102.41838 , -102.20622 ],\n",
       "         [-103.44059 , -102.010826, -104.15845 , ..., -101.013664,\n",
       "          -104.04089 , -103.10441 ],\n",
       "         [-100.80304 , -103.5187  , -102.42621 , ..., -102.20908 ,\n",
       "          -101.96985 , -103.33895 ],\n",
       "         ...,\n",
       "         [-101.52315 , -103.45521 , -100.85267 , ..., -102.48611 ,\n",
       "          -101.545525, -103.68167 ],\n",
       "         [-104.46868 , -102.65754 , -101.78134 , ..., -105.70754 ,\n",
       "          -103.953514, -101.97116 ],\n",
       "         [-103.92946 , -105.51536 , -105.32408 , ..., -103.948685,\n",
       "          -101.006294, -103.647   ]],\n",
       "\n",
       "        [[-113.14753 , -109.52212 , -109.7026  , ..., -110.45437 ,\n",
       "          -111.10255 , -111.14729 ],\n",
       "         [-112.43876 , -112.429474, -108.472885, ..., -111.63791 ,\n",
       "          -109.97764 , -110.79655 ],\n",
       "         [-113.6013  , -109.2333  , -114.11895 , ..., -110.58923 ,\n",
       "          -107.3445  , -109.55392 ],\n",
       "         ...,\n",
       "         [-110.167046, -111.05879 , -112.63056 , ..., -115.0092  ,\n",
       "          -110.06509 , -113.53347 ],\n",
       "         [-107.093796, -109.18237 , -108.97945 , ..., -113.24368 ,\n",
       "          -110.945335, -107.380516],\n",
       "         [-109.992004, -110.12477 , -107.46748 , ..., -106.93114 ,\n",
       "          -110.71244 , -109.807655]],\n",
       "\n",
       "        [[-102.95973 ,  -98.27057 , -106.51588 , ..., -105.913574,\n",
       "          -100.77659 , -104.948616],\n",
       "         [ -97.34523 , -101.95662 , -100.84522 , ...,  -97.74945 ,\n",
       "          -103.92091 , -101.62069 ],\n",
       "         [ -98.14279 , -102.16034 , -105.60738 , ...,  -99.01652 ,\n",
       "          -106.28093 , -101.38453 ],\n",
       "         ...,\n",
       "         [-102.15002 , -105.319885,  -99.47494 , ...,  -99.664085,\n",
       "          -102.606995, -105.94708 ],\n",
       "         [ -95.590065, -101.34109 ,  -96.47862 , ...,  -97.47338 ,\n",
       "          -102.35772 , -106.77505 ],\n",
       "         [ -97.63504 ,  -99.75681 ,  -97.09213 , ..., -100.84094 ,\n",
       "          -103.258156, -103.14637 ]],\n",
       "\n",
       "        [[-113.59255 , -117.19305 , -112.272446, ..., -113.034805,\n",
       "          -114.24794 , -112.56153 ],\n",
       "         [-115.71674 , -117.38091 , -111.44188 , ..., -112.480095,\n",
       "          -114.23472 , -112.73705 ],\n",
       "         [-113.18684 , -112.27743 , -112.42022 , ..., -112.541   ,\n",
       "          -116.627205, -112.783134],\n",
       "         ...,\n",
       "         [-117.33564 , -112.44642 , -112.28178 , ..., -112.76408 ,\n",
       "          -116.55557 , -110.66223 ],\n",
       "         [-114.17284 , -112.81336 , -113.56217 , ..., -114.06198 ,\n",
       "          -110.19988 , -112.652245],\n",
       "         [-111.71835 , -115.03451 , -116.06183 , ..., -113.856224,\n",
       "          -114.99727 , -115.158714]]],\n",
       "\n",
       "\n",
       "       [[[-113.387314, -118.51412 , -113.09189 , ..., -110.51518 ,\n",
       "          -108.233955, -103.56005 ],\n",
       "         [-110.95115 , -116.8934  , -110.120026, ..., -111.50214 ,\n",
       "          -115.44173 , -106.164856],\n",
       "         [-109.13474 , -109.37506 , -119.17437 , ...,  -92.4971  ,\n",
       "          -112.0336  , -107.96326 ],\n",
       "         ...,\n",
       "         [-101.66238 , -106.33767 , -103.3032  , ..., -101.6973  ,\n",
       "          -100.03784 , -117.70362 ],\n",
       "         [ -93.638306, -119.965935, -101.122345, ..., -114.50465 ,\n",
       "          -105.05378 ,  -93.27058 ],\n",
       "         [-104.95738 ,  -96.05581 , -108.53942 , ...,  -95.3078  ,\n",
       "          -119.0112  , -119.39503 ]],\n",
       "\n",
       "        [[-106.214966, -104.73293 , -102.06984 , ..., -102.20516 ,\n",
       "          -102.16159 , -103.18749 ],\n",
       "         [-103.695496, -102.66061 , -105.4797  , ..., -102.070526,\n",
       "          -103.66451 , -102.03252 ],\n",
       "         [-103.68155 , -102.22082 , -102.12698 , ..., -105.163956,\n",
       "          -102.67916 , -103.903366],\n",
       "         ...,\n",
       "         [-102.551994, -102.14997 , -101.25057 , ..., -102.35807 ,\n",
       "          -103.05216 , -103.27053 ],\n",
       "         [-104.37775 , -105.730995, -103.71527 , ..., -102.420975,\n",
       "          -103.78403 , -104.54685 ],\n",
       "         [-102.34399 , -103.4564  , -104.11568 , ..., -104.474014,\n",
       "          -104.27798 , -103.61016 ]],\n",
       "\n",
       "        [[-113.08496 , -108.81054 , -111.39612 , ..., -112.24449 ,\n",
       "          -112.27599 , -113.13298 ],\n",
       "         [-109.35175 , -112.214516, -110.92727 , ..., -112.088646,\n",
       "          -109.68512 , -108.439766],\n",
       "         [-111.33333 , -111.60563 , -108.76146 , ..., -110.81835 ,\n",
       "          -114.61348 , -112.54172 ],\n",
       "         ...,\n",
       "         [-113.79563 , -113.57597 , -113.083405, ..., -112.78422 ,\n",
       "          -113.264725, -110.815506],\n",
       "         [-111.10118 , -112.41086 , -112.9725  , ..., -113.404   ,\n",
       "          -113.129105, -109.19508 ],\n",
       "         [-110.906624, -107.87007 , -108.362495, ..., -110.40008 ,\n",
       "          -113.96316 , -108.77949 ]],\n",
       "\n",
       "        [[-103.50482 ,  -98.14834 , -101.36792 , ...,  -95.34621 ,\n",
       "           -95.16767 ,  -99.355354],\n",
       "         [ -95.706406, -105.29328 ,  -99.67451 , ...,  -96.04198 ,\n",
       "           -97.68329 , -100.478386],\n",
       "         [-100.9782  , -100.83044 , -107.41229 , ..., -102.402275,\n",
       "          -100.2692  , -101.534706],\n",
       "         ...,\n",
       "         [-101.66723 ,  -96.90808 , -102.50472 , ..., -103.99483 ,\n",
       "           -98.08244 ,  -98.43237 ],\n",
       "         [ -99.69711 ,  -98.46707 ,  -98.24539 , ..., -104.541115,\n",
       "          -102.14087 ,  -96.15944 ],\n",
       "         [-105.71219 ,  -99.20143 ,  -95.869194, ..., -104.89195 ,\n",
       "          -100.16459 , -100.482124]],\n",
       "\n",
       "        [[-113.40728 , -112.80217 , -110.42246 , ..., -116.04052 ,\n",
       "          -116.2606  , -113.24506 ],\n",
       "         [-110.636375, -115.80892 , -115.4099  , ..., -110.36296 ,\n",
       "          -113.463425, -115.55898 ],\n",
       "         [-113.86662 , -113.84915 , -118.043594, ..., -118.180275,\n",
       "          -114.38812 , -113.81974 ],\n",
       "         ...,\n",
       "         [-113.56775 , -113.81704 , -117.29659 , ..., -112.061295,\n",
       "          -110.51767 , -116.52353 ],\n",
       "         [-112.41151 , -115.063065, -112.35302 , ..., -116.68128 ,\n",
       "          -111.72386 , -114.2281  ],\n",
       "         [-111.4461  , -113.5321  , -114.08704 , ..., -114.6073  ,\n",
       "          -114.54039 , -114.73757 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd77f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.4508435 ,  0.08463298, -0.385235  , ..., -1.1686708 ,\n",
       "          -1.1208788 , -0.33767045],\n",
       "         [-1.05566   ,  0.7358076 , -0.79789406, ..., -0.22638187,\n",
       "           1.7529507 , -0.7733383 ],\n",
       "         [ 1.1955929 ,  1.4248011 , -0.75330335, ..., -0.873407  ,\n",
       "          -0.02137   , -1.173744  ],\n",
       "         ...,\n",
       "         [-1.2898002 , -1.3701265 , -0.458097  , ...,  0.92653775,\n",
       "          -0.4665863 ,  0.5171513 ],\n",
       "         [ 1.0035074 ,  0.04794805, -0.4469342 , ..., -0.7265465 ,\n",
       "           0.9905148 , -0.49255392],\n",
       "         [ 1.4415755 ,  1.3664562 ,  1.4176054 , ..., -0.4808115 ,\n",
       "          -1.6929626 , -1.2498816 ]],\n",
       "\n",
       "        [[ 1.771431  ,  0.88217473, -1.4084415 , ...,  0.63214314,\n",
       "           0.73673517, -1.6371592 ],\n",
       "         [ 1.2837431 ,  0.6258052 , -1.0912535 , ...,  0.99158424,\n",
       "           0.23036443,  0.07250367],\n",
       "         [-0.49308574, -0.7319278 , -1.3941584 , ..., -1.2408602 ,\n",
       "           1.1234243 , -0.6575787 ],\n",
       "         ...,\n",
       "         [-1.1751304 , -1.6333505 , -0.8505911 , ...,  1.2105477 ,\n",
       "           1.542756  ,  0.47883272],\n",
       "         [ 1.1845189 ,  0.4428469 ,  1.2908568 , ...,  1.4800493 ,\n",
       "           0.8869777 , -0.69163215],\n",
       "         [-1.28645   , -0.78383046,  0.5427463 , ...,  0.12360106,\n",
       "          -0.9495773 ,  0.574908  ]],\n",
       "\n",
       "        [[ 0.9611104 , -1.0034624 , -1.7090884 , ...,  0.38118777,\n",
       "          -0.19895642,  1.8124641 ],\n",
       "         [-0.97114366,  1.7027403 , -0.77876633, ..., -0.47710523,\n",
       "           1.2027811 , -0.996533  ],\n",
       "         [ 1.1604204 , -1.012473  , -1.0326474 , ...,  0.47638083,\n",
       "          -1.6128091 ,  0.1615901 ],\n",
       "         ...,\n",
       "         [ 1.3943288 ,  0.90700275,  0.46512163, ..., -1.2123526 ,\n",
       "          -1.4195482 ,  0.8170556 ],\n",
       "         [-1.2202985 , -0.9463951 ,  0.7626453 , ...,  1.6807665 ,\n",
       "           0.7695824 , -1.0564924 ],\n",
       "         [ 0.36347824, -1.3807662 , -0.01928954, ...,  0.7735814 ,\n",
       "           0.92951024,  1.3397157 ]],\n",
       "\n",
       "        [[ 0.22316034,  1.108177  ,  0.6368015 , ..., -0.64801484,\n",
       "          -0.14113235,  1.3073012 ],\n",
       "         [-0.41318488, -0.22572595,  1.3174851 , ..., -0.9085179 ,\n",
       "           1.0204762 ,  0.1187356 ],\n",
       "         [ 1.4598066 , -0.14561073,  1.4642253 , ..., -0.71914214,\n",
       "          -0.9134794 , -0.38914102],\n",
       "         ...,\n",
       "         [ 1.4318757 ,  0.20661362, -1.1147883 , ...,  0.78909004,\n",
       "           0.20615913, -1.362997  ],\n",
       "         [-0.22724465,  1.8718761 ,  0.458224  , ..., -1.1067008 ,\n",
       "          -0.8368442 , -0.3889539 ],\n",
       "         [ 0.10353018, -1.3304832 , -0.2058964 , ...,  0.07270464,\n",
       "           0.51288164,  1.2575895 ]],\n",
       "\n",
       "        [[ 0.58393836, -0.9407747 ,  0.3297722 , ...,  0.20143117,\n",
       "           0.48911682,  1.6410352 ],\n",
       "         [ 0.33322507, -0.20845538,  1.2158202 , ..., -0.1083587 ,\n",
       "          -0.8517291 , -1.1963241 ],\n",
       "         [ 1.7879115 , -0.38944784, -1.3910556 , ..., -0.63441414,\n",
       "          -1.786935  ,  0.1929495 ],\n",
       "         ...,\n",
       "         [ 1.330296  , -0.9159414 , -1.0153804 , ...,  1.2015933 ,\n",
       "          -1.586135  , -0.6899755 ],\n",
       "         [ 1.404195  , -0.8081548 , -1.4572433 , ...,  0.696134  ,\n",
       "          -0.01063794,  1.4141293 ],\n",
       "         [ 0.6603763 , -1.2519317 , -1.584848  , ...,  1.6989864 ,\n",
       "           0.03858954, -1.7600317 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.432095  , -0.17977706,  0.7445044 , ..., -1.5549622 ,\n",
       "          -1.2329054 , -1.6937951 ],\n",
       "         [-0.8741567 , -0.22945003, -1.2274315 , ...,  0.35791445,\n",
       "          -1.2301941 ,  0.2683129 ],\n",
       "         [ 1.1904955 , -0.17789675,  0.7164217 , ..., -1.4852215 ,\n",
       "           0.8109445 , -0.12073821],\n",
       "         ...,\n",
       "         [-0.45791715,  1.0901618 , -1.0671092 , ..., -1.101189  ,\n",
       "          -0.6546531 ,  1.179378  ],\n",
       "         [ 0.7613307 , -0.5392677 ,  1.0595266 , ..., -1.1519988 ,\n",
       "           1.1882446 , -0.14436935],\n",
       "         [ 1.40265   ,  1.4115146 ,  0.70259625, ..., -1.3809594 ,\n",
       "           0.17848505, -1.7674985 ]],\n",
       "\n",
       "        [[-1.0499809 ,  0.97417456, -0.34816504, ...,  1.0369028 ,\n",
       "           0.7198889 , -1.0994648 ],\n",
       "         [ 0.9036215 , -0.7446419 , -1.5665623 , ...,  0.7514692 ,\n",
       "           0.69587106,  1.5146016 ],\n",
       "         [-1.0638628 ,  0.14591494,  0.9199047 , ..., -1.594126  ,\n",
       "          -1.0262095 ,  0.20175359],\n",
       "         ...,\n",
       "         [-1.2282218 , -1.5675141 ,  1.6614097 , ...,  0.53781015,\n",
       "          -1.6422068 , -0.6489356 ],\n",
       "         [-0.69981664,  0.7831896 ,  1.1207231 , ..., -1.3316941 ,\n",
       "           0.07271184,  0.34137192],\n",
       "         [ 0.4755785 , -0.8312963 , -0.76819   , ..., -0.9157468 ,\n",
       "           0.21492775, -1.2724817 ]],\n",
       "\n",
       "        [[-0.9729488 , -1.6911991 , -1.6624403 , ...,  0.8472185 ,\n",
       "           0.9411624 , -0.11971013],\n",
       "         [ 1.0105569 , -1.0929142 ,  0.41598973, ...,  1.247703  ,\n",
       "           0.7770968 ,  0.26962787],\n",
       "         [-0.13914496,  0.17463551, -0.09056826, ..., -1.4351807 ,\n",
       "          -0.50493646,  1.6482722 ],\n",
       "         ...,\n",
       "         [ 1.3136885 ,  0.353522  , -0.7403164 , ..., -0.07772919,\n",
       "          -0.63902056, -0.841221  ],\n",
       "         [-0.29866895, -1.5009792 ,  1.049233  , ...,  0.49072477,\n",
       "           0.838138  , -1.5421555 ],\n",
       "         [ 1.0497301 , -0.6971311 ,  1.5553108 , ...,  1.1611779 ,\n",
       "          -0.48184025, -1.5260336 ]],\n",
       "\n",
       "        [[ 0.72171026,  0.18846166, -0.46066105, ...,  0.6068864 ,\n",
       "           1.6134135 , -0.91449195],\n",
       "         [-0.1447627 , -0.83896047, -1.0501568 , ...,  0.47577587,\n",
       "          -1.1201282 , -0.6153415 ],\n",
       "         [-0.5355275 , -1.6097167 ,  0.65084827, ...,  0.6456151 ,\n",
       "           1.0560892 , -1.635901  ],\n",
       "         ...,\n",
       "         [-0.2825537 ,  0.41334286, -1.418018  , ...,  0.65208966,\n",
       "           1.2709047 , -0.3778262 ],\n",
       "         [-1.3972754 ,  0.7793805 ,  1.6205758 , ..., -0.17930008,\n",
       "          -0.6373348 , -0.55145276],\n",
       "         [ 0.72212046, -0.806026  ,  0.99779767, ..., -0.95649534,\n",
       "          -0.71914226,  0.02917986]],\n",
       "\n",
       "        [[-0.25803173,  0.04527468, -0.5452953 , ..., -0.20502228,\n",
       "           0.76116395, -1.3021533 ],\n",
       "         [ 1.1364621 ,  0.00911242, -1.3612638 , ..., -1.5666322 ,\n",
       "          -0.3087684 ,  0.85497016],\n",
       "         [ 0.28243864,  0.6768414 , -1.10214   , ..., -0.02411558,\n",
       "           1.4783343 , -0.584266  ],\n",
       "         ...,\n",
       "         [-0.27294827, -0.56888825,  0.9936715 , ..., -0.92281884,\n",
       "           0.7832325 , -1.1793941 ],\n",
       "         [ 1.7118387 ,  0.00613317,  0.09214163, ...,  1.1881994 ,\n",
       "          -1.7442507 , -0.26892278],\n",
       "         [ 0.7656924 ,  1.7330316 ,  1.1273683 , ...,  0.34833905,\n",
       "          -0.4678822 ,  0.7447487 ]]],\n",
       "\n",
       "\n",
       "       [[[-1.4358349 , -1.1059549 ,  0.22406545, ...,  0.6371243 ,\n",
       "          -0.8484129 , -0.8021574 ],\n",
       "         [ 1.1396346 , -0.05127123, -1.4052907 , ..., -1.224214  ,\n",
       "           0.7224173 ,  0.02674306],\n",
       "         [-0.25928164,  0.12273122, -1.0474821 , ...,  0.78536147,\n",
       "           1.3231083 , -0.7589195 ],\n",
       "         ...,\n",
       "         [-0.02194357,  0.6799246 ,  0.44948876, ...,  1.6424607 ,\n",
       "          -1.560262  ,  1.6814798 ],\n",
       "         [ 1.2190642 ,  0.05800196, -1.5528945 , ...,  0.92181826,\n",
       "           0.50289816,  1.628413  ],\n",
       "         [-1.1686838 ,  0.40824193, -0.50688213, ..., -1.7496892 ,\n",
       "           0.21458715,  1.1189432 ]],\n",
       "\n",
       "        [[-1.6236277 ,  0.42587167, -0.40314364, ...,  1.3905703 ,\n",
       "           0.4200513 , -0.08564872],\n",
       "         [-0.27123854,  0.95950973, -1.2835717 , ...,  0.9561473 ,\n",
       "          -1.1759442 ,  0.1379146 ],\n",
       "         [ 1.7543433 ,  0.8758531 ,  1.7052922 , ...,  1.3331147 ,\n",
       "           0.6097606 , -1.230459  ],\n",
       "         ...,\n",
       "         [ 1.6729578 , -1.3510977 ,  1.418268  , ...,  1.4515059 ,\n",
       "           0.9058613 ,  0.45760888],\n",
       "         [-0.8704638 ,  1.0255805 ,  1.2641768 , ..., -1.5426857 ,\n",
       "          -0.3378546 ,  1.1088821 ],\n",
       "         [ 0.7055135 , -1.1889366 , -0.6515442 , ..., -0.32563075,\n",
       "           1.4212722 , -1.5701544 ]],\n",
       "\n",
       "        [[-1.112492  ,  1.3182939 , -0.4815945 , ...,  0.3134742 ,\n",
       "          -1.4119939 ,  1.7921485 ],\n",
       "         [-0.03596214,  1.1158262 , -0.23012446, ..., -0.20226939,\n",
       "          -0.8456264 ,  1.2459066 ],\n",
       "         [-1.4230378 ,  1.502641  , -1.334874  , ..., -0.48382968,\n",
       "           1.58234   ,  0.5003303 ],\n",
       "         ...,\n",
       "         [-0.9820663 , -0.62445563,  1.3612235 , ..., -1.0887076 ,\n",
       "          -0.9464352 , -0.78722364],\n",
       "         [ 1.0848802 ,  1.2505068 ,  0.30520236, ..., -0.86857635,\n",
       "          -0.13854939,  1.763234  ],\n",
       "         [ 0.2535748 ,  1.5298569 ,  1.2610892 , ...,  1.8271289 ,\n",
       "           0.1463396 , -0.9299973 ]],\n",
       "\n",
       "        [[-1.4488888 ,  1.4464211 , -1.1405888 , ..., -1.3743007 ,\n",
       "          -0.9132329 ,  0.31384835],\n",
       "         [-0.15942979,  0.03826477, -1.3755327 , ...,  0.64894795,\n",
       "           0.5612658 , -0.44793794],\n",
       "         [ 0.48660457,  0.43235415, -0.8707791 , ..., -0.51877326,\n",
       "          -1.0508841 , -0.6157982 ],\n",
       "         ...,\n",
       "         [ 0.09706523,  0.04511824, -1.1795602 , ..., -1.0393308 ,\n",
       "           1.3176607 , -0.40627292],\n",
       "         [ 1.8505985 , -1.0174462 , -0.10792087, ...,  0.8119095 ,\n",
       "           1.7013869 , -1.4340749 ],\n",
       "         [-0.94521296,  1.8264434 ,  1.6476712 , ...,  0.5834042 ,\n",
       "           1.790316  , -1.5566865 ]],\n",
       "\n",
       "        [[-1.0088909 , -1.7300754 , -1.190403  , ..., -1.5134587 ,\n",
       "           0.45450202, -0.32414454],\n",
       "         [-0.24321848, -0.9264183 , -0.06244823, ...,  1.6474907 ,\n",
       "          -0.9713833 , -1.1236554 ],\n",
       "         [ 0.6377974 , -1.1619009 ,  0.60994804, ...,  1.6673642 ,\n",
       "          -0.9973002 ,  0.7775401 ],\n",
       "         ...,\n",
       "         [-0.87322754, -0.11039985,  1.6877054 , ...,  0.29258427,\n",
       "          -0.83045524,  1.4870269 ],\n",
       "         [ 0.81964916, -0.15073678,  0.9619224 , ...,  1.4898446 ,\n",
       "           1.6224469 , -0.53517663],\n",
       "         [ 0.6712237 ,  0.79059005, -0.07906185, ...,  0.64035034,\n",
       "          -1.4234726 , -0.20316197]]],\n",
       "\n",
       "\n",
       "       [[[ 0.75724244, -0.5513486 ,  1.0965815 , ...,  0.06845386,\n",
       "           1.377186  ,  1.2727238 ],\n",
       "         [-0.06235002, -0.43853882, -0.5531581 , ...,  1.6476538 ,\n",
       "           0.70802003, -0.70007825],\n",
       "         [ 0.1512558 ,  1.7264634 ,  0.30239502, ...,  1.4403425 ,\n",
       "          -0.5879654 , -0.72606194],\n",
       "         ...,\n",
       "         [-1.4346817 ,  1.1848755 ,  0.6699255 , ...,  0.5370504 ,\n",
       "          -0.17651889,  1.1376815 ],\n",
       "         [ 1.7637016 ,  0.0663287 ,  1.1087186 , ..., -1.7208991 ,\n",
       "           0.16072787,  1.2952164 ],\n",
       "         [ 1.4114566 ,  0.58970135, -0.71274465, ...,  0.2692112 ,\n",
       "           1.46593   ,  0.51130295]],\n",
       "\n",
       "        [[-1.6508044 , -1.3227382 ,  1.7388946 , ...,  0.49402153,\n",
       "           0.49405077,  1.2156665 ],\n",
       "         [-0.81737304,  1.4401431 , -1.0641361 , ...,  0.90652364,\n",
       "          -1.2919003 ,  0.96433634],\n",
       "         [-1.4822521 , -0.19760154,  0.4537385 , ..., -0.715075  ,\n",
       "          -0.16024414, -0.99695647],\n",
       "         ...,\n",
       "         [-0.425918  ,  1.5865154 ,  1.0332043 , ..., -0.15429632,\n",
       "           0.7323445 ,  0.7574748 ],\n",
       "         [-0.74613005, -1.0954245 ,  0.62818396, ...,  0.19175473,\n",
       "           0.61222047, -0.877643  ],\n",
       "         [ 0.2073689 ,  1.2170452 , -0.52546984, ..., -0.01081913,\n",
       "          -0.71491206, -1.1131502 ]],\n",
       "\n",
       "        [[ 0.19944224,  1.2979347 ,  1.0952713 , ...,  0.4053084 ,\n",
       "           1.777376  ,  0.80065536],\n",
       "         [ 0.6801648 , -0.4069856 , -0.54552627, ...,  1.2667053 ,\n",
       "          -1.5183984 ,  1.5296134 ],\n",
       "         [-0.36583558,  1.4275664 , -0.23921157, ...,  0.43526217,\n",
       "          -1.7671397 , -1.4790944 ],\n",
       "         ...,\n",
       "         [ 0.0620205 , -0.0573302 , -0.7781117 , ...,  0.4903128 ,\n",
       "          -0.21469994, -0.5990769 ],\n",
       "         [-0.40026042, -1.2896127 ,  0.48569205, ..., -1.4471912 ,\n",
       "          -0.7657094 ,  0.417642  ],\n",
       "         [-1.4386213 ,  1.1174895 , -0.15859552, ..., -1.2879288 ,\n",
       "          -0.5101041 ,  1.335347  ]],\n",
       "\n",
       "        [[-1.5786418 , -0.8120788 , -0.23119038, ...,  0.9782069 ,\n",
       "           1.6388336 , -0.90031654],\n",
       "         [ 0.8907594 ,  0.86635566, -0.04485612, ...,  1.0427926 ,\n",
       "          -0.98950124,  1.8217192 ],\n",
       "         [ 0.7478685 , -0.42327955, -1.4528496 , ..., -0.39961013,\n",
       "           0.5673872 , -0.59960794],\n",
       "         ...,\n",
       "         [-0.08978746, -0.126332  ,  0.83499134, ..., -0.5402753 ,\n",
       "          -0.3134892 , -0.7063136 ],\n",
       "         [-0.5500006 ,  0.66742826, -0.5697265 , ..., -0.71470755,\n",
       "           0.6541596 ,  1.1823738 ],\n",
       "         [ 0.11087993,  0.22477145,  0.5095327 , ..., -1.6230069 ,\n",
       "           0.6542065 , -0.15970878]],\n",
       "\n",
       "        [[-1.3708683 ,  1.263473  ,  1.512369  , ..., -1.6942935 ,\n",
       "          -1.505725  , -1.1050438 ],\n",
       "         [ 1.4124297 ,  0.17657243,  0.11452562, ...,  1.4170274 ,\n",
       "           0.91937536, -1.4244876 ],\n",
       "         [ 0.01320998, -1.7720717 , -1.6525644 , ..., -1.4506087 ,\n",
       "          -1.4978901 , -0.62986714],\n",
       "         ...,\n",
       "         [ 0.01176476, -1.1871175 , -1.7904141 , ...,  1.4176064 ,\n",
       "           0.6501023 , -0.05249877],\n",
       "         [-0.12831561,  0.61961216,  0.78330064, ..., -0.39611438,\n",
       "           0.86119026,  0.9769542 ],\n",
       "         [ 1.0518942 , -0.3422593 ,  1.6693791 , ..., -1.1011302 ,\n",
       "           0.88537353,  1.5313706 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bca9fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "my_conv forward time: 0.33388376235961914\n",
      "my_conv_with_loops forward time: 2.353210210800171\n",
      "torch_conv forward time: 0.08339810371398926\n",
      "my_conv fully matrix multiplication: 0.28624987602233887\n",
      "my_conv semi_matrix_backward time: 1.4046761989593506\n",
      "my_conv_with_loops backward time: 2.42439603805542\n",
      "torch_conv backward time: 0.15736699104309082\n",
      "batch_size: 2\n",
      "my_conv forward time: 0.5330967903137207\n",
      "my_conv_with_loops forward time: 2.108637809753418\n",
      "torch_conv forward time: 0.16659021377563477\n",
      "my_conv fully matrix multiplication: 0.4738328456878662\n",
      "my_conv semi_matrix_backward time: 2.6497817039489746\n",
      "my_conv_with_loops backward time: 4.740177631378174\n",
      "torch_conv backward time: 0.3679213523864746\n",
      "batch_size: 4\n",
      "my_conv forward time: 1.0229949951171875\n",
      "my_conv_with_loops forward time: 2.224562644958496\n",
      "torch_conv forward time: 0.1580660343170166\n",
      "my_conv fully matrix multiplication: 0.8497607707977295\n",
      "my_conv semi_matrix_backward time: 5.198260545730591\n",
      "my_conv_with_loops backward time: 9.5380277633667\n",
      "torch_conv backward time: 0.37662506103515625\n",
      "batch_size: 8\n",
      "my_conv forward time: 1.939718246459961\n",
      "my_conv_with_loops forward time: 2.3732821941375732\n",
      "torch_conv forward time: 0.17607378959655762\n",
      "my_conv fully matrix multiplication: 1.6494240760803223\n",
      "my_conv semi_matrix_backward time: 10.129782915115356\n",
      "my_conv_with_loops backward time: 18.280099391937256\n",
      "torch_conv backward time: 0.5119674205780029\n",
      "batch_size: 16\n",
      "my_conv forward time: 3.8003203868865967\n",
      "my_conv_with_loops forward time: 3.2648208141326904\n",
      "torch_conv forward time: 0.24799227714538574\n",
      "my_conv fully matrix multiplication: 3.595266103744507\n",
      "my_conv semi_matrix_backward time: 21.00715708732605\n",
      "my_conv_with_loops backward time: 41.56549143791199\n",
      "torch_conv backward time: 0.45488643646240234\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d vs Conv2dWithLoops vs torch.nn.Conv2d time comparison forward and backward\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "n_input_channels = 4\n",
    "n_output_channels = 2\n",
    "width = 3\n",
    "height = 5\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 3\n",
    "\n",
    "output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "for batch_size in [1, 2, 4, 8, 16]:\n",
    "    print(\"batch_size:\", batch_size)\n",
    "\n",
    "    input_data = np.random.rand(batch_size, n_input_channels, height, width).astype(np.float32)\n",
    "    input_data_torch = torch.from_numpy(input_data).float()\n",
    "    input_data_torch.requires_grad = True\n",
    "    output_gradient = np.random.rand(batch_size, n_output_channels, output_height, output_width).astype(np.float32)\n",
    "\n",
    "    torch_conv = torch.nn.Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "\n",
    "    my_conv_with_loops = Conv2dWithLoops(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv_with_loops.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    my_conv = Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding)\n",
    "    my_conv.weights = torch_conv.weight.detach().numpy()\n",
    "\n",
    "    n_iterations = 1000\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out = my_conv.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_out_with_loops = my_conv_with_loops.forward(input_data)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out = torch_conv(input_data_torch)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv forward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g = my_conv.backward_as_matrix_multiplication(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv fully matrix multiplication: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g = my_conv.semi_matrix_backward(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv semi_matrix_backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        my_input_g_with_loops = my_conv_with_loops.backward(output_gradient)\n",
    "    end = time.time()\n",
    "    print(f\"my_conv_with_loops backward time: {end - start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_iterations):\n",
    "        torch_out.backward(torch.tensor(output_gradient), retain_graph=True)\n",
    "    end = time.time()\n",
    "    print(f\"torch_conv backward time: {end - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bbf79ea567ebf64b92b9cf68f2b08a0cf8db5ff8a7f29f56d99d802810464d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
